# Writing the cleaned & adjusted collect_data.py with your requested changes.
from typing import List, Dict, Optional, Tuple, Callable
import sys, re
import json, time, requests
# === í•™êµ íŒíŠ¸ ë¡œë” & í•™êµëª… ì¶”ì¶œ ===
import os, json, unicodedata
from functools import lru_cache
import re
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from database import Base, Notice, engine  # noqa
from sqlalchemy.orm import sessionmaker
from sqlalchemy.dialects.sqlite import insert as sqlite_insert
from sqlalchemy.exc import IntegrityError
import re, time
from typing import Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List

try:
    from bjd_mapper import get_bjd_name
    HAS_BJD_MAPPER = True
except ImportError:
    print("[Warning] bjd_mapper.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì£¼ì†Œ ë³€í™˜ì´ ì œí•œë©ë‹ˆë‹¤.")
    HAS_BJD_MAPPER = False
import re
from difflib import SequenceMatcher

# -------------------------------------------------------
# CONFIG LOADER (ë¡œì»¬ config.py + Streamlit secrets ìë™ ì§€ì›)
# -------------------------------------------------------

# 1) Streamlit import ì‹œë„
try:
    import streamlit as st
    HAS_ST = True
except:
    HAS_ST = False
    class _DummySecrets(dict):
        def get(self, key, default=None):
            return default
    st = type("dummy", (), {"secrets": _DummySecrets()})

# 2) ë¡œì»¬ config.py import ì‹œë„
try:
    import config as _local_config
    HAS_LOCAL_CONFIG = True
except:
    _local_config = None
    HAS_LOCAL_CONFIG = False


# 3) ìµœì¢… config getter
def _cfg(key: str, default=None):
    # ë¡œì»¬ ê°œë°œí™˜ê²½ ìš°ì„ 
    if HAS_LOCAL_CONFIG and hasattr(_local_config, key):
        return getattr(_local_config, key)

    # Streamlit Cloud â€“ secrets.toml ìš°ì„ 
    if HAS_ST:
        return st.secrets.get(key, default)

    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ default
    return default


def normalize_model_for_compare(m: str) -> str:
    """ë¶ˆí•„ìš” ê¸°í˜¸ ì œê±°í•˜ê³  ì†Œë¬¸ì ì •ê·œí™”"""
    if not m:
        return ""
    m = m.lower().strip()
    m = re.sub(r"[^a-z0-9]", "", m)  # ì˜ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
    return m

def model_similarity(a: str, b: str) -> float:
    """ëª¨ë¸ëª… ìœ ì‚¬ë„ ê³„ì‚° (0~1)"""
    a_n = normalize_model_for_compare(a)
    b_n = normalize_model_for_compare(b)
    if not a_n or not b_n:
        return 0
    return SequenceMatcher(None, a_n, b_n).ratio()



LOG_EXCLUDES = False   # íƒ€ì§€ì—­/ì œì™¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

def print_exclude_once(base_notice: dict, client_name: Optional[str], addr_or_mall: Optional[str]):
    if not LOG_EXCLUDES:
        return
    key = base_notice.get("detail_link") or f"{base_notice.get('stage')}|{base_notice.get('project_name')}|{client_name}"
    if not PRINT_DEDUP or key not in _SEEN_EXCLUDE_KEYS:
        if PRINT_DEDUP:
            _SEEN_EXCLUDE_KEYS.add(key)
        print(f"  [âŒ ì œì™¸ (íƒ€ ì§€ì—­)] {client_name or ''} - {addr_or_mall or ''}")



EXCLUDE_LOG_MAX = 0 if not LOG_EXCLUDES else 50

# íŒŒì¼ ìƒë‹¨ ìœ í‹¸
import os, json, unicodedata
def _norm(s: str) -> str:
    return unicodedata.normalize("NFKC", (s or "").strip())

def load_school_map() -> dict[str, str]:
    # 1) íŒŒì´ì¬ ëª¨ë“ˆ ìš°ì„ 
    try:
        from client_hints_schools import CLIENT_HINTS_SCHOOLS as _S  # :contentReference[oaicite:1]{index=1}
        # A/B í˜•íƒœë„ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì²« ì§€ì‚¬ë§Œ ë¯¸ë¦¬ ì •ë¦¬
        return {_norm(k): _norm(v.split("/")[0]) for k, v in _S.items()}
    except Exception:
        pass

    # 2) JSON(ìˆìœ¼ë©´)
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        p = os.path.join(base_dir, "client_hints_schools.json")
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                data = json.load(f)
            return {_norm(k): _norm(v.split("/")[0]) for k, v in data.items()}
    except Exception:
        pass

    # 3) ì—†ìœ¼ë©´ ë¹ˆ dict
    return {}

def load_client_hints_schools() -> dict[str, str]:
    """
    client_hints_schools.jsonì„ ì½ì–´ {í•™êµëª…: ì§€ì‚¬} ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜.
    íŒŒì¼ì´ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ dict.
    """
    # ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •
    # ì˜ˆ: ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ë¼ë©´:
    base_dir = os.path.dirname(os.path.abspath(__file__))
    json_path = os.path.join(base_dir, "client_hints_schools.json")

    if not os.path.exists(json_path):
        return {}

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # í‚¤/ê°’ ì •ê·œí™”
        return {_norm(k): (v.split("/")[0].strip()) for k, v in data.items()}
    except Exception:
        return {}
from typing import Tuple

def decide_office_and_address_by_apt_or_bjd(kapt_code: str = "", bjd_code: str = "", addr_text: str = "") -> Tuple[str, str]:
    """
    ìš°ì„ ìˆœìœ„:
      1) apt_list(ë‹¨ì§€ì½”ë“œ)ì—ì„œ address/officeë¥¼ ìš°ì„  ì‚¬ìš©
      2) apt_listì— officeê°€ ì—†ìœ¼ë©´ bjd_code(ë˜ëŠ” addr)ë¡œ ì‚¬ì—…ì†Œ ì¶”ë¡ 
      3) addressê°€ ë¹„ì–´ìˆìœ¼ë©´ bjd_mapperë¡œ ì£¼ì†Œ ë³´ê°•
    """
    # 1) apt_list ìš°ì„ 
    csv_addr, csv_office, csv_bjd = lookup_apt_by_code(kapt_code)
    picked_addr = (csv_addr or addr_text or "").strip()
    chosen_office = (csv_office or "").strip()
    bjd_for_fallback = (csv_bjd or bjd_code or "").strip()

    # 2) office ê²°ì • (apt_listê°€ ë¹„ì–´ìˆìœ¼ë©´ bjd ê¸°ì¤€)
    if not chosen_office:
        chosen_office = _assign_office_from_bjd_code(bjd_code=bjd_for_fallback, addr_text=picked_addr)

    # 3) ì£¼ì†Œ ë³´ê°• (apt_list/ì›ë¬¸ ì—†ìœ¼ë©´ bjd_mapperë¡œ)
    if not picked_addr:
        picked_addr = resolve_address_from_bjd(bjd_code=bjd_for_fallback, addr_text=picked_addr)

    return chosen_office, picked_addr


def process_kapt_item(it: dict, page_stage: str = "ì…ì°°ê³µê³ ") -> dict | None:
    """
    K-APT ë‹¨ì¼ ì•„ì´í…œ ì²˜ë¦¬:
    - apt_list(ë‹¨ì§€ì½”ë“œ) ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ bjd_mapper í´ë°±ìœ¼ë¡œ ì£¼ì†Œ/ì§€ì‚¬ ê²°ì •
    - í‘œì‹œì£¼ì†Œ(display_address) ìƒì„±(ë„ë¡œëª… + (ë™) ë³´ê°•)
    - Notice dict ìƒì„±
    """
    # 0) ì•ˆì „ ì¶”ì¶œ
    kapt_code   = _as_text(it.get("aptCode") or it.get("kaptCode"))
    bjd_code    = _as_text(it.get("bjdCode") or it.get("bidArea") or it.get("bjd_code"))
    addr_raw    = _as_text(it.get("roadAddr") or it.get("addr") or it.get("bidAddr"))
    project_name = _as_text(it.get("bidTitle") or it.get("projectName") or "ê³µê³ ëª… ì—†ìŒ")
    client_name  = _as_text(it.get("bidKaptname") or it.get("client") or project_name)
    amount_txt   = _as_text(it.get("amount") or "")
    notice_dt    = to_ymd(it.get("bidRegdate") or it.get("noticeDate"))

    # 1) ì£¼ì†Œ/ì‚¬ì—…ì†Œ ê²°ì • (apt_list ìš°ì„  â†’ bjd í´ë°±)
    # K-APT ê¸°ë³¸ì •ë³´(ì—°ë½ì²˜/ì£¼ì†Œ ë³´ê°•) ì¦‰ì‹œ ì¡°íšŒ
    basic = fetch_kapt_basic_info(kapt_code) if kapt_code else None
    phone_mgmt = _extract_kapt_phone(basic)  # â† ê´€ë¦¬ì‚¬ë¬´ì†Œ ì „í™”
    # ì£¼ì†Œ/ì‚¬ì—…ì†Œ ê²°ì •
    office, addr_core = decide_office_and_address_by_apt_or_bjd(
        kapt_code=kapt_code, bjd_code=bjd_code, addr_text=(addr_raw or (basic or {}).get("doroJuso") or (basic or {}).get("kaptAddr") or "")
    )

    # 2) í‘œì‹œì£¼ì†Œ ìƒì„±(ë„ë¡œëª… + (ë™) ë³´ê°•)
    #    _compose_display_addrëŠ” dictì—ì„œ addr/roadAddr/bjdCode/as1/as2/as3 ë“±ì„ ì°¸ê³ í•©ë‹ˆë‹¤.
    compose_input = dict(it)
    compose_input.update({
        "addr": addr_core or addr_raw,
        "roadAddr": addr_raw,
        "bjdCode": bjd_code,
        "kaptCode": kapt_code,
    })
    display_addr = _compose_display_addr(compose_input)
    it["display_address"] = display_addr  # í•„ìš” ì‹œ ë‹¤ë¥¸ ê³³ì—ì„œë„ í™œìš© ê°€ëŠ¥

    # 3) ìƒì„¸ë§í¬ ìƒì„±(ì—†ìœ¼ë©´ ì•ˆì „ í´ë°±)
    bid_num = _as_text(it.get("bidNum"))
    if bid_num:
        detail_link = f"https://www.k-apt.go.kr/bid/bidDetail.do?no={bid_num}"
    else:
        detail_link = "https://www.k-apt.go.kr/bid/bidList.do"

    # 4) ê¸°ë³¸ Notice ë¼ˆëŒ€
    base = _build_base_notice(
        stage=page_stage,
        biz_type=_as_text(it.get("codeClassifyType1") or "ê¸°íƒ€"),
        project_name=project_name,
        client=client_name,
        phone=phone_mgmt,             # âœ… ê´€ë¦¬ì‚¬ë¬´ì†Œ ì „í™” ì¦‰ì‹œ ë°˜ì˜
        model="",
        qty=0,
        amount=amount_txt,
        is_cert="í™•ì¸í•„ìš”",
        notice_date=notice_dt,
        detail_link=detail_link,
        source="K-APT",
        kapt_code=kapt_code
    )

    # 5) ì£¼ì†Œ/ì‚¬ì—…ì†Œ ìµœì¢… ë°˜ì˜
    addr_final = display_addr or addr_core or addr_raw
    base["office"]    = office
    base["mall_addr"] = addr_final

    # 6) Notice dict ë§ˆê°
    n = finalize_notice_dict(base, None, addr_final, client_name)
    return n


def fetch_pages_parallel(url, params_list):
    results = []
    with ThreadPoolExecutor(max_workers=5) as executor:  # ë™ì‹œ 5ìŠ¤ë ˆë“œ
        futures = [executor.submit(http_get_json, url, p) for p in params_list]
        for f in as_completed(futures):
            results.append(f.result())
    return results



def bulk_upsert_notices(notices):
    if not notices:
        return
    
    session.begin()
    try:
        stmt = sqlite_insert(Notice).values(notices)
        
        # ì‚¬ìš©ìê°€ ì§ì ‘ ê´€ë¦¬í•˜ëŠ” is_favorite, status, memoëŠ” ì—…ë°ì´íŠ¸ì—ì„œ ì œì™¸
        update_cols = {
            col.name: col
            for col in stmt.excluded
            if col.name not in ['id', 'is_favorite', 'status', 'memo']
        }

        stmt = stmt.on_conflict_do_update(
            index_elements=["source_system", "detail_link", "model_name", "assigned_office"],
            set_=update_cols
        )

        session.execute(stmt)
        session.commit()
    except Exception as e:
        session.rollback()
        print(f"  [Error] Bulk upsert ì‹¤íŒ¨: {e}")





@lru_cache(maxsize=5000)
def get_usr_info_cached(dminstt_code):
    return get_full_address_from_usr_info(dminstt_code)

import random
def safe_get(url, params):
    for i in range(3):
        try:
            return http_get_json(url, params)
        except:
            time.sleep(0.5 * (2 ** i) + random.random())
    return None


USE_KEA_CHECK = True  # ì¼ë‹¨ ë•ë‹ˆë‹¤. (í•„ìš”í•  ë•Œë§Œ True)

# =========================
# ê³µí†µ ì„¤ì •
# =========================
API_HOST = "apis.data.go.kr"
API_SCHEME = "http"   # ë‚´ë¶€ë§/ë°©í™”ë²½ í™˜ê²½ ê³ ë ¤


# === K-APT ìµœì†Œ ë¡œê·¸ í¬ë§· (ì£¼ì†Œë¥¼ ëì— ë¶™ì—¬ì„œ ì¶œë ¥) ===
def _fmt_tail(addr: str) -> str:
    addr = _as_text(addr).strip()
    return f" - {addr}" if addr else ""

def log_kapt_excluded(name: str, addr: str = ""):
    if not LOG_EXCLUDES:
        return
    print(f"[âŒ ì œì™¸ (íƒ€ ì§€ì—­)] {_as_text(name)}{_fmt_tail(addr)}")

def log_kapt_pending(office: str, name: str, addr: str = ""):
    print(f"[ğŸ§º ì €ì¥ ëŒ€ê¸°] {office} / {name}" + (f" - {addr}" if addr else ""))

def log_kapt_saved(office: str, name: str, addr: str = ""):
    print(f"[âœ… ì €ì¥ ì™„ë£Œ] {_as_text(office)} / {_as_text(name)}{_fmt_tail(addr)}")

def log_kapt_bulk_saved(n: int):
    print(f"[âœ… ì¼ê´„ ì €ì¥] {int(n)}ê±´")


# =========================
# ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
# =========================
USE_NAME_BASED_USRINFO = False   # ê¸°ê´€ëª… ê¸°ë°˜ UsrInfo ë³´ì¡°ì¡°íšŒ ì‚¬ìš© ì—¬ë¶€
VERBOSE = False                  # ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

def log(msg: str):
    if VERBOSE:
        print(msg)


# =========================
# ë¡œê·¸ ì¤‘ë³µ ë°©ì§€ (detail_link ê¸°ì¤€ 1íšŒë§Œ)
# =========================
PRINT_DEDUP = True
_SEEN_EXCLUDE_KEYS = set()


# ì—”ë“œí¬ì¸íŠ¸
ORDER_PLAN_LIST_PATH = "/1230000/ao/OrderPlanSttusService/getOrderPlanSttusListThng"
BID_LIST_PATH        = "/1230000/ao/PubDataOpnStdService/getDataSetOpnStdBidPblancInfo"
CNTRCT_LIST_PATH     = "/1230000/ao/CntrctInfoService/getCntrctInfoListThng"
DLVR_LIST_PATH       = "/1230000/at/ShoppingMallPrdctInfoService/getDlvrReqInfoList"
USR_INFO_PATH        = "/1230000/ao/UsrInfoService/getDminsttInfo"
# [ì¶”ê°€] ê³µë™ì£¼íƒ(K-APT)
KAPT_BID_LIST_PATH = "/1613000/ApHusBidPblAncInfoOfferServiceV2/getPblAncDeSearchV2"
KAPT_PRIVATE_CONTRACT_PATH = "/1613000/ApHusPrvCntrNoticeInfoOfferServiceV2/getRegDeSearchV2"
# [ì¶”ê°€] K-APT ë‹¨ì§€ì •ë³´/ìœ ì§€ê´€ë¦¬ì´ë ¥ API ì—”ë“œí¬ì¸íŠ¸
KAPT_BASIC_INFO_PATH = "/1613000/AptBasisInfoServiceV4/getAphusBassInfoV4"
KAPT_DETAIL_INFO_PATH = "/1613000/AptBasisInfoServiceV4/getAphusDtlInfoV4"
KAPT_MAINTENANCE_PATH = "/1613000/ApHusMntMngHistInfoOfferServiceV2/getElctyExtgElvtrMntncHistInfoSearchV2"


def _kapt_items_safely(data) -> list[dict]:
    """
    K-APT ì‘ë‹µì„ ì–´ë–¤ í˜•íƒœë¡œ ë°›ë”ë¼ë„ list[dict]ë¡œ ì•ˆì „ ì •ê·œí™”.
    í—ˆìš© ì¼€ì´ìŠ¤:
      - {"response":{"body":{"items": ...}}}
      - {"response":{"body":{"item": ...}}}
      - {"body":{"items": ...}} / {"body":{"item": ...}}
      - ìµœìƒìœ„ê°€ ê³§ë°”ë¡œ list/dict ì¸ ê²½ìš°
    """
    if data is None:
        return []

    # 1) ìµœìƒìœ„ê°€ ë°”ë¡œ listë©´ dictë§Œ ì¶”ë ¤ì„œ ë°˜í™˜
    if isinstance(data, list):
        return [x for x in data if isinstance(x, dict)]

    # 2) ìµœìƒìœ„ dictì—ì„œ ê³„ì¸µì ìœ¼ë¡œ íŒŒê³ ë“¤ê¸°
    if isinstance(data, dict):
        # ê°€ì¥ ì¼ë°˜í˜•
        body = ((data.get("response") or {}).get("body") or {})
        for key in ("items", "item"):
            cont = body.get(key)
            if cont is not None:
                return _as_items_list(cont)

        # ë³€í˜•: ìµœìƒìœ„ì— ë°”ë¡œ items/item
        for key in ("items", "item", "list", "data"):
            cont = data.get(key)
            if cont is not None:
                return _as_items_list(cont)

        # ì—¬ì°¨í•˜ë©´ dict ì „ì²´ë¥¼ ë‹¨ê±´ ì·¨ê¸‰
        return [data] if data else []

    # ê¸°íƒ€ íƒ€ì…ì€ ë¬´ì‹œ
    return []


def _as_items_list(obj) -> list[dict]:
    """
    dict/list/None/ë‹¨ê±´ í˜¼ì¬ ì‘ë‹µì„ ì•ˆì „í•˜ê²Œ list[dict]ë¡œ ì •ê·œí™”.
    """
    if obj is None:
        return []
    if isinstance(obj, list):
        # ë¦¬ìŠ¤íŠ¸ ì›ì†Œê°€ dictê°€ ì•„ë‹ ìˆ˜ë„ ìˆìœ¼ë‹ˆ dictë§Œ í•„í„°
        return [x for x in obj if isinstance(x, dict)]
    if isinstance(obj, dict):
        # body.items í˜¹ì€ ë‹¨ê±´ dict
        # í”í•œ ì¼€ì´ìŠ¤: {"item":[{...},{...}]}
        for key in ("items","item","list","data"):
            v = obj.get(key)
            if isinstance(v, list):
                return [x for x in v if isinstance(x, dict)]
            if isinstance(v, dict):
                return [v]
        return [obj]
    # ë‚˜ë¨¸ì§€ëŠ” ë¬¸ìì—´ ë“± â†’ ë¹ˆ ëª©ë¡
    return []



def api_url(path: str) -> str:
    return f"{API_SCHEME}://{API_HOST}{path}"

# HTTP ì„¸ì…˜/ì¬ì‹œë„
SESSION = requests.Session()
SESSION.trust_env = True
SESSION.headers.update({
    "User-Agent": "EERS-Collector/2.2 (+https://g2b.go.kr)",
    "Accept": "application/json, text/plain, */*",
    "Connection": "keep-alive",
})
_retries = Retry(total=3, backoff_factor=0.4, status_forcelist=(429, 500, 502, 503, 504))
_adapter = HTTPAdapter(
    max_retries=_retries,
    pool_connections=100,   # ì¶”ê°€
    pool_maxsize=100        # ì¶”ê°€
)
SESSION.mount("http://", _adapter)
SESSION.mount("https://", _adapter)
DEFAULT_TIMEOUT = (5, 20)  # (connect, read)

# =========================
# ìœ í‹¸
# =========================
# In collect_data.py (at the very bottom of the file)
# [ADD] ë‚˜ë¼ì¥í„° ì „ìš©: í…ìŠ¤íŠ¸ì—ì„œ 'â—‹â—‹ì´ˆ/ì¤‘/ê³ /ëŒ€í•™(êµ)' í•™êµëª…ë§Œ ë½‘ê¸°

# ë””ë²„ê·¸ ì¶œë ¥ (VERBOSEì¼ ë•Œë§Œ)
def _debug(msg: str):
    if VERBOSE:
        print(msg)

# í•©ê³„ ìš”ì•½ í•œ ì¤„
def _print_total_summary(total: int, *, tag: str | None = None):
    pages = (int(total) + PAGE_SIZE - 1) // PAGE_SIZE if total > 0 else 0
    if tag:
        #print(f"- ì´ {total}ê±´ / {pages}p ({tag})")
        print(f"- ì´ {total}ê±´")
    else:
        #print(f"- ì´ {total}ê±´ / {pages}p")
        print(f"- ì´ {total}ê±´")


# ===== ë¡œê·¸ í—¬í¼ (ë‚˜ë¼ì¥í„° ì‹œê·¸ë‹ˆì²˜ ì •ë ¬) =====
def _print_data_none():
    print("  - ë°ì´í„° ì—†ìŒ")

def _print_bulk_saved(n: int, prefix: str = ""):
    # prefixê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ "  [âœ… prefix ì¼ê´„ ì €ì¥] Nê±´"
    if prefix:
        print(f"  [âœ… {prefix} ì¼ê´„ ì €ì¥] {int(n)}ê±´")
    else:
        print(f"  [âœ… ì¼ê´„ ì €ì¥] {int(n)}ê±´")

def _debug(msg: str):
    if VERBOSE:
        print(msg)


def _extract_school_name(*parts: str) -> str | None:
    txt = " ".join([p for p in parts if p]).strip()
    if not txt:
        return None
    # ê´„í˜¸ ì œê±° + ê³µë°± ì •ë¦¬
    txt = re.sub(r"[\[\(ï¼ˆ].*?[\]\)ï¼‰]", " ", txt)
    txt = " ".join(txt.split())

    SUFFIX = r"(?:ì´ˆë“±í•™êµ|ì¤‘í•™êµ|ê³ ë“±í•™êµ|ëŒ€í•™êµ|ëŒ€í•™|ë§ˆì´ìŠ¤í„°ê³ ë“±í•™êµ)"
    # ì „ì²´ì—ì„œ 'â€¦í•™êµ' ë©ì–´ë¦¬ í›„ë³´ë“¤ ì¶”ì¶œ
    candidates = re.findall(rf"([ê°€-í£0-9A-Za-zÂ·\-\s]+?{SUFFIX})", txt)
    if not candidates:
        return None

    # ê°€ì¥ ì˜¤ë¥¸ìª½ í›„ë³´ ì„ íƒ
    cand = sorted(set(candidates), key=lambda s: (txt.rfind(s), len(s)))[-1].strip()

    # ğŸ”§ ì—¬ê¸°ì„œ ë§ˆì§€ë§‰ í† í°ë§Œ ë‚¨ê¸°ê¸°: (êµìœ¡ì²­ + í•™êµ) â†’ (í•™êµ)
    m = re.search(rf"([^\s]+?{SUFFIX})\s*$", cand)
    if m:
        cand = m.group(1).strip()

    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”(ê°€ë” ì„ì—¬ ë“¤ì–´ì˜¤ëŠ” íŠ¹ìˆ˜ê³µë°± ëŒ€ë¹„)
    import unicodedata
    cand = unicodedata.normalize("NFKC", cand)
    return cand or None

# [ADD] ë‚˜ë¼ì¥í„° ì „ìš©: í•™êµëª… ê¸°ë°˜ ì§€ì‚¬ ë°°ì • (CLIENT_HINTS_SCHOOLS ìš°ì„ )
def _assign_office_by_school_name(client_name: str, project_name: str) -> str | None:
    """
    - client_hints_schools.py(ì™¸ë¶€ ì‚¬ì „)ë¥¼ ìµœìš°ì„  ì‚¬ìš©
    - ì—†ê±°ë‚˜ ë¯¸ìŠ¤ë§¤ì¹˜ë©´ CLIENT_HINTS(í†µí•© ì‚¬ì „)ì—ì„œ 'í•™êµ' í‚¤ë§Œ í´ë°±
    - ë¹„êµëŠ” ëª¨ë‘ NFKC ì •ê·œí™” + ì–‘ë°©í–¥ ë¶€ë¶„ì¼ì¹˜
    """
    school = _extract_school_name(client_name, project_name)
    if not school:
        return None
    s_norm = _norm(school)

    # 1ìˆœìœ„: ë³„ë„ í•™êµ ì‚¬ì „ (client_hints_schools.py)
    try:
        from client_hints_schools import CLIENT_HINTS_SCHOOLS as _S
        # í‚¤/ê°’ ëª¨ë‘ ì •ê·œí™”, ê°’ì€ "A/B"ë©´ ì²« ì§€ì‚¬ë§Œ
        S = {_norm(k): _norm(v.split("/")[0]) for k, v in _S.items()}
        for k in sorted(S.keys(), key=len, reverse=True):
            # ì–‘ë°©í–¥ ë¶€ë¶„ì¼ì¹˜ í—ˆìš© (êµìœ¡ì²­+í•™êµ í˜•íƒœ ë“± ê¸´/ì§§ì€ ì–‘ìª½ ì»¤ë²„)
            if s_norm == k or s_norm in k or k in s_norm:
                return S[k]
    except Exception:
        pass

    # 2ìˆœìœ„: í†µí•© íŒíŠ¸ ì‚¬ì „(CLASSIC) - 'í•™êµ' í‚¤ë§Œ ëŒ€ìƒ
    try:
        C = CLIENT_HINTS  # collect_data.py ë“±ì—ì„œ ì„ ì–¸/ë³‘í•©ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
        for k in sorted(C.keys(), key=len, reverse=True):
            if "í•™êµ" not in k:
                continue
            k_norm = _norm(k)
            if s_norm == k_norm or s_norm in k_norm or k_norm in s_norm:
                # ê°’ë„ "A/B"ë©´ ì²« ì§€ì‚¬ë§Œ
                return _norm((C[k] or "").split("/")[0])
    except Exception:
        pass

    return None

def _as_text(x) -> str:
    """ë¦¬ìŠ¤íŠ¸/ìˆ«ì/None ë“±ë„ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜."""
    if x is None:
        return ""
    if isinstance(x, str):
        return x
    if isinstance(x, (int, float)):
        return str(x)
    if isinstance(x, list):
        return " ".join(_as_text(v) for v in x)
    try:
        # dict ë“±ì€ JSON ë¬¸ìì—´í™” (ensure_ascii=Falseë¡œ í•œê¸€ ìœ ì§€)
        return json.dumps(x, ensure_ascii=False)
    except Exception:
        return str(x)
    

@lru_cache(maxsize=1)
def _load_school_hints() -> dict:
    """
    client_hints_schools.py / client_hints_schools.json ë³‘í•© ë¡œë“œ
    ìš°ì„ ìˆœìœ„: PY â†’ JSON (PYê°€ ìˆìœ¼ë©´ ìš°ì„ )
    """
    hints = {}

    # 1) .py ë¡œë“œ
    try:
        from client_hints_schools import CLIENT_HINTS_SCHOOLS as _PY_HINTS
        if isinstance(_PY_HINTS, dict):
            hints.update(_PY_HINTS)
    except Exception:
        try:
            # ì¼ë¶€ í”„ë¡œì íŠ¸ëŠ” í‚¤ ì´ë¦„ì´ CLIENT_HINTS ì¸ ê²½ìš°ê°€ ìˆìŒ
            from client_hints_schools import CLIENT_HINTS as _PY_HINTS2
            if isinstance(_PY_HINTS2, dict):
                hints.update(_PY_HINTS2)
        except Exception:
            pass

    # 2) .json ë¡œë“œ (ìˆìœ¼ë©´ ë³‘í•©: PYì— ì—†ëŠ” í‚¤ë§Œ ì¶”ê°€)
    try:
        base_dir = os.path.dirname(__file__)
        jpath = os.path.join(base_dir, "client_hints_schools.json")
        if os.path.isfile(jpath):
            with open(jpath, "r", encoding="utf-8") as f:
                j = json.load(f)
                if isinstance(j, dict):
                    for k, v in j.items():
                        hints.setdefault(k, v)
    except Exception:
        pass

    return hints

def extract_school_name(client_name: str) -> str:
    """
    ê³ ê°ëª… ë¬¸ìì—´ì—ì„œ 'í•™êµëª…'ë§Œ íŒíŠ¸ í…Œì´ë¸” í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ.
    - ê°€ì¥ ê¸´ í‚¤ê°€ ë¶€ë¶„ì¼ì¹˜ í•˜ëŠ” ê²ƒì„ ìš°ì„  ì±„íƒ(ì •í™•ë„â†‘)
    - '(... )' ê¼¬ë¦¬í‘œ ì œê±°, ê³µë°±/ì¤‘ë³µ ê³µë°± ì •ë¦¬
    """
    if not client_name:
        return ""

    # ê´„í˜¸ ë“± ëë¶€ë¶„ ë¶€ê°€í‘œê¸° ì œê±°
    name = re.sub(r"\s*\([^)]*\)\s*$", "", client_name).strip()
    name_no_space = re.sub(r"\s+", "", name)

    hints = _load_school_hints()
    if not hints:
        return name  # íŒíŠ¸ê°€ ì—†ìœ¼ë©´ ì›ë¬¸ ë°˜í™˜

    # í‚¤ë¥¼ ê¸¸ì´ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ â†’ ê°€ì¥ ê¸´ í‚¤ë¶€í„° ë§¤ì¹­
    keys = sorted(hints.keys(), key=lambda k: len(k), reverse=True)

    best = ""
    for key in keys:
        k = key.strip()
        if not k:
            continue
        k_no_space = re.sub(r"\s+", "", k)
        # ë¶€ë¶„ í¬í•¨(ê³µë°± ë¬´ì‹œ ë¹„êµ í¬í•¨)
        if (k in name) or (k_no_space in name_no_space):
            best = k
            break

    return best or name  # ìµœì¢… ì‹¤íŒ¨ì‹œ ì›ë¬¸ ë°˜í™˜

def office_by_school_hint(school_name: str) -> str:
    """
    ì¶”ì¶œëœ 'í•™êµëª…'ìœ¼ë¡œ ì‚¬ì—…ì†Œ íŒíŠ¸ ì¡°íšŒ.
    ê°’ì— 'A/B'ì²˜ëŸ¼ ìŠ¬ë˜ì‹œê°€ ìˆìœ¼ë©´ ì²« í•­ëª© ì‚¬ìš©.
    """
    if not school_name:
        return ""
    hints = _load_school_hints()
    hit = hints.get(school_name.strip())
    if not hit:
        return ""
    return hit.split("/")[0].strip()


def _has_dong_level_str(a: str) -> bool:
    return bool(re.search(r"(ë™|ì|ë©´|ë¦¬)\b", a or ""))

def _narrow_office_with_basic_info(assigned: str, kapt_code: str, addr_txt: str, bjd_code: str):
    """
    A/B(ë³µìˆ˜ê´€í• ) ì´ê±°ë‚˜ ì£¼ì†Œì— ë™/ì/ë©´ ë ˆë²¨ì´ ì—†ìœ¼ë©´
    K-APT ê¸°ë³¸ì •ë³´ë¡œ ì£¼ì†Œ/ë²•ì •ë™ì½”ë“œ ë³´ê°• í›„ ê´€í•  ì¬íŒì •.
    """
    assigned = _as_text(assigned).strip()
    addr_txt = _as_text(addr_txt)
    bjd_code = _as_text(bjd_code)

    try:
        need_narrow = ("/" in assigned) or (not _has_dong_level_str(addr_txt))
        if not need_narrow or not kapt_code:
            return assigned, addr_txt, bjd_code

        basic = fetch_kapt_basic_info(kapt_code) or {}
        addr2 = (basic.get("doroJuso") or basic.get("kaptAddr") or addr_txt or "").strip()
        bjd2  = str(basic.get("bjdCode") or bjd_code or "").strip()

        # ì£¼ì†Œê°€ ì•„ì§ë„ ë¹„ë©´ bjd_mapperë¡œ ë³´ê°•
        if not addr2 and bjd2:
            try:
                from bjd_mapper import get_bjd_name
                addr2 = (get_bjd_name(bjd2) or "").strip()
            except Exception:
                pass

        reassigned = _assign_office_from_bjd_code(bjd2, addr2)
        # ì„±ê³µì ìœ¼ë¡œ ë‹¨ì¼ ê´€í• ë¡œ ë‚´ë ¤ê°€ë©´ êµì²´
        if reassigned and "/" not in reassigned and not reassigned.startswith("ê´€í• "):
            return reassigned, addr2 or addr_txt, bjd2 or bjd_code
        # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ê°’ ìœ ì§€
        return assigned, addr_txt, bjd_code
    except Exception:
        return assigned, addr_txt, bjd_code



def _has_dong_level(a: str) -> bool:
    a = _as_text(a)
    return bool(__import__("re").search(r"(ë™|ì|ë©´|ë¦¬)\b", a))


def _to_int(v) -> int:
    """'1,234' / '10.0' / ' 10 ' ë“±ë„ ì•ˆì „ ë³€í™˜."""
    try:
        if v is None: return 0
        s = str(v).strip().replace(",", "")
        if not s: return 0
        return int(float(s))
    except Exception:
        return 0


def cleanup_session():
    """ì „ì—­ ì„¸ì…˜ì„ ë‹«ì•„ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í•´ì œí•©ë‹ˆë‹¤."""
    global session
    if session:
        try:
            session.close()
            print("[DB] Worker session closed.")
        except Exception as e:
            print(f"[DB] Worker session close error: {e}")



def _handle_broad_keyword_case(client_name: Optional[str], addr: Optional[str], base_notice: dict) -> bool:
    name = client_name or ""
    if not name:
        return False

    # âœ… ë¶€ì‚°/í•´ìš´ëŒ€êµ¬ ë“± íƒ€ê¶Œì—­ì´ ëª…ì‹œë¼ ìˆìœ¼ë©´ ì¦‰ì‹œ ì œì™¸
    other_hits = any(kw in name for kw in ["ë¶€ì‚°", "í•´ìš´ëŒ€êµ¬", "í•´ìš´ëŒ€"])  # í•„ìš”í•œ ê²½ìš° ë” ì¶”ê°€
    target_hits = any(kw in name for kw in ["ëŒ€êµ¬", "ê²½ìƒë¶ë„", "ê²½ë¶", "í¬í•­", "ê²½ì£¼", "ê²½ì‚°", "ê¹€ì²œ", "ì˜ì²œ", "ì¹ ê³¡", "ì„±ì£¼", "ì²­ë„", "ê³ ë ¹", "ì˜ë•"])
    if other_hits and not target_hits:
        return False

    offices_to_assign = None
    # ğŸ” ì´ì „: if "ëŒ€êµ¬" in name: ...
    # âœ… ë³€ê²½: 'ëŒ€êµ¬', 'ëŒ€êµ¬ì‹œ', 'ëŒ€êµ¬ê´‘ì—­ì‹œ'ë¥¼ 'ë‹¨ì–´'ë¡œë§Œ ì¸ì‹
    if _contains_token(name, ["ëŒ€êµ¬", "ëŒ€êµ¬ì‹œ", "ëŒ€êµ¬ê´‘ì—­ì‹œ"]):
        offices_to_assign = DAEGU_OFFICES
    # ğŸ” ì´ì „: elif "í¬í•­" in name:
    elif _contains_token(name, ["í¬í•­", "í¬í•­ì‹œ"]):
        offices_to_assign = ["í¬í•­ì§€ì‚¬", "ë¶í¬í•­ì§€ì‚¬"]

    if offices_to_assign:
        offices_str = "/".join(offices_to_assign)
        n = dict(base_notice)
        n["assigned_office"] = offices_str
        n["address"] = addr or ""
        upsert_notice(n)
        session.commit()
        print(f"  [âš ï¸ ì €ì¥ (ë³µìˆ˜ ê´€í• )] {offices_str} / {n.get('client')}")
        return True

    # ë‚˜ë¨¸ì§€ ê´‘ì—­ í‚¤ì›Œë“œëŠ” í–‰ì •êµ¬ì—­ ë‹¨ì–´ ê²½ê³„ë¡œ íŒë‹¨
    for keyword, office in BROAD_KEYWORD_OFFICE_MAP.items():
        if _contains_token(name, [keyword]):
            n = dict(base_notice)
            n["assigned_office"] = office
            n["address"] = addr or ""
            upsert_notice(n)
            session.commit()
            print(f"  [âœ… ì €ì¥ ì™„ë£Œ] {n.get('assigned_office')} / {n.get('client')}")
            return True

    return False

# === K-APT í‚¤ì›Œë“œ í•„í„°: config â†’ ENV â†’ ê¸°ë³¸ê°’ ===
import os, json, re

def _get_conf_list(attr_name: str, env_name: str, default_list):
    # 1) config.pyì— ë¦¬ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    try:
        import config as _cfg
        if hasattr(_cfg, attr_name):
            v = getattr(_cfg, attr_name)
            if isinstance(v, (list, tuple, set)): return list(v)
            if isinstance(v, str) and v.strip(): return [v.strip()]
    except Exception:
        pass
    # 2) í™˜ê²½ë³€ìˆ˜(JSON ë°°ì—´ ë˜ëŠ” ì½¤ë§ˆ êµ¬ë¶„)
    s = os.getenv(env_name, "")
    if s.strip():
        try:
            parsed = json.loads(s)
            if isinstance(parsed, (list, tuple)): return list(parsed)
        except Exception:
            return [t.strip() for t in s.split(",") if t.strip()]
    # 3) ê¸°ë³¸ê°’
    return list(default_list or [])

# ê¸°ë³¸ í¬í•¨/ì œì™¸ (ì›í•˜ë©´ ë°”ê¾¸ì„¸ìš”)
_KAPT_INC_RAW = _get_conf_list("KAPT_INCLUDE_KEYWORDS", "KAPT_INCLUDE_KEYWORDS",
                               ["ìŠ¹ê°•ê¸°", "led", "ë³€ì••ê¸°", "ì¸ë²„í„°", "íŒí”„", "/ì—˜ë¦¬ë² ì´í„°|ì¸ë²„í„°|ëª¨í„°|ì œì–´ë°˜/"])
_KAPT_EXC_RAW = _get_conf_list("KAPT_EXCLUDE_KEYWORDS", "KAPT_EXCLUDE_KEYWORDS",
                               ["ì¡°ê²½", "ì œì„¤", "/ë„ì¥|ì™¸ë²½/"])

def _compile_patterns(patterns):
    out = []
    for p in (patterns or []):
        s = str(p or "").strip()
        if not s: continue
        try:
            if len(s) >= 2 and s[0] == "/" and s[-1] == "/":
                out.append(("regex", re.compile(s[1:-1], re.IGNORECASE)))
            else:
                out.append(("text", s.lower()))
        except Exception:
            out.append(("text", s.lower()))
    return out

_INC_PAT = _compile_patterns(_KAPT_INC_RAW)
_EXC_PAT = _compile_patterns(_KAPT_EXC_RAW)

def _match_patterns(text: str, pats):
    if not pats: return False
    t = (text or "").lower()
    for kind, obj in pats:
        if kind == "text":
            if obj in t: return True
        else:
            if obj.search(t): return True
    return False

def _pass_keyword_filter(title: str, *extras: str) -> bool:
    """
    í¬í•¨ í‚¤ì›Œë“œê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì „ì²´ í¬í•¨.
    ì œì™¸ í‚¤ì›Œë“œëŠ” ì–´ë–¤ í•˜ë‚˜ë¼ë„ ë§¤ì¹˜ë˜ë©´ íƒˆë½.
    ì—¬ëŸ¬ í•„ë“œë¥¼ í•©ì³ ê²€ì‚¬(ê°€ë³€ ì¸ì).
    """
    cat = " ".join([title] + [e for e in extras if e]).strip()
    if _EXC_PAT and _match_patterns(cat, _EXC_PAT):
        return False
    if _INC_PAT:
        return _match_patterns(cat, _INC_PAT)
    return True


# ê²½ê³„(í† í°) ì¸ì‹: 'ëŒ€êµ¬'ëŠ” ì¡ê³  'í•´ìš´ëŒ€êµ¬'ëŠ” ì•ˆ ì¡ìŒ
_HANGUL_ALNUM = r"[0-9A-Za-zê°€-í£]"
def _contains_token(text: str, patterns: List[str]) -> bool:
    if not text:
        return False
    s = _norm_text(text)  # ì´ë¯¸ ìˆìœ¼ë‹ˆ ì¬ì‚¬ìš©
    for p in patterns:
        # ì•ë’¤ê°€ í•œê¸€/ì˜ë¬¸/ìˆ«ìê°€ ì•„ë‹ˆë©´ 'ë‹¨ì–´'ë¡œ ê°„ì£¼
        if re.search(rf"(?<!{_HANGUL_ALNUM}){re.escape(p)}(?!{_HANGUL_ALNUM})", s):
            return True
    return False


# [ADD] ë¡œê·¸ìš© ì‚¬ì—…ì†Œ ë¬¸ìì—´ í¬ë§·í„°
def _fmt_offices_for_log(val):
    if not val:
        return ""
    if isinstance(val, (list, tuple, set)):
        return ", ".join(str(x) for x in val if x)
    # ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° 'A/B' ë¥¼ 'A, B' ë¡œ ë³´ê¸° ì¢‹ê²Œ
    return str(val).replace("/", ", ")
# === apt_list.csv ìºì‹œ/ì¡°íšŒ (ìš°ì„ ìˆœìœ„ 1) ===
import csv


def _get_resource_path(relative_path):
    """
    PyInstallerë¡œ ë¹Œë“œëœ í™˜ê²½ì—ì„œ ë¦¬ì†ŒìŠ¤ ê²½ë¡œë¥¼ ì°¾ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    # PyInstallerë¡œ ë¹Œë“œëœ ê²½ìš°, sys._MEIPASSëŠ” ì„ì‹œ ì••ì¶• í•´ì œ ê²½ë¡œë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.
    try:
        base_path = sys._MEIPASS
    # ì¼ë°˜ íŒŒì´ì¬ í™˜ê²½ì¸ ê²½ìš°, í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    except Exception:
        base_path = os.path.abspath(os.path.dirname(__file__))

    return os.path.join(base_path, relative_path)

@lru_cache(maxsize=1)
def _load_apt_list_cache() -> dict:
    """
    apt_list.csvë¥¼ PyInstaller ë²ˆë“¤ ë˜ëŠ” ì¼ë°˜ ê²½ë¡œì—ì„œ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    path_to_use = _get_resource_path("apt_list.csv")
    if not os.path.isfile(path_to_use):
        print("[Warning] apt_list.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹¨ì§€ ì •ë³´ ë§¤í•‘ì´ ì œí•œë©ë‹ˆë‹¤.")
        return {}

    db = {}
    with open(path_to_use, "r", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for row in rdr:
            code = (row.get("kapt_code") or row.get("kaptCode") or "").strip()
            if not code:
                continue
            db[code] = {
                "address": (row.get("address") or row.get("addr") or "").strip(),
                "office": (row.get("office") or "").strip(),
                "bjd_code": (row.get("bjd_code") or row.get("bjdCode") or "").strip()
            }
    return db

def lookup_apt_by_code(kapt_code: str) -> tuple[str, str, str]:
    """
    (address, office, bjd_code) ì„¸íŠ¸ ë°˜í™˜.
    - apt_list.csv ìºì‹œì—ì„œ ì§ì ‘ ì¡°íšŒ
    - kapt_codeê°€ ì—†ê±°ë‚˜ ë§¤ì¹­ ì•ˆ ë˜ë©´ ("", "", "") ë°˜í™˜
    """
    if not kapt_code:
        return ("", "", "")

    m = _load_apt_list_cache()
    hit = m.get(kapt_code.strip())
    if not hit:
        return ("", "", "")

    return (
        hit.get("address", "").strip(),
        hit.get("office", "").strip(),
        hit.get("bjd_code", "").strip()
    )



def http_get_json(url: str, params: dict, *, retries: int = 3, timeout: int = 12, backoff: float = 0.8):
    """
    ì•ˆì „ JSON GET:
    - JSON ì•„ë‹Œ ì‘ë‹µ(ë¹ˆ ë¬¸ìì—´/HTML/XML)ì¼ ë•Œ None ë°˜í™˜
    - 5xx/429/íƒ€ì„ì•„ì›ƒì€ ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„
    - ì‘ë‹µ Content-Type ê²€ì‚¬ ë° ë³¸ë¬¸ ì„ í–‰ë¬¸ì ê²€ì‚¬
    - BOM/ì œë¡œí­ ë¬¸ì ì•ˆì „ íŒŒì‹±
    """
    last_err = None
    for attempt in range(1, retries + 1):
        try:
            r = SESSION.get(url, params=params, timeout=timeout)

            # ì¬ì‹œë„ ëŒ€ìƒ ìƒíƒœì½”ë“œ
            if r.status_code in (429, 500, 502, 503, 504):
                last_err = RuntimeError(f"HTTP {r.status_code}")
                raise last_err

            # 204 No Content ë“±
            if r.status_code == 204 or not r.text:
                return None

            # Content-Type ê¸°ë³¸ ì ê²€
            ctype = (r.headers.get("Content-Type") or "").lower()
            text = (r.text or "").lstrip()

            # JSON í•©ë¦¬ì„± ì ê²€: content-type ë˜ëŠ” ì„ í–‰ ë¬¸ì
            looks_json = ("json" in ctype) or text.startswith("{") or text.startswith("[")
            if not looks_json:
                # K-APT/ë‚˜ë¼ì¥í„°ê°€ ê°€ë” HTML(ì ê²€/ì˜¤ë¥˜)ì„ ì£¼ëŠ” ì¼€ì´ìŠ¤ ë°©ì§€
                return None

            # --- BOM/ì œë¡œí­ ë¬¸ì ì•ˆì „ íŒŒì‹± ---
            try:
                return r.json()
            except Exception:
                try:
                    # BOM ì œê±° ë””ì½”ë”©
                    txt = r.content.decode("utf-8-sig", errors="replace")
                    return json.loads(txt)
                except Exception:
                    # ì œì–´ë¬¸ì ì œê±° í›„ íŒŒì‹±
                    txt = (r.text or "")
                    # í”í•œ ë¬¸ì œë¬¸ì ì œê±°
                    for bad in ("\ufeff", "\u200b", "\u200c", "\u200d"):
                        txt = txt.replace(bad, "")
                    txt = txt.strip()
                    return json.loads(txt)

        except Exception as e:
            last_err = e
            if attempt < retries:
                # ì§€ìˆ˜ ë°±ì˜¤í”„
                time.sleep(backoff ** attempt)
                continue
            # ë§ˆì§€ë§‰ ì‹œë„ ì‹¤íŒ¨
            return None
    # ë…¼ë¦¬ì ìœ¼ë¡œ ì—¬ê¸° ì˜¤ì§€ ì•Šì§€ë§Œ, ì•ˆì „ë§
    return None

def to_ymd(s: Optional[str]) -> str:
    if not s:
        return ""
    s = str(s).strip()
    if len(s) >= 8 and s[:8].isdigit():
        return f"{s[:4]}-{s[4:6]}-{s[6:8]}"
    # 2025-08-12T10:20:00 â†’ 2025-08-12
    return s.split("T")[0].split()[0]

def _as_dict(x):
    if isinstance(x, dict):
        return x
    if isinstance(x, list) and x:
        return x[0]
    return {}



# =================================================================
# [ì¶”ê°€ ê¸°ëŠ¥] KEA ê³ íš¨ìœ¨ì—ë„ˆì§€ê¸°ìì¬ ì¸ì¦ì •ë³´ API ì¡°íšŒ
# =================================================================

import xml.etree.ElementTree as ET

# KEA API ì—”ë“œí¬ì¸íŠ¸
KEA_API_URL = "http://apis.data.go.kr/B553530/CRTIF/CRITF_01_LIST"

def _normalize_model(model: str) -> str:
    """ëª¨ë¸ëª…ì„ API ì¡°íšŒì— ì í•©í•˜ê²Œ ì •ê·œí™”í•©ë‹ˆë‹¤."""
    if not model:
        return ""
    return model.strip()

@lru_cache(maxsize=4096)
def kea_has_model_cached(model: str) -> bool | None:
    """
    kea_has_model í•¨ìˆ˜ì˜ ê²°ê³¼ë¥¼ ìºì‹œí•˜ì—¬ ì¤‘ë³µ API í˜¸ì¶œì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    return kea_has_model(model)

def kea_has_model(model: str) -> bool | None:
    """
    KEA APIì—ì„œ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    - True: ì¸ì¦ ëª¨ë¸ ì¡´ì¬
    - False: ì¡°íšŒí–ˆì§€ë§Œ ì—†ìŒ
    - None: API ì˜¤ë¥˜ë¡œ íŒì • ë¶ˆê°€
    """
    model_q = _normalize_model(model)
    if not model_q or "ì—†ìŒ" in model_q or "í•„ìš”" in model_q:
        return None

    params = {
        "serviceKey":_cfg("KEA_SERVICE_KEY"), # config.pyì— KEA_SERVICE_KEYê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        "pageNo": 1,
        "numOfRows": 10,
        "apiType": "json",
        "q2": model_q  # ëª¨ë¸ëª… ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
    }

    try:
        response = SESSION.get(KEA_API_URL, params=params, timeout=12)
        response.raise_for_status()
        data = response.json()
        total_count = data.get('totalCount')
        if total_count is None and 'response' in data:
            body = data.get('response', {}).get('body', {})
            if body:
                total_count = body.get('totalCount', 0)
        if isinstance(total_count, str) and total_count.isdigit():
            total_count = int(total_count)
        if isinstance(total_count, int):
            return total_count > 0
    except requests.exceptions.RequestException as e:
        print(f"  [KEA API Error] API í˜¸ì¶œ ì‹¤íŒ¨ (ëª¨ë¸: {model_q}): {e}")
    except (ValueError, KeyError) as e:
        print(f"  [KEA API Error] ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨ (ëª¨ë¸: {model_q}): {e}")
    return None


def kea_check_certification(model: str) -> str:
    """
    ì‹¤ì œ ì¸ì¦ì—¬ë¶€ íŒì •:
    - ì¸ì¦(O)
    - ë¯¸ì¸ì¦(X)
    - íŒì •ë¶ˆê°€(í™•ì¸í•„ìš”)
    """
    res = kea_has_model_cached(model)
    if res is True:
        return "O"
    elif res is False:
        return "X"
    return "í™•ì¸í•„ìš”"

def kea_cert_with_similarity(model: str) -> str:
    """
    KEA API í˜¸ì¶œ í›„, ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ì™€ ëª¨ë¸ëª…ì„ ìœ ì‚¬ë„ ë¹„êµ.
    ìœ ì‚¬ë„ 0.75 ì´ìƒì´ë©´ ì¸ì¦ ì²˜ë¦¬.
    """
    model_q = _normalize_model(model)
    if not model_q:
        return "í™•ì¸í•„ìš”"

    try:
        params = {
            "serviceKey": _cfg("KEA_SERVICE_KEY"),
            "pageNo": 1,
            "numOfRows": 50,
            "apiType": "json",
            "q2": model_q
        }
        r = SESSION.get(KEA_API_URL, params=params, timeout=10)
        r.raise_for_status()
        data = r.json()

        # KEA JSON êµ¬ì¡° íŒŒì•…
        items = []
        if "items" in data:
            items = data["items"]
        elif "response" in data:
            body = data["response"].get("body", {})
            items = body.get("items", body.get("item", []))

        if not isinstance(items, list):
            items = [items]

        # ìœ ì‚¬ë„ ê¸°ë°˜ ì¸ì¦ íŒì •
        for it in items:
            kea_model = it.get("mdlpNm") or it.get("modelNm") or ""
            if not kea_model:
                continue

            sim = model_similarity(model, kea_model)
            if sim >= 0.75:   # â˜… ìœ ì‚¬ë„ ê¸°ì¤€ (ë³‘í›ˆë‹˜ í•„ìš” ì‹œ ì¡°ì • ê°€ëŠ¥)
                return "O"   # ì¸ì¦ëœ ê²ƒìœ¼ë¡œ íŒë‹¨
                
        # APIëŠ” ì‘ë‹µí–ˆìœ¼ë‚˜ ìœ ì‚¬ë„ê°€ ë‚®ì•„ ì¸ì¦ ì•„ë‹˜
        return "X"

    except Exception as e:
        print(f"[KEA Similarity Error] ëª¨ë¸:{model}, err:{e}")
        return "í™•ì¸í•„ìš”"


# =========================
# DB
# =========================
Session = sessionmaker(bind=engine)
session = Session()

def _upsert_with_target(n: dict, conflict_cols: List[str]):
    """ì§€ì •í•œ conflict íƒ€ê²Ÿìœ¼ë¡œ upsert ì‹œë„"""
    stmt = sqlite_insert(Notice).values(**n)
    update_columns = {c.name: c for c in stmt.excluded
                      if c.name not in ("id", "detail_link", "model_name")}
    stmt = stmt.on_conflict_do_update(
        index_elements=conflict_cols,
        set_=update_columns
    )
    session.execute(stmt)

# [ìˆ˜ì •] DB ì €ì¥ ë¡œì§: source_systemì„ í¬í•¨í•˜ë„ë¡ upsert_notice ìˆ˜ì •
def upsert_notice(n: dict):
    """
    - source_systemì„ í¬í•¨í•œ ë³µí•©í‚¤ë¡œ upsert ì‹œë„
    """
    try:
        # `database.py`ì—ì„œ UniqueConstraintê°€ ë³€ê²½ë˜ì—ˆìœ¼ë¯€ë¡œ, ê·¸ì— ë§ëŠ” ì»¬ëŸ¼ ì‚¬ìš©
        _upsert_with_target(n, ["source_system", "detail_link", "model_name", "assigned_office"])
    except IntegrityError:
        session.rollback()
        # IntegrityErrorê°€ ë°œìƒí•˜ë©´ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê±°ë‚˜ ë‹¤ë¥¸ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        print(f"  [Warn] Upsert failed, possibly due to constraint violation: {n.get('detail_link')}")


# =========================
# í•„í„°(ê´€ì‹¬ë„)  â† ìš°ì„ ìˆœìœ„ 1ë²ˆ (ë¨¼ì € ê±°ë¦…ë‹ˆë‹¤) â€” ê°•í™”íŒ
# =========================

def _norm_text(*texts: str) -> str:
    """ê°„ë‹¨ ì •ê·œí™”: ì†Œë¬¸ì, ê³µë°± ì¶•ì•½, ê´„í˜¸/íŠ¹ìˆ˜ë¬¸ì ìµœì†Œ ì œê±°"""
    
    s = " ".join((t or "") for t in texts).lower()
    s = re.sub(r"[\(\)\[\]{}<>]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# í‘œê¸°/ë™ì˜ì–´ í¬í•¨ (ì†Œë¬¸ì ë¹„êµ ê¸°ì¤€)
DEVICE_KEYWORDS = [
    # ì¡°ëª…/LED
    "led", "ì—˜ì´ë””", "ë°œê´‘ë‹¤ì´ì˜¤ë“œ", "ì¡°ëª…", "ê°€ë¡œë“±", "ë³´ì•ˆë“±", "í„°ë„ë“±", "ìŠ¤ë§ˆíŠ¸ LED", "ìŠ¤ë§ˆíŠ¸LED",
    # íšŒì „/ë™ë ¥
    "ëª¨í„°", "ì „ë™ê¸°", "íŒí”„", "ë¸”ë¡œì›Œ", "íŒ¬", "ì—ì–´ë“œë¼ì´ì–´", "pcm",
    # ê¸°íƒ€
    "íˆíŠ¸íŒí”„", "ëƒ‰ë™ê¸°", "í„°ë³´ì••ì¶•ê¸°", "ê¹€ê±´ì¡°ê¸°",
    # ì „ë ¥/ì œì–´
    "ë³€ì••ê¸°", "íŠ¸ëœìŠ¤", "ì¸ë²„í„°", "ì¸ë²„í„° ì œì–´í˜•",
    # ì„¤ë¹„/ê¸°ê³„
    "ê³µê¸°ì••ì¶•ê¸°", "ì‚¬ì¶œì„±í˜•ê¸°",
    # ìˆ˜ì†¡/ìŠ¹ê°•
    "ìŠ¹ê°•ê¸°", "ì—˜ë¦¬ë² ì´í„°"
]

IMPROVEMENT_KEYWORDS = [
    "ë³´ìˆ˜", "ê°œì„ ", "ì„±ëŠ¥ê°œì„ ", "íš¨ìœ¨ê°œì„ ", "ê°œì²´", "êµì²´",
    "ì •ë¹„", "ê°œëŸ‰", "ë¦¬ëª¨ë¸ë§", "ê°œë³´ìˆ˜", "ë…¸í›„êµì²´", "ì—…ê·¸ë ˆì´ë“œ",
]

ENERGY_PROGRAM_KEYWORDS = [
    "ê³ íš¨ìœ¨", "ì—ë„ˆì§€ì ˆê°", "íš¨ìœ¨í–¥ìƒ", "ì—ë„ˆì§€ì ˆì•½", "ì „ë ¥ê¸°ê¸ˆ",
    "ì§€ì›ì‚¬ì—…", "ë³´ì¡°ê¸ˆ", "ì •ë¶€ì§€ì›", "íš¨ìœ¨ë“±ê¸‰", "ì—ë„ˆì§€ì´ìš©í•©ë¦¬í™”"
]

# ë¬´ê´€/ì œì™¸ (ê°•í•œ ì œì™¸)
HARD_DENY_KEYWORDS = [
    "ì¸ë ¥", "íŒŒê²¬", "ìš©ì—­", "êµìœ¡ìš©ì—­", "ì»¨ì„¤íŒ…", "ìœ„íƒìš´ì˜", "ì„ì°¨", "ìœ„íƒêµìœ¡", "êµìœ¡í›ˆë ¨",
    "ê¸‰ì‹", "ì¸ì‡„", "ì†Œí”„íŠ¸ì›¨ì–´", "ìœ ì§€ë³´ìˆ˜", "í† ëª©", "ê±´ì¶•", "ì¡°ê²½", "ë„ë¡œ", "í´ë¼ìš°ë“œ", "ë¹„í’ˆ", "ì‚¬ë¬´ìš©í’ˆ",
    "ì‚¬ë¬´ê°€êµ¬", "ë¹„í’ˆêµ¬ë§¤", "ë¬¸êµ¬ë¥˜", "ì˜ë£Œì†Œëª¨í’ˆ", "ì‹ìì¬", "ì„¸íƒë¬¼", "ì²­ì†Œìš©ì—­", "í•´ìš´ëŒ€êµ¬", "ì „ê´‘íŒ", "ì €ì†Œë“ì¸µ",
    "ì‹ ì¶•", "íš¡ë‹¨ë³´ë„", "íƒœì–‘ê´‘", "ë²½ì‹œê³„", "ëª¨ë‹ˆí„°", "ë¬´ë“œë“±", "ì—°í•„ê½‚ì´", "êµí†µì‹ í˜¸ê¸°", "í•´ìš´ëŒ€", "OAê¸°ê¸°", "ì·¨ì•½ê³„ì¸µ", 
    # â–¼â–¼â–¼ [ìˆ˜ì •] ëŒ€êµ¬ë³¸ë¶€ ê´€í•  ì™¸ ê²½ë¶ ì§€ì—­ì„ ì—¬ê¸°ì— ì¶”ê°€í•˜ì—¬ ì¦‰ì‹œ ì œì™¸ â–¼â–¼â–¼
    "ì•ˆë™", "ìƒì£¼", "ë¬¸ê²½", "ì˜ì„±", "ì˜ˆì²œ", "ì˜ì£¼",
    "ë´‰í™”", "ì²­ì†¡", "ì˜ì–‘", "ìš¸ì§„", "êµ¬ë¯¸", "êµ°ìœ„", "ìš¸ë¦‰", "ê¸°ê´€ëª… ì—†ìŒ"
]

# ê¶Œì—­ íŒë³„(í…ìŠ¤íŠ¸ì— ë‹¤ë¥¸ ê´‘ì—­ê¶Œë§Œ ë¶„ëª…íˆ ë‚˜ì˜¤ë©´ ì»·)
TARGET_REGION_KEYWORDS = [
    "ëŒ€êµ¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ", "ê²½ë¶", "ê²½ìƒë¶ë„",
    "í¬í•­", "ê²½ì£¼", "ê²½ì‚°", "ê¹€ì²œ", "ì˜ì²œ", "ì¹ ê³¡", "ì„±ì£¼", "ì²­ë„", "ê³ ë ¹", "ì˜ë•"
]
OTHER_REGION_KEYWORDS = [
    # ìˆ˜ë„ê¶Œ
    "ì„œìš¸", "ê°•ë‚¨", "ê°•ë™", "ê°•ë¶", "ê°•ì„œ", "ê´€ì•…", "ê´‘ì§„", "êµ¬ë¡œ", "ê¸ˆì²œ",
    "ë…¸ì›", "ë„ë´‰", "ë™ëŒ€ë¬¸", "ë™ì‘", "ë§ˆí¬", "ì„œëŒ€ë¬¸", "ì„œì´ˆ", "ì„±ë™",
    "ì„±ë¶", "ì†¡íŒŒ", "ì–‘ì²œ", "ì˜ë“±í¬", "ìš©ì‚°", "ì€í‰", "ì¢…ë¡œ", "ì¤‘ë‘",
    "ê²½ê¸°", "ìˆ˜ì›", "ì„±ë‚¨", "ê³ ì–‘", "ìš©ì¸", "ë¶€ì²œ", "ì•ˆì‚°", "ì•ˆì–‘",
    "ë‚¨ì–‘ì£¼", "í™”ì„±", "í‰íƒ", "ì˜ì •ë¶€", "ì‹œí¥", "íŒŒì£¼", "ê¹€í¬", "ê´‘ëª…",
    "ê´‘ì£¼", "êµ°í¬", "ì´ì²œì‹œ", "ì˜¤ì‚°", "ì•ˆì„±", "í•˜ë‚¨", "ì˜ì™•", "ì–‘ì£¼", 
    "í¬ì²œ", "ì—¬ì£¼", "ì–‘í‰", "ê°€í‰", "ì—°ì²œ",
    "ì¸ì²œ", "ê³„ì–‘", "ë‚¨ë™", "ë¯¸ì¶”í™€", "ë¶€í‰", "ì—°ìˆ˜", "ê°•í™”", "ì˜¹ì§„", "ê²½ê¸°ë„",

    # ê°•ì›ê¶Œ
    "ê°•ì›", "ì¶˜ì²œ", "ì›ì£¼", "ê°•ë¦‰", "ë™í•´", "íƒœë°±", "ì†ì´ˆ", "ì‚¼ì²™", "ê°•ì›ë„",
    "í™ì²œ", "íš¡ì„±", "ì˜ì›”", "í‰ì°½", "ì •ì„ ", "ì² ì›", "í™”ì²œ", "ì–‘êµ¬",
    "ì¸ì œ", "ê³ ì„±", "ì–‘ì–‘",

    # ì¶©ì²­ê¶Œ
    "ì¶©ë¶", "ì²­ì£¼", "ì¶©ì£¼", "ì œì²œ", "ë³´ì€", "ì˜¥ì²œ", "ì˜ë™", "ì¶©ì²­ë¶ë„", "ì¶©ì²­ë‚¨ë„", 
    "ê´´ì‚°", "ìŒì„±", "ë‹¨ì–‘", "ì¶©ë‚¨",
    "ì¶©ë‚¨", "ì²œì•ˆ", "ê³µì£¼", "ë³´ë ¹", "ì•„ì‚°", "ì„œì‚°", "ë…¼ì‚°", "ê³„ë£¡",
    "ë‹¹ì§„", "ê¸ˆì‚°", "ë¶€ì—¬", "ì„œì²œ", "ì²­ì–‘", "í™ì„±", "ì˜ˆì‚°", "íƒœì•ˆ",
    "ëŒ€ì „", "ìœ ì„±", "ëŒ€ë•", "ì„¸ì¢…",

    # ì „ë¼ê¶Œ
    "ì „ë¶", "ì „ì£¼", "êµ°ì‚°", "ìµì‚°", "ì •ì", "ë‚¨ì›", "ê¹€ì œ", "ì „ë¼ë¶ë„", "ì „ë¼ë‚¨ë„",
    "ì™„ì£¼", "ì§„ì•ˆ", "ë¬´ì£¼", "ì¥ìˆ˜", "ì„ì‹¤", "ìˆœì°½", "ê³ ì°½", "ë¶€ì•ˆ",
    "ì „ë‚¨", "ëª©í¬", "ì—¬ìˆ˜", "ìˆœì²œ", "ë‚˜ì£¼", "ê´‘ì–‘",
    "ë‹´ì–‘", "ê³¡ì„±", "êµ¬ë¡€", "ê³ í¥", "ë³´ì„±", "í™”ìˆœ", "ì¥í¥", "ê°•ì§„",
    "í•´ë‚¨", "ì˜ì•”", "ë¬´ì•ˆ", "í•¨í‰", "ì˜ê´‘", "ì¥ì„±", "ì™„ë„", "ì§„ë„", "ì‹ ì•ˆ",
    "ê´‘ì£¼", "ê´‘ì‚°",

    # ê²½ìƒê¶Œ (ëŒ€êµ¬ë³¸ë¶€ ì œì™¸)
    "ë¶€ì‚°", "ì˜ë„", "ë¶€ì‚°ì§„", "ë™ë˜", "í•´ìš´ëŒ€", "ì‚¬í•˜", "ê¸ˆì •", "ê²½ìƒë‚¨ë„",
    "ì—°ì œ", "ì‚¬ìƒ", "ê¸°ì¥",
    "ìš¸ì‚°", "ìš¸ì£¼",
    "ê²½ë‚¨", "ì°½ì›", "ê¹€í•´", "ì–‘ì‚°", "ì§„ì£¼", "ê±°ì œ", "í†µì˜",
    "ì‚¬ì²œ", "ë°€ì–‘", "í•¨ì•ˆ", "ì°½ë…•", "ê³ ì„±", "ë‚¨í•´", "í•˜ë™",
    "ì‚°ì²­", "í•¨ì–‘", "ê±°ì°½", "í•©ì²œ",


    # ì œì£¼ê¶Œ
    "ì œì£¼", "ì„œê·€í¬",
]


def is_relevant_text(*texts: str) -> bool:
    """
    ê°•í™”ëœ ê´€ì‹¬ ê³µê³  í•„í„°:
      1) êµ°ìœ„/í•˜ë“œê±°ì ˆ ë‹¨ì–´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì œì™¸
      2) íƒ€ ê¶Œì—­ í‚¤ì›Œë“œê°€ ëª…ì‹œë¼ ìˆê³ , íƒ€ê¹ƒ ê¶Œì—­ì´ ì—†ìœ¼ë©´ ì œì™¸
      3) ìŠ¤ì½”ì–´ >= 2ë§Œ í†µê³¼ (ì¥ë¹„=2 / ì—ë„ˆì§€=1 / ê°œì„ =1)
         - ì¥ë¹„ í‚¤ì›Œë“œ 1ê°œë§Œ ìˆì–´ë„ í†µê³¼ (2ì )
         - ê°œì„ (1)+ì—ë„ˆì§€(1) ì¡°í•©ë„ í†µê³¼ (2ì )
    """
    s = _norm_text(*texts)

    # 1) ìµœìš°ì„  ì œì™¸
    if any(k in s for k in (kw.lower() for kw in HARD_DENY_KEYWORDS)):
        return False

    # 2) ì§€ì—­ ì˜¤íƒ ì»· (ë‹¤ë¥¸ ê¶Œì—­ + ìš°ë¦¬ ê¶Œì—­ ë¶€ì¬)
    has_other = any(k in s for k in (kw.lower() for kw in OTHER_REGION_KEYWORDS))
    has_target = any(k in s for k in (kw.lower() for kw in TARGET_REGION_KEYWORDS))
    if has_other and not has_target:
        return False

    # 3) ê°€ì¤‘ì¹˜ ìŠ¤ì½”ì–´
    score = 0
    if any(k in s for k in (kw.lower() for kw in DEVICE_KEYWORDS)):
        score += 2
    if any(k in s for k in (kw.lower() for kw in ENERGY_PROGRAM_KEYWORDS)):
        score += 1
    if any(k in s for k in (kw.lower() for kw in IMPROVEMENT_KEYWORDS)):
        score += 1

    return score >= 2

def _safe_hint_match(text: str, hint_key: str) -> bool:
    """
    CLIENT_HINTS í‚¤ì›Œë“œê°€ í…ìŠ¤íŠ¸ì— ìˆì„ ë•Œ, ë¶ˆí•„ìš”í•œ ì „êµ­ ì˜¤íƒì„ ì¤„ì´ê¸° ìœ„í•œ ê°€ë“œ.
    - 'êµ°ìœ„' í¬í•¨ ì‹œ ë¬´ì¡°ê±´ ë¶ˆê°€
    - ë„ˆë¬´ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œëŠ” 'ëŒ€êµ¬/ê²½ë¶/í¬í•­' ì¤‘ í•˜ë‚˜ì˜ ë§¥ë½ ë˜ëŠ”
      ì§€ëª…(êµ¬/êµ°/ì‹œ/ì/ë©´/ë™) ë™ë°˜ ì‹œë§Œ í—ˆìš©í•˜ë„ë¡ í™•ì¥ ê°€ëŠ¥.
    """
    s = _norm_text(text)
    if "êµ°ìœ„" in s:
        return False

    # ì˜ˆ) 'ì¤‘êµ¬ì²­', 'ë¶êµ¬ì²­' ê°™ì€ ì „êµ­ ì¼ë°˜ì–´ëŠ” CLIENT_HINTSì— ë„£ì§€ ì•Šì•˜ê³ ,
    #     ë„£ë”ë¼ë„ ì•„ë˜ì²˜ëŸ¼ 'ëŒ€êµ¬ ' ì ‘ë‘ê°€ ì—†ëŠ” ê²½ìš° ì»· ê°€ëŠ¥ (í•„ìš” ì‹œ í™•ì¥)
    general_local_terms = ["ì¤‘êµ¬ì²­", "ë¶êµ¬ì²­", "ë‚¨êµ¬ì²­", "ì„œêµ¬ì²­"]
    if any(g in hint_key for g in general_local_terms):
        # CLIENT_HINTSì—ëŠ” 'ëŒ€êµ¬ ì¤‘êµ¬ì²­'ì²˜ëŸ¼ ë„£ì–´ë‘ì—ˆìœ¼ë¯€ë¡œ ì—¬ê¸´ ì‚¬ì‹¤ìƒ íŒ¨ìŠ¤
        return False

    # 'ëŒ€êµ¬/ê²½ë¶/í¬í•­' ë§¥ë½ ì²´í¬ (ê³¼ë„ ì»· ë°©ì§€ ìœ„í•´ ì™„ì „ í•˜ë“œ ì¡°ê±´ì€ ì•„ë‹˜. í•„ìš”ì‹œ ê°•í™”)
    has_context = any(k in s for k in ["ëŒ€êµ¬", "ê²½ë¶", "ê²½ìƒë¶ë„", "í¬í•­", "ê²½ì£¼", "ê²½ì‚°", "ê¹€ì²œ", "ì˜ì²œ", "ì¹ ê³¡", "ì„±ì£¼", "ì²­ë„", "ê³ ë ¹", "ì˜ë•"])
    # ë„ˆë¬´ ì¼ë°˜ì ì¸ ê¸°ì—…/ê¸°ê´€ëª… í‚¤ì›Œë“œê°€ ìƒê¸¸ ê²½ìš°, ë§¥ë½ ì—†ìœ¼ë©´ ì»·í•˜ë„ë¡ ì¶”ê°€
    # if hint_key in ["í™˜ê²½ê³µë‹¨", "ì‹œì„¤ê³µë‹¨", "ë³¸ì‚¬", "ë³¸ë¶€"] and not has_context:
    #     return False

    return True


# =========================
# íŠ¹ìˆ˜ê¶Œì—­/ì§€ì‚¬ í›„ë³´ ì •ì˜
# =========================
DAEGU_OFFICES      = ["ì§í• ", "ë™ëŒ€êµ¬ì§€ì‚¬", "ì„œëŒ€êµ¬ì§€ì‚¬", "ë‚¨ëŒ€êµ¬ì§€ì‚¬"]  # ëŒ€êµ¬ê¶Œ ì „ì²´ ì§€ì‚¬
GYONGBUK_OFFICES   = ["í¬í•­ì§€ì‚¬", "ë¶í¬í•­ì§€ì‚¬", "ê²½ì£¼ì§€ì‚¬", "ê²½ì‚°ì§€ì‚¬", "ê¹€ì²œì§€ì‚¬", "ì˜ì²œì§€ì‚¬", "ì¹ ê³¡ì§€ì‚¬", "ì„±ì£¼ì§€ì‚¬", "ì²­ë„ì§€ì‚¬", "ê³ ë ¹ì§€ì‚¬", "ì˜ë•ì§€ì‚¬"]

# ë‹¬ì„œêµ¬/ë‹¬ì„±êµ°/í¬í•­ì‹œ ë¶êµ¬ = íŠ¹ìˆ˜ê¶Œì—­ (êµ¬ê¹Œì§€ë§Œ ë‚˜ì˜¤ë©´ ë³µìˆ˜ í›„ë³´)
SPECIAL_GU_PATTERNS = [
    # (ì •ê·œì‹, í›„ë³´ ì§€ì‚¬ 2ê°œ, 'ì „ì²´'ì—ì„œ í•˜ë‚˜ë§Œ ëŒ€í‘œë¡œ ì“°ëŠ” ê¸°ë³¸ì§€ì‚¬)
    (re.compile(r"(ëŒ€êµ¬ê´‘ì—­ì‹œ\s*)?ë‹¬ì„œêµ¬"), ["ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ì„œëŒ€êµ¬ì§€ì‚¬"], "ë‚¨ëŒ€êµ¬ì§€ì‚¬"),
    (re.compile(r"(ëŒ€êµ¬ê´‘ì—­ì‹œ\s*)?ë‹¬ì„±êµ°"), ["ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ë™ëŒ€êµ¬ì§€ì‚¬"], "ë‚¨ëŒ€êµ¬ì§€ì‚¬"),
    (re.compile(r"(ê²½ìƒë¶ë„\s*)?í¬í•­ì‹œ\s*ë¶êµ¬"), ["í¬í•­ì§€ì‚¬", "ë¶í¬í•­ì§€ì‚¬"], "í¬í•­ì§€ì‚¬"),
]

# ë™/ì/ë©´/ë¡œ/ê¸¸ ë ˆë²¨ ì‹ë³„
def has_dong_level(addr: str) -> bool:
    if not addr:
        return False
    return any(t in addr for t in ("ë™", "ì", "ë©´", "ë¡œ", "ê¸¸"))

def _decorate_candidates_in_addr(addr: str, a: str, b: str) -> str:
    """ì£¼ì†Œ í‘œì‹œëŠ” 'ì›ì£¼ì†Œ\n(A/B)' í˜•íƒœë¡œ"""
    addr = (addr or "").strip()
    if not addr:
        return f"ê´€í• ì§€ì‚¬ í™•ì¸ í•„ìš”\n({a}/{b})"
    return f"{addr}\n({a}/{b})"

def _special_gu_offices_if_match(addr: str) -> Optional[List[str]]:
    for pat, candidates, _default in SPECIAL_GU_PATTERNS:
        if pat.search(addr):
            return candidates
    return None

def _assign_office_by_addr(addr: str) -> Optional[str]:
    """assign_offices_by_address ê²°ê³¼ë¥¼ ë‹¨ì¼ ì§€ì‚¬ë¡œ ì¶•ì•½í•´ ë°˜í™˜"""
    offices = assign_offices_by_address(addr)
    if len(offices) == 1:
        return offices[0]
    return None

def decorate_address_with_candidates(addr: str, offices: List[str]) -> str:
    if len(offices) >= 2:
        return _decorate_candidates_in_addr(addr, offices[0], offices[1])
    return addr or ""

import re
from typing import List

def assign_offices_by_address(addr: str) -> List[str]:
    """
    ì£¼ì†Œë§Œìœ¼ë¡œ ì§€ì‚¬ í›„ë³´ë¥¼ ê²°ì •.
    - ë™/ì/ë©´ê¹Œì§€ ë‚˜ì˜¤ë©´ ë‹¨ì¼ ì§€ì‚¬ ì„¸ë¶„í™”
    - ë‹¬ì„œêµ¬/ë‹¬ì„±êµ°/í¬í•­ì‹œ ë¶êµ¬ëŠ” 'êµ¬'ê¹Œì§€ë§Œ ë‚˜ì˜¤ë©´ 2 í›„ë³´ ë°˜í™˜ ê°€ëŠ¥
    - í¬í•­(ë¶êµ¬ ì œì™¸)ì€ í¬í•­ì§€ì‚¬
    - ë§¤ì¹­ ë¶ˆê°€ ì‹œ [] ë°˜í™˜(ìƒìœ„ ë‹¨ê³„ì—ì„œ íŒíŠ¸/í›„ì† ì²˜ë¦¬)
    """
    if not addr:
        return []

    s = re.sub(r"\s+", "", addr)  # ê³µë°± ì œê±°ë³¸

    # â”€â”€ 1) ëŒ€êµ¬ ê¶Œì—­(ìš°ì„ ) â”€â”€
    if "ëŒ€êµ¬ê´‘ì—­ì‹œ" in s or s.startswith("ëŒ€êµ¬"):
        if has_dong_level(addr):
            if re.search(r"ëŒ€êµ¬ê´‘ì—­ì‹œ(ì¤‘êµ¬|ë¶êµ¬)", s):
                return ["ì§í• "]
            if re.search(r"ëŒ€êµ¬ê´‘ì—­ì‹œ(ë™êµ¬|ìˆ˜ì„±êµ¬)", s):
                return ["ë™ëŒ€êµ¬ì§€ì‚¬"]
            if re.search(r"ëŒ€êµ¬ê´‘ì—­ì‹œ(ì„œêµ¬|ë‚¨êµ¬)", s):
                return ["ì„œëŒ€êµ¬ì§€ì‚¬"]
            if "ëŒ€êµ¬ê´‘ì—­ì‹œë‹¬ì„œêµ¬" in s:
                if re.search(r"(ê°ì‚¼ë™|ë‘ë¥˜ë™|ë³¸ë¦¬ë™|ì„±ë‹¹ë™|ì£½ì „ë™)", addr):
                    return ["ì„œëŒ€êµ¬ì§€ì‚¬"]
                return ["ë‚¨ëŒ€êµ¬ì§€ì‚¬"]
            if "ëŒ€êµ¬ê´‘ì—­ì‹œë‹¬ì„±êµ°" in s:
                if re.search(r"(ë‹¤ì‚¬ì|í•˜ë¹ˆë©´)", addr):
                    return ["ì„œëŒ€êµ¬ì§€ì‚¬"]
                if "ê°€ì°½ë©´" in s:
                    return ["ë™ëŒ€êµ¬ì§€ì‚¬"]
                return ["ë‚¨ëŒ€êµ¬ì§€ì‚¬"]
            # ëŒ€êµ¬ëŠ” ë§ì§€ë§Œ êµ¬/êµ° íŒë… ë¶ˆê°€ â†’ ìƒìœ„ ì²˜ë¦¬
        else:
            if re.search(r"ëŒ€êµ¬ê´‘ì—­ì‹œ(ì¤‘êµ¬|ë¶êµ¬)", s):
                return ["ì§í• "]
            if re.search(r"ëŒ€êµ¬ê´‘ì—­ì‹œ(ë™êµ¬|ìˆ˜ì„±êµ¬)", s):
                return ["ë™ëŒ€êµ¬ì§€ì‚¬"]
            if re.search(r"ëŒ€êµ¬ê´‘ì—­ì‹œ(ì„œêµ¬|ë‚¨êµ¬)", s):
                return ["ì„œëŒ€êµ¬ì§€ì‚¬"]
            if "ëŒ€êµ¬ê´‘ì—­ì‹œë‹¬ì„œêµ¬" in s or "ëŒ€êµ¬ê´‘ì—­ì‹œë‹¬ì„±êµ°" in s:
                # í•„ìš” ì‹œ SPECIAL_GU_PATTERNSë¡œ í›„ë³´ 2ê°œ ì²˜ë¦¬
                return []

    # â”€â”€ 2) í¬í•­ ê¶Œì—­ â”€â”€
    if "í¬í•­ì‹œ" in s or s.startswith("í¬í•­"):
        if "í¬í•­ì‹œë¶êµ¬" in s:
            if has_dong_level(addr):
                if re.search(r"(í¥í•´|ì†¡ë¼|ì‹ ê´‘|ì²­í•˜|ê¸°ê³„|ê¸°ë¶|ì£½ì¥)", addr):
                    return ["ë¶í¬í•­ì§€ì‚¬"]
                return ["í¬í•­ì§€ì‚¬"]
            return ["í¬í•­ì§€ì‚¬", "ë¶í¬í•­ì§€ì‚¬"]
        return ["í¬í•­ì§€ì‚¬"]

    # â”€â”€ 3) íŠ¹ìˆ˜ íŒ¨í„´(êµ¬ê¹Œì§€ë§Œ ë“±) â”€â”€
    for pat, candidates, _default in SPECIAL_GU_PATTERNS:
        if pat.search(addr):
            return candidates

    # â”€â”€ 4) ê²½ë¶ê¶Œ(ê·¸ ì™¸ ì‹œêµ°) â”€â”€
    if "ê²½ì£¼ì‹œ" in addr:  return ["ê²½ì£¼ì§€ì‚¬"]
    if "ê²½ì‚°ì‹œ" in addr:  return ["ê²½ì‚°ì§€ì‚¬"]
    if "ê¹€ì²œì‹œ" in addr:  return ["ê¹€ì²œì§€ì‚¬"]
    if "ì˜ì²œì‹œ" in addr:  return ["ì˜ì²œì§€ì‚¬"]
    if "ì¹ ê³¡êµ°" in addr:  return ["ì¹ ê³¡ì§€ì‚¬"]
    if "ì„±ì£¼êµ°" in addr:  return ["ì„±ì£¼ì§€ì‚¬"]
    if "ì²­ë„êµ°" in addr:  return ["ì²­ë„ì§€ì‚¬"]
    if "ê³ ë ¹êµ°" in addr:  return ["ê³ ë ¹ì§€ì‚¬"]
    if "ì˜ë•êµ°" in addr:  return ["ì˜ë•ì§€ì‚¬"]

    # â”€â”€ 5) ê¸°íƒ€ â”€â”€
    if any(g in addr for g in ["ë™êµ¬", "ìˆ˜ì„±êµ¬"]):
        return ["ë™ëŒ€êµ¬ì§€ì‚¬"]
    if any(g in addr for g in ["ì„œêµ¬", "ë‚¨êµ¬"]):
        return ["ì„œëŒ€êµ¬ì§€ì‚¬"]
    if "ë‹¬ì„œêµ¬" in addr or "ë‹¬ì„±êµ°" in addr:
        return []

    return []

# =========================
# UsrInfo(ìƒì„¸ì£¼ì†Œ) & Mall(ì‹œêµ°êµ¬) ìš°ì„ ìˆœìœ„ ì„ íƒ
# =========================
def get_full_address_from_usr_info(dminstt_code: str) -> Optional[str]:
    """
    UsrInfoService.getDminsttInfo (ì½”ë“œ ê¸°ì¤€)
    - inqryDiv=2(ë³€ê²½ì¼ ê¸°ì¤€) + 12ê°œì›” ê¸°ê°„ í•„ìˆ˜
    - adrs + dtlAdrs â†’ ìƒì„¸ì£¼ì†Œ, ì—†ìœ¼ë©´ rgnNm fallback
    - ë‚´ë¶€ ìŠ¤ë¡œí‹€(120ms) + ë‹¨ìˆœ ìºì‹œ ì ìš©
    """
    if not dminstt_code:
        return None

    # --- ê°„ë‹¨ ìºì‹œ & ìŠ¤ë¡œí‹€(í•¨ìˆ˜ ì†ì„± ì‚¬ìš©, ì™¸ë¶€ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”) ---
    import time
    from datetime import datetime, timedelta

    if not hasattr(get_full_address_from_usr_info, "_cache"):
        get_full_address_from_usr_info._cache = {}  # type: ignore[attr-defined]
    if not hasattr(get_full_address_from_usr_info, "_last_call"):
        get_full_address_from_usr_info._last_call = 0.0  # type: ignore[attr-defined]

    _cache: dict = get_full_address_from_usr_info._cache  # type: ignore[attr-defined]
    _last_call: float = get_full_address_from_usr_info._last_call  # type: ignore[attr-defined]

    # ìºì‹œ ì¡°íšŒ(í”„ë¡œì„¸ìŠ¤ ìƒì¡´ ë™ì•ˆ ìœ ì§€)
    if dminstt_code in _cache:
        return _cache[dminstt_code]

    # ìŠ¤ë¡œí‹€: ìµœì†Œ 120ms ê°„ê²©
    wait = 0.12 - (time.time() - _last_call)
    if wait > 0:
        time.sleep(wait)

    # 12ê°œì›” ê¸°ê°„(ë¬¸ì„œ ì œí•œ ê³ ë ¤)
    end = datetime.now()
    start = end - timedelta(days=365)
    inqryBgnDt = start.strftime("%Y%m%d") + "0000"
    inqryEndDt = end.strftime("%Y%m%d") + "2359"

    params = {
        "ServiceKey": _cfg("NARA_SERVICE_KEY"),
        "type": "json",
        "inqryDiv": "2",                 # âœ… ë³€ê²½: ê¸°ê°„(ë³€ê²½ì¼) ê¸°ì¤€
        "inqryBgnDt": inqryBgnDt,        # âœ… 12ê°œì›” ë²”ìœ„ ì‹œì‘
        "inqryEndDt": inqryEndDt,        # âœ… 12ê°œì›” ë²”ìœ„ ë
        "dminsttCd": dminstt_code,       # ì½”ë“œ ê¸°ì¤€ ì¡°íšŒ
        "numOfRows": "1",
        "pageNo": "1",
    }

    try:
        data = http_get_json(api_url(USR_INFO_PATH), params)
        body = _as_dict(data.get("response", {}).get("body"))
        items = _as_items_list(body)
        if items:
            it = items[0]
            full = f"{it.get('adrs','')}".strip()
            dtl  = f"{it.get('dtlAdrs','')}".strip()
            text = (full + " " + dtl).strip() or it.get("rgnNm")
            # ìºì‹œì— ì ì¬
            _cache[dminstt_code] = text
            return text
    except Exception as e:
        print(f"  [Warn] ì‚¬ìš©ìì •ë³´ API ì‹¤íŒ¨: {dminstt_code} ({e})")
    finally:
        # ë§ˆì§€ë§‰ í˜¸ì¶œì‹œê° ê°±ì‹ 
        get_full_address_from_usr_info._last_call = time.time()  # type: ignore[attr-defined]

    return None


def parse_dminstt_code_from_complex(s: str) -> Tuple[Optional[str], Optional[str]]:
    """'[ì½”ë“œ^ì´ë¦„^ê¸°ê´€ëª…]|[...]' í˜•ì‹ì—ì„œ ì²« í•­ëª©ì˜ ì½”ë“œ/ëª… íŒŒì‹±"""
    if not s:
        return None, None
    try:
        first = s.strip("[]").split("],[")[0].strip("[]")
        parts = first.split("^")
        if len(parts) >= 3:
            return parts[1], parts[2]
    except Exception:
        pass
    return None, None
    
def guess_mall_addr(item: dict) -> Optional[str]:
    keys_try = [
        # ê³µí†µ/ë‚©í’ˆìš”êµ¬
        "insttAddr","dmndInsttAddr","dminsttAddr","adres","adrs","addr","adresCn",
        "lc","instNmAddr","insttAdres","dminsttAdres","dmndInsttAdres",
        "insttZipAddr","zipAdres",
        "dlvrReqInsttAddr","dlvrReqAddr","dlvrAddr","destAddr","delivAddr",

        # âœ… ê³„ì•½ì™„ë£Œ(ë¬¼í’ˆ)ì—ì„œ ìì£¼ ë³´ì´ëŠ” í‚¤
        "cntrctInsttAddr","cntrctInsttAdres","cntrctInsttZipAddr",
        "cntrctInsttRgnNm",
        "prchseInsttAddr","prchseInsttAdres","prchseInsttZipAddr",
        "prchseInsttRgnNm",
        "insttRgnNm","dminsttRgnNm",  # ì§€ì—­ëª…ë§Œ ë‚´ë ¤ì˜¤ëŠ” ê²½ìš°ë„ í‘œì‹œ
    ]
    for k in keys_try:
        v = item.get(k)
        if v and isinstance(v, str) and len(v.strip()) >= 2:
            return v.strip()
    return None


def _pick_addr_by_priority(client_code: Optional[str], mall_addr: Optional[str]) -> Tuple[str, str]:
    """
    ë°˜í™˜: (ì„ íƒì£¼ì†Œ, source)  # source: 'usr' | 'mall' | 'none'
    """
    usr_addr = get_full_address_from_usr_info(client_code) if client_code else None
    if usr_addr:
        return usr_addr.strip(), "usr"
    if mall_addr:
        return str(mall_addr).strip(), "mall"
    return "", "none"

# =========================
# ê¸°ê´€ëª… íŒíŠ¸ â†’ ì €ì¥(ê´€í• ë¶ˆëª…) íŒë‹¨
# (ì¤‘ë³µ/ì „êµ­ ì˜¤íƒ ë°©ì§€ë¥¼ ìœ„í•´ êµ¬ì²´í™”: 'ëŒ€êµ¬ ë™êµ¬ì²­' ë“± ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶€ì—¬)
# ë‹¬ì„œêµ¬ì²­ í‚¤ì›Œë“œëŠ” ì œê±°(íŠ¹ìˆ˜ê¶Œì—­ì€ ì£¼ì†Œ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬)
# =========================
CLIENT_HINTS = {
    # ========================
    # ëŒ€êµ¬ê´‘ì—­ì‹œ ì§í•  (ì¤‘êµ¬/ë¶êµ¬, ë³¸ë¶€ê¸‰)
    # ========================
    "ëŒ€êµ¬ê´‘ì—­ì‹œì²­": "ì§í• ", "ëŒ€êµ¬ì‹œì²­": "ì§í• ", "ëŒ€êµ¬ê´‘ì—­ì‹œ ë³¸ì²­": "ì§í• ",
    "ëŒ€êµ¬ ì¤‘êµ¬ì²­": "ì§í• ", "ëŒ€êµ¬ê´‘ì—­ì‹œ ì¤‘êµ¬": "ì§í• ",  # <-- ADD
    "ëŒ€êµ¬ ë¶êµ¬ì²­": "ì§í• ", "ëŒ€êµ¬ê´‘ì—­ì‹œ ë¶êµ¬": "ì§í• ",  # <-- ADD
    "ëŒ€êµ¬ ì¤‘êµ¬ì²­": "ì§í• ", "ëŒ€êµ¬ ë¶êµ¬ì²­": "ì§í• ",
    "í•œêµ­ì „ë ¥ê³µì‚¬ ëŒ€êµ¬ë³¸ë¶€": "ì§í• ", "í•œì „ ëŒ€êµ¬ë³¸ë¶€": "ì§í• ",
    "ëŒ€êµ¬í™˜ê²½ê³µë‹¨ ë³¸ë¶€": "ì§í• ", "ëŒ€êµ¬ì‹œì„¤ê³µë‹¨ ë³¸ì‚¬": "ì§í• ",
    "ì—‘ìŠ¤ì½”": "ì§í• ", "ì˜ì§„ì „ë¬¸ëŒ€": "ì§í• ", "ê²½ë¶ëŒ€í•™êµ": "ì§í• ", "ê²½ë¶ëŒ€ë³‘ì›": "ì§í• ",
    "ëŒ€êµ¬ì§€ë°©ê²½ì°°ì²­": "ì§í• ", "ëŒ€êµ¬ë¶ë¶€ê²½ì°°ì„œ": "ì§í• ", "ê²½ë¶ëŒ€í•™êµ ê³µê³¼ëŒ€í•™": "ì§í• ",
    "ëŒ€êµ¬ì‹œì„¤ê³µë‹¨ ë¶êµ¬ì‚¬ì—…ì†Œ": "ì§í• ", "ëŒ€êµ¬ë†ìˆ˜ì‚°ë¬¼ìœ í†µê´€ë¦¬ê³µì‚¬": "ì§í• ",
    "ëŒ€êµ¬ë„ë‚¨": "ì§í• ", "ëŒ€êµ¬ì—­í•œë¼í•˜ìš°ì  íŠ¸ì„¼íŠ¸ë¡œ": "ì§í• ",
    # ========================
    # ë™ëŒ€êµ¬ì§€ì‚¬ (ë™êµ¬, ìˆ˜ì„±êµ¬, ê°€ì°½ë©´)
    # ========================
    "ëŒ€êµ¬ ë™êµ¬ì²­": "ë™ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ ìˆ˜ì„±êµ¬ì²­": "ë™ëŒ€êµ¬ì§€ì‚¬", "ìˆ˜ì„±êµ¬": "ë™ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê²½ë¶ì²¨ë‹¨ì˜ë£Œì‚°ì—…ì§„í¥ì¬ë‹¨": "ë™ëŒ€êµ¬ì§€ì‚¬", "ì²¨ë³µì¬ë‹¨": "ë™ëŒ€êµ¬ì§€ì‚¬",
    "êµ­ë¦½ëŒ€êµ¬ë°•ë¬¼ê´€": "ë™ëŒ€êµ¬ì§€ì‚¬", "í•œêµ­ê°€ìŠ¤ê³µì‚¬ ë³¸ì‚¬": "ë™ëŒ€êµ¬ì§€ì‚¬", 
    "ëŒ€êµ¬ë¯¸ìˆ ê´€": "ë™ëŒ€êµ¬ì§€ì‚¬", "ìˆ˜ì„±ëŒ€í•™êµ": "ë™ëŒ€êµ¬ì§€ì‚¬",
    "ë™ëŒ€êµ¬ì—­": "ë™ëŒ€êµ¬ì§€ì‚¬", "ë‹¬ì„±êµ° ê°€ì°½ë©´": "ë™ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬í™˜ê²½ê³µë‹¨ ë™ë¶€ì‚¬ì—…ì†Œ": "ë™ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ë™ë¶€ê²½ì°°ì„œ": "ë™ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ìˆ˜ì„±ê²½ì°°ì„œ": "ë™ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ ë™êµ¬ì²­": "ë™ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ë™êµ¬": "ë™ëŒ€êµ¬ì§€ì‚¬", # <-- ADD
    "ëŒ€êµ¬ ìˆ˜ì„±êµ¬ì²­": "ë™ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ìˆ˜ì„±êµ¬": "ë™ëŒ€êµ¬ì§€ì‚¬", # <-- ADD
    "ëŒ€êµ¬ì°½ì˜ìœµí•©êµìœ¡ì›": "ë™ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ìœ¨í•˜": "ë™ëŒ€êµ¬ì§€ì‚¬", 

    # ========================
    # ì„œëŒ€êµ¬ì§€ì‚¬ (ì„œêµ¬, ë‚¨êµ¬, ë‹¬ì„œêµ¬/ë‹¬ì„±êµ° ì¼ë¶€)
    # ========================
    "ëŒ€êµ¬ ì„œêµ¬ì²­": "ì„œëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ ë‚¨êµ¬ì²­": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ë‘ë¥˜ê³µì›": "ì„œëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ì˜ë£Œì›": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ê³„ëª…ëŒ€í•™êµ ë™ì‚°ë³‘ì›": "ì„œëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ê°€í†¨ë¦­ëŒ€í•™êµë³‘ì›": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ì„œëŒ€êµ¬ì‚°ì—…ë‹¨ì§€": "ì„œëŒ€êµ¬ì§€ì‚¬", "ì„œëŒ€êµ¬ì‚°ë‹¨": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬í™˜ê²½ê³µë‹¨ ì„œë¶€ì‚¬ì—…ì†Œ": "ì„œëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ì‹œì„¤ê³µë‹¨ ë‚¨êµ¬ì‚¬ì—…ì†Œ": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ì„œë¶€ê²½ì°°ì„œ": "ì„œëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ë‚¨ë¶€ê²½ì°°ì„œ": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ ì„œêµ¬ì²­": "ì„œëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ê´‘ì—­ì‹œ ì„œêµ¬": "ì„œëŒ€êµ¬ì§€ì‚¬", 
    "ëŒ€êµ¬ê´‘ì—­ì‹œë¦½ì„œë¶€ë„ì„œê´€": "ì„œëŒ€êµ¬ì§€ì‚¬", "ì£½ê³¡ì •ìˆ˜ì‚¬ì—…ì†Œ": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê´‘ì—­ì‹œ ìƒìˆ˜ë„ì‚¬ì—…ë³¸ë¶€ ì£½ê³¡ì •ìˆ˜ì‚¬ì—…ì†Œ": "ì„œëŒ€êµ¬ì§€ì‚¬",

    # ========================
    # ë‚¨ëŒ€êµ¬ì§€ì‚¬ (ë‹¬ì„œêµ¬/ë‹¬ì„±êµ° ëŒ€ë¶€ë¶„)
    # ========================
    "ë‹¬ì„±êµ°ì²­": "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬/ë™ëŒ€êµ¬ì§€ì‚¬",
    "ë‹¬ì„œêµ¬ì²­": "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„œêµ¬": "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬", 
    "ëŒ€êµ¬ê´‘ì—­ì‹œ ë‹¬ì„±êµ°": "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬/ë™ëŒ€êµ¬ì§€ì‚¬",
    "ì„±ì„œì‚°ì—…ë‹¨ì§€": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ì„±ì„œì‚°ë‹¨": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬êµ­ê°€ì‚°ì—…ë‹¨ì§€": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬êµ­ê°€ì‚°ë‹¨": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "êµ­ë¦½ëŒ€êµ¬ê³¼í•™ê´€": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "í…Œí¬ë…¸í´ë¦¬ìŠ¤": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",  "ê³„ëª…ëŒ€í•™êµ": "ì„œëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬í™˜ê²½ê³µë‹¨ ë‚¨ë¶€ì‚¬ì—…ì†Œ": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ì‹œì„¤ê³µë‹¨ ë‹¬ì„œì‚¬ì—…ì†Œ": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê¸°ê³„ë¶€í’ˆì—°êµ¬ì›": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ì§€ì—­ë‚œë°©ê³µì‚¬ ëŒ€êµ¬": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ëŒ€ê´‘í…ìŠ¤íƒ€ì¼": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "ì›”ë°°êµ­ë¯¼ì²´ìœ¡ì„¼í„°": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ìœ ê°€ í…Œí¬ë…¸í´ë¦¬ìŠ¤": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "ë‹¬ì„±ì¢…í•©ìŠ¤í¬ì¸ íŒŒí¬": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ëŒ€êµ¬ê²½ë¶ê³¼í•™ê¸°ìˆ ì›": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬í•™ìƒë¬¸í™”ì„¼í„°": "ë‚¨ëŒ€êµ¬ì§€ì‚¬", "ë‹¬ì„±ì¤‘": "ë‚¨ëŒ€êµ¬ì§€ì‚¬",

    # ========================
    # í¬í•­ì§€ì‚¬ (í¬í•­ ë‚¨êµ¬)
    # ========================
    "í¬í•­ì‹œì²­": "í¬í•­ì§€ì‚¬", "í¬í•­ ë‚¨êµ¬ì²­": "í¬í•­ì§€ì‚¬", "í¬ìŠ¤ì½”": "í¬í•­ì§€ì‚¬",
    "í¬í•­ê³µê³¼ëŒ€í•™êµ": "í¬í•­ì§€ì‚¬", "í¬í•­ì‹œë¦½ë¯¸ìˆ ê´€": "í¬í•­ì§€ì‚¬",
    "í¬í•­í™˜ê²½ê´€ë¦¬ì›": "í¬í•­ì§€ì‚¬", "í¬í•­ë‚¨ë¶€ê²½ì°°ì„œ": "í¬í•­ì§€ì‚¬",
    "í¬í•­ì˜ë£Œì›": "í¬í•­ì§€ì‚¬", "í¬í•­ë¸”ë£¨ë°¸ë¦¬êµ­ê°€ì‚°ë‹¨": "í¬í•­ì§€ì‚¬",
    "í¬í•­ì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­": "í¬í•­ì§€ì‚¬", "í•´ì–‘ìˆ˜ì‚°ë¶€ í¬í•­ì§€ë°©í•´ì–‘ìˆ˜ì‚°ì²­": "í¬í•­ì§€ì‚¬",
    
    # ========================
    # ë¶í¬í•­ì§€ì‚¬ (í¬í•­ ë¶êµ¬)
    # ========================
    "í¬í•­ ë¶êµ¬ì²­": "ë¶í¬í•­ì§€ì‚¬", "í¬í•­í…Œí¬ë…¸íŒŒí¬": "ë¶í¬í•­ì§€ì‚¬", "í•œë™ëŒ€í•™êµ": "ë¶í¬í•­ì§€ì‚¬",
    "í¬í•­ë¶ë¶€ê²½ì°°ì„œ": "ë¶í¬í•­ì§€ì‚¬", "í¬í•­ìœµí•©ê¸°ìˆ ì‚°ë‹¨": "ë¶í¬í•­ì§€ì‚¬",
    "í¬í•­êµ­í† ê´€ë¦¬ì‚¬ë¬´ì†Œ": "ë¶í¬í•­ì§€ì‚¬", "êµ­í† êµí†µë¶€ ë¶€ì‚°ì§€ë°©êµ­í† ê´€ë¦¬ì²­ í¬í•­êµ­í† ê´€ë¦¬ì‚¬ë¬´ì†Œ": "ë¶í¬í•­ì§€ì‚¬",
    "í•œêµ­ë†ì–´ì´Œê³µì‚¬ ê²½ë¶ì§€ì—­ë³¸ë¶€ í¬í•­ì§€ì‚¬": "ë¶í¬í•­ì§€ì‚¬", "í¬í•­í¥í•´": "ë¶í¬í•­ì§€ì‚¬",
    "ê²½ìƒë¶ë„ í™˜ë™í•´ì§€ì—­ë³¸ë¶€": "ë¶í¬í•­ì§€ì‚¬", "í™˜ë™í•´ì§€ì—­ë³¸ë¶€": "ë¶í¬í•­ì§€ì‚¬", "ê²½ìƒë¶ë„ ë™ë¶€ì²­ì‚¬": "ë¶í¬í•­ì§€ì‚¬",
    # ========================
    # ê²½ì£¼ì§€ì‚¬
    # ========================
    "ê²½ì£¼ì‹œì²­": "ê²½ì£¼ì§€ì‚¬", "ê²½ì£¼í™”ë°±ì»¨ë²¤ì…˜ì„¼í„°": "ê²½ì£¼ì§€ì‚¬", "ê²½ì£¼ì—‘ìŠ¤í¬": "ê²½ì£¼ì§€ì‚¬",
    "ë™êµ­ëŒ€í•™êµ ê²½ì£¼ìº í¼ìŠ¤": "ê²½ì£¼ì§€ì‚¬", "í•œêµ­ìˆ˜ë ¥ì›ìë ¥ ë³¸ì‚¬": "ê²½ì£¼ì§€ì‚¬", "ë³´ë¬¸ë‹¨ì§€": "ê²½ì£¼ì§€ì‚¬", 
    "ê²½ì£¼ì˜ë£Œì›": "ê²½ì£¼ì§€ì‚¬", "ê²½ì£¼ì™¸ë™ì‚°ë‹¨": "ê²½ì£¼ì§€ì‚¬", "ê²½ì£¼ê²½ì°°ì„œ": "ê²½ì£¼ì§€ì‚¬", "ì—‘ìŠ¤í¬": "ê²½ì£¼ì§€ì‚¬",
    "í•œêµ­ìˆ˜ë ¥ì›ìë ¥": "ê²½ì£¼ì§€ì‚¬", "APEC": "ê²½ì£¼ì§€ì‚¬",
   

    # ========================
    # ê²½ì‚°ì§€ì‚¬
    # ========================
    "ê²½ì‚°ì‹œì²­": "ê²½ì‚°ì§€ì‚¬", "ì˜ë‚¨ëŒ€í•™êµ": "ê²½ì‚°ì§€ì‚¬", "ëŒ€êµ¬ê°€í†¨ë¦­ëŒ€í•™êµ": "ê²½ì‚°ì§€ì‚¬",
    "ê²½ì‚°ì§€ì‹ì‚°ì—…ì§€êµ¬": "ê²½ì‚°ì§€ì‚¬", "ê²½ì‚°ê²½ì°°ì„œ": "ê²½ì‚°ì§€ì‚¬", "ëŒ€êµ¬í•œì˜ëŒ€í•™êµ": "ê²½ì‚°ì§€ì‚¬",
    "í˜¸ì‚°ëŒ€í•™êµ": "ê²½ì‚°ì§€ì‚¬", "ëŒ€êµ¬ëŒ€í•™êµ": "ê²½ì‚°ì§€ì‚¬",
   
    # ========================
    # ê¹€ì²œì§€ì‚¬
    # ========================
    "ê¹€ì²œì‹œì²­": "ê¹€ì²œì§€ì‚¬", "í•œêµ­ë„ë¡œê³µì‚¬ ë³¸ì‚¬": "ê¹€ì²œì§€ì‚¬", "í˜ì‹ ë„ì‹œ(ê¹€ì²œ)": "ê¹€ì²œì§€ì‚¬",
    "ê¹€ì²œì˜ë£Œì›": "ê¹€ì²œì§€ì‚¬", "ê¹€ì²œê²½ì°°ì„œ": "ê¹€ì²œì§€ì‚¬",
   

    # ========================
    # ì˜ì²œì§€ì‚¬
    # ========================
    "ì˜ì²œì‹œì²­": "ì˜ì²œì§€ì‚¬", "ì˜ì²œí•˜ì´í…Œí¬íŒŒí¬": "ì˜ì²œì§€ì‚¬", "ì˜ì²œê²½ì°°ì„œ": "ì˜ì²œì§€ì‚¬",

    # ========================
    # êµìœ¡ê¸°ê´€ (Education Offices)
    # ========================
    "ëŒ€êµ¬ê´‘ì—­ì‹œë‹¬ì„±êµìœ¡ì§€ì›ì²­": "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬/ë™ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê´‘ì—­ì‹œë‹¬ì„±êµìœ¡ì²­": "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬/ë™ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê´‘ì—­ì‹œë‚¨ë¶€êµìœ¡ì§€ì›ì²­": "ì„œëŒ€êµ¬ì§€ì‚¬/ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê´‘ì—­ì‹œë‚¨ë¶€êµìœ¡ì²­": "ì„œëŒ€êµ¬ì§€ì‚¬/ë‚¨ëŒ€êµ¬ì§€ì‚¬",
    "ëŒ€êµ¬ê´‘ì—­ì‹œì„œë¶€êµìœ¡ì§€ì›ì²­": "ì„œëŒ€êµ¬ì§€ì‚¬/ì§í• ",
    "ëŒ€êµ¬ê´‘ì—­ì‹œì„œë¶€êµìœ¡ì²­": "ì„œëŒ€êµ¬ì§€ì‚¬/ì§í• ",
    "ëŒ€êµ¬ê´‘ì—­ì‹œë™ë¶€êµìœ¡ì§€ì›ì²­": "ë™ëŒ€êµ¬ì§€ì‚¬/ì§í• ",
    "ëŒ€êµ¬ê´‘ì—­ì‹œë™ë¶€êµìœ¡ì²­": "ë™ëŒ€êµ¬ì§€ì‚¬/ì§í• ",
}
try:
    from client_hints_schools import CLIENT_HINTS_SCHOOLS
    CLIENT_HINTS.update(CLIENT_HINTS_SCHOOLS)
except Exception as e:
    print("[school hints] load failed:", e)


BROAD_KEYWORD_OFFICE_MAP = {
    "ê²½ì£¼ì‹œ": "ê²½ì£¼ì§€ì‚¬", "ê²½ì£¼": "ê²½ì£¼ì§€ì‚¬",
    "ê²½ì‚°ì‹œ": "ê²½ì‚°ì§€ì‚¬", "ê²½ì‚°": "ê²½ì‚°ì§€ì‚¬",
    "ê¹€ì²œì‹œ": "ê¹€ì²œì§€ì‚¬", "ê¹€ì²œ": "ê¹€ì²œì§€ì‚¬",
    "ì˜ì²œì‹œ": "ì˜ì²œì§€ì‚¬", "ì˜ì²œ": "ì˜ì²œì§€ì‚¬",
    "ì¹ ê³¡êµ°": "ì¹ ê³¡ì§€ì‚¬", "ì¹ ê³¡": "ì¹ ê³¡ì§€ì‚¬",
    "ì„±ì£¼êµ°": "ì„±ì£¼ì§€ì‚¬", "ì„±ì£¼": "ì„±ì£¼ì§€ì‚¬",
    "ì²­ë„êµ°": "ì²­ë„ì§€ì‚¬", "ì²­ë„": "ì²­ë„ì§€ì‚¬",
    "ê³ ë ¹êµ°": "ê³ ë ¹ì§€ì‚¬", "ê³ ë ¹": "ê³ ë ¹ì§€ì‚¬",
    "ì˜ë•êµ°": "ì˜ë•ì§€ì‚¬", "ì˜ë•": "ì˜ë•ì§€ì‚¬",
}



def assign_offices_by_keywords(client_name: str, project_name: str) -> List[str]:
    """ì£¼ì†Œë¡œ ëª» ì •í•˜ë©´, ìˆ˜ìš”ê¸°ê´€ëª… + ì‚¬ì—…ëª…(ì œëª©)ì—ì„œ íŒíŠ¸ ì¶”ë¡ """
    text = f"{client_name or ''} {project_name or ''}"
    # 'êµ°ìœ„' ìš°ì„  ì œì™¸(denyê°€ ê±¸ëŸ¬ì£¼ì§€ë§Œ, ì—¬ê¸°ì„œë„ 1ì°¨ ë°©ì–´)
    if "êµ°ìœ„" in text:
        return []
    # êµ¬ì²´ í‚¤ì›Œë“œ ìš°ì„  ë§¤ì¹­
    # 1ìˆœìœ„: ê°€ì¥ êµ¬ì²´ì ì¸ ì „ì²´ ê¸°ê´€ëª…ìœ¼ë¡œ ê²€ìƒ‰ (ì˜ˆ: "ëŒ€êµ¬ ë™êµ¬ì²­")
    # sortedë¥¼ í†µí•´ ê¸´ ì´ë¦„(ë” êµ¬ì²´ì ì¸ ì´ë¦„)ì„ ë¨¼ì € ë¹„êµ
    for k, office in sorted(CLIENT_HINTS.items(), key=lambda x: len(x[0]), reverse=True):
        if k and k in text:
            # officeê°€ "A/B" í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ split í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            return office.split('/')

    # 2ìˆœìœ„: ê´€í•  ì‹œ/êµ° í‚¤ì›Œë“œë¡œ ë‹¨ì¼ ì‚¬ì—…ì†Œ ê²€ìƒ‰ (ì˜ˆ: "ì„±ì£¼", "ê²½ì£¼")
    # _contains_tokenì„ ì‚¬ìš©í•˜ì—¬ 'ì„±ì£¼ì‚°' ê°™ì€ ë‹¨ì–´ì˜ ì¼ë¶€ê°€ ì¼ì¹˜í•˜ëŠ” ì˜¤ë¥˜ ë°©ì§€
    for keyword, office in BROAD_KEYWORD_OFFICE_MAP.items():
        if _contains_token(text, [keyword]):
            return [office]
            
    # ëŒ€êµ¬/ê²½ë¶ ëŒ€ì—­ í‚¤ì›Œë“œë¡œ 'ê´€í• ë¶ˆëª…(ë¶„ë°°) í›„ë³´' ë°˜í™˜
    if any(t in text for t in ["ëŒ€êµ¬ê´‘ì—­ì‹œ", " ëŒ€êµ¬", "ëŒ€êµ¬ ", "ëŒ€êµ¬"]):
        return DAEGU_OFFICES  # ëŒ€êµ¬ê¶Œ ì „ ì§€ì‚¬ì— ë…¸ì¶œ

    return []


def _assign_office_by_client_name(client_name: str) -> Optional[str]:
    """CLIENT_HINTSë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ê´€ëª…ì—ì„œ ì§ì ‘ ê´€í•  ì§€ì‚¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    if not client_name:
        return None
    for kw in sorted(CLIENT_HINTS.keys(), key=len, reverse=True):
        if kw in client_name:
            return CLIENT_HINTS[kw]
    return None

# =========================
# ì €ì¥ ë¡œì§
# =========================
def _save_dual_office_rows(base_notice: dict, addr: str, offices: List[str]):
    """
    A/B íŠ¹ìˆ˜ê¶Œì—­ì€ ì´ì œ ë‹¨ì¼ í–‰ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    - assigned_office: "A/B" (GUIì—ì„œ ì¤„ë°”ê¿ˆ ì²˜ë¦¬)
    - address: API ì›ë³¸ ì£¼ì†Œë§Œ ì €ì¥ (ì¥ì‹/ì£¼ì„ ê¸ˆì§€)
    - status: ì›ë³¸ ìœ ì§€ (ë³´ì¡°í–‰ ì €ì¥ ì—†ìŒ)
    """
    n = dict(base_notice)
    n["assigned_office"] = f"{offices[0]}/{offices[1]}"
    n["address"] = addr or ""
    n["status"] = n.get("status", "")
    try:
        upsert_notice(n)
        session.commit()
        print(f"  [âœ… ì €ì¥ ì™„ë£Œ (A/B-ë‹¨ì¼í–‰)] {n.get('assigned_office')} / {n.get('client')}")
    except Exception as e:
        session.rollback()
        print(f"  [Error] A/B ë‹¨ì¼í–‰ ì €ì¥ ì‹¤íŒ¨: {e}")


import unicodedata

def _is_exact_lh_dgrb(name: Optional[str]) -> bool:
    t = unicodedata.normalize("NFKC", (name or "").strip())
    return t == "í•œêµ­í† ì§€ì£¼íƒê³µì‚¬ ëŒ€êµ¬ê²½ë¶ì§€ì—­ë³¸ë¶€"

def expand_and_store_with_priority(
    base_notice: dict,
    client_code: Optional[str],
    mall_addr: Optional[str],
    client_name: Optional[str],
    save: bool = True
):
    
    def _fill_kea_if_needed(n: dict):
        # íƒ€ì§€ì—­ ì»·ì„ ëª¨ë‘ í†µê³¼í•œ í›„ì—ë§Œ í˜¸ì¶œë¨
        if not USE_KEA_CHECK:
            return
        if n.get("is_certified") != "í™•ì¸í•„ìš”":
            return
        m = (n.get("model_name") or "").strip()
    # âœ… KEA ì¡°íšŒ ìŠ¤í‚µ ëŒ€ìƒ(ë¬´ì˜ë¯¸ ëª¨ë¸ëª…)
        SKIP_MODELS = {
            "ëª¨ë¸ëª… ì—†ìŒ", "ì„¸ë¶€ë‚´ì—­ ë¯¸í™•ì¸", "N/A",
            "ê³„íš ë‹¨ê³„ í™•ì¸", "ê³µê³  í™•ì¸ í•„ìš”", "ì…ì°° í™•ì¸ í•„ìš”", "ê³„ì•½ í™•ì¸ í•„ìš”",
        }
        if not m or m in SKIP_MODELS:
            return

        try:
            r = kea_has_model_cached(m)
            n["is_certified"] = "O(ì¸ì¦)" if r is True else ("X(ë¯¸ì¸ì¦)" if r is False else "í™•ì¸í•„ìš”")
        except Exception as e:
            # KEA ì—ëŸ¬ëŠ” ì €ì¥ì„ ë§‰ì§€ ì•Šê³ , ìƒíƒœë§Œ 'í™•ì¸í•„ìš”' ìœ ì§€
            print(f"  [KEA] ì¡°íšŒ ìŠ¤í‚µ/ì˜¤ë¥˜: {e}")

    def _save(n):
        _fill_kea_if_needed(n) 
        if save:
            upsert_notice(n); session.commit()
            print(f"  [âœ… ì €ì¥ ì™„ë£Œ] {n.get('assigned_office','')} / {n.get('client')}")
            return None
        else:
            print(f"  [ğŸ§º ì €ì¥ ëŒ€ê¸°] {n.get('assigned_office','')} / {n.get('client')}")
            return n
    # íƒ€ê¶Œì—­ë§Œ ëª…ì‹œ & ëª©í‘œê¶Œì—­ ë¶€ì¬ ì‹œ ì»· (ê¸°ê´€ëª…ê¹Œì§€ í¬í•¨í•´ ì¬í™•ì¸)
    _alltxt_norm = _norm_text(base_notice.get("project_name",""), client_name or "", mall_addr or "")
   
    # [FIX] Add a hard-deny check on the combined text at the very beginning.
    
    if any(k in _alltxt_norm for k in (kw.lower() for kw in HARD_DENY_KEYWORDS)):
        print_exclude_once(base_notice, client_name, mall_addr or "")
        return

    # The existing region check can remain as a secondary filter
    if any(k in _alltxt_norm for k in (kw.lower() for kw in OTHER_REGION_KEYWORDS)) and not any(k in _alltxt_norm for k in (kw.lower() for kw in TARGET_REGION_KEYWORDS)):
        print_exclude_once(base_notice, client_name, mall_addr or "")
        return
    

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # [íŠ¹ìˆ˜] ê¸°ê´€ëª…ì´ 'í•œêµ­í† ì§€ì£¼íƒê³µì‚¬ ëŒ€êµ¬ê²½ë¶ì§€ì—­ë³¸ë¶€'ì¼ ë•Œ:
    #  - í•™êµëŠ” ë³´ì§€ ì•ŠìŒ
    #  - ìš°ì„ ìˆœìœ„: (1) ì‚¬ì—…ëª… â†’ (2) ê¸°ê´€ëª… â†’ (3) ì£¼ì†Œ(ë§ˆì§€ë§‰)
    if _is_exact_lh_dgrb(client_name):
        project_title = (base_notice.get("project_name") or "").strip()

        # (1) ì‚¬ì—…ëª…(ì œëª©) ìš°ì„ 
        if project_title:
            offices = assign_offices_by_keywords("", project_title)  # ì œëª©ë§Œ ì „ë‹¬
            if offices:
                n = dict(base_notice)
                n["assigned_office"] = "/".join(offices) if isinstance(offices, (list, tuple)) else str(offices)
                n["address"] = mall_addr or ""
                return _save(n)

        # (2) ê¸°ê´€ëª… ê¸°ë°˜ (ë‹¨ì¼ ë§¤í•‘ â†’ í‚¤ì›Œë“œ)
        if client_name:
            office = _assign_office_by_client_name(client_name)
            if office:
                n = dict(base_notice)
                n["assigned_office"] = office
                n["address"] = mall_addr or ""
                return _save(n)

            offices = assign_offices_by_keywords(client_name, "")
            if offices:
                n = dict(base_notice)
                n["assigned_office"] = "/".join(offices) if isinstance(offices, (list, tuple)) else str(offices)
                n["address"] = mall_addr or ""
                return _save(n)

        # (3) ì£¼ì†Œ ê¸°ë°˜ (ë§ˆì§€ë§‰ì—ë§Œ ì‹œë„)
        addr, src = _pick_addr_by_priority(client_code, mall_addr)

        # í•„ìš” ì‹œ ê¸°ê´€ëª…â†’UsrInfo ë³´ì¡°(ì£¼ì†Œê°€ ì—†ê±°ë‚˜, usrê°€ ì•„ë‹ˆê³  ë™ë ˆë²¨ ë¯¸í¬í•¨ì¸ ê²½ìš°)
        if USE_NAME_BASED_USRINFO and (not addr or (src != 'usr' and not has_dong_level(addr))) and client_name:
            try:
                name_addr = _usr_addr_by_name_cached(client_name.strip())
                if name_addr:
                    addr, src = name_addr, 'usr'
            except Exception:
                pass

        if addr:
            # ëª©í‘œ ê¶Œì—­ ì²´í¬(íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ì—ë„ ë™ì¼ ê¸°ì¤€ ì ìš©)
            if not _is_address_in_scope(addr):
                print_exclude_once(base_notice, client_name, addr)
                return

            # ìƒì„¸ë™ ì—†ìŒ + íŠ¹ìˆ˜ êµ¬ ë§¤ì¹­ ì‹œ A/B ë‹¨ì¼í–‰ ì €ì¥
            if not has_dong_level(addr):
                offices = _special_gu_offices_if_match(addr)
                if offices and len(offices) == 2:
                    n = dict(base_notice)
                    n["assigned_office"] = f"{offices[0]}/{offices[1]}"
                    n["address"] = addr or ""
                    return _save(n)

            # ë‹¨ì¼ ì˜¤í”¼ìŠ¤ ì§€ì •
            office = _assign_office_by_addr(addr)
            if office:
                n = dict(base_notice)
                n["assigned_office"] = office
                n["address"] = addr
                return _save(n)

        # ì—¬ê¸°ê¹Œì§€ë„ ëª» ì¡ìœ¼ë©´ ìµœì¢… ì œì™¸ (ì£¼ì†ŒëŠ” ë” ì•ˆ ë´„)
        print_exclude_once(base_notice, client_name, mall_addr or "")
        return    
    # === (ì¶”ê°€) ê¸°ê´€ëª… ê¸°ë°˜ UsrInfo ì£¼ì†Œ ì¡°íšŒ ë³´ì¡°ê¸° (ì´ í•¨ìˆ˜ ë‚´ë¶€ ìºì‹œ/ìŠ¤ë¡œí‹€)
    def _usr_addr_by_name_cached(name: str) -> Optional[str]:
        if not name:
            return None

        # ê°„ë‹¨ ìºì‹œ/ìŠ¤ë¡œí‹€(í•¨ìˆ˜ ì†ì„± ì‚¬ìš©)
        import time
        from datetime import datetime, timedelta

        if not hasattr(_usr_addr_by_name_cached, "_cache"):
            _usr_addr_by_name_cached._cache = {}  # type: ignore[attr-defined]
        if not hasattr(_usr_addr_by_name_cached, "_last_call"):
            _usr_addr_by_name_cached._last_call = 0.0  # type: ignore[attr-defined]

        _cache: dict = _usr_addr_by_name_cached._cache  # type: ignore[attr-defined]
        _last_call: float = _usr_addr_by_name_cached._last_call  # type: ignore[attr-defined]

        if name in _cache:
            return _cache[name]

        wait = 0.12 - (time.time() - _last_call)
        if wait > 0:
            time.sleep(wait)

        # ìµœê·¼ 12ê°œì›” ë²”ìœ„ (UsrInfoëŠ” ê¸°ê°„ í•„ìˆ˜)
        end = datetime.now()
        start = end - timedelta(days=365)
        inqryBgnDt = start.strftime("%Y%m%d") + "0000"
        inqryEndDt = end.strftime("%Y%m%d") + "2359"

        params = {
            "ServiceKey": _cfg("NARA_SERVICE_KEY"),
            "type": "json",
            "inqryDiv": "2",                 # ë³€ê²½ì¼ ê¸°ì¤€
            "inqryBgnDt": inqryBgnDt,
            "inqryEndDt": inqryEndDt,
            "dminsttNm": name,               # ê¸°ê´€ëª… ê¸°ì¤€ ì¡°íšŒ
            "numOfRows": "1",
            "pageNo": "1",
        }
        try:
            data = http_get_json(api_url(USR_INFO_PATH), params)
            body = _as_dict(data.get("response", {}).get("body"))
            items = _as_items_list(body)
            if not items:
                return None
            it = items[0]
            full = f"{it.get('adrs','')}".strip()
            dtl  = f"{it.get('dtlAdrs','')}".strip()
            text = (full + " " + dtl).strip() or it.get("rgnNm")
            _cache[name] = text
            return text
        except Exception as e:
            print(f"  [Warn] UsrInfo(name) ì‹¤íŒ¨: {name} ({e})")
            return None
        finally:
            _usr_addr_by_name_cached._last_call = time.time()  # type: ignore[attr-defined]

    # 1/2ìˆœìœ„: ì£¼ì†Œ ì •ë³´ë¡œ ë°°ì •
    addr, src = _pick_addr_by_priority(client_code, mall_addr)
    # ìˆ˜ì • ì½”ë“œ
    if USE_NAME_BASED_USRINFO and (not addr or src != 'usr' and not has_dong_level(addr)) and client_name:
        name_addr = _usr_addr_by_name_cached(client_name.strip())
        if name_addr:
            addr = name_addr
            src = 'usr'



    def _is_address_in_scope(a: str) -> bool:
        s = (a or "").replace(" ", "")
        # 1) ëŒ€êµ¬ ì „ì—­ í—ˆìš©
        if "ëŒ€êµ¬" in s:
            return True
        # 2) ê²½ìƒë¶ë„ ë‚´ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ëŒ€ìƒ ê¶Œì—­ë§Œ í—ˆìš©)
        gb_allow = ["í¬í•­ì‹œ", "ê²½ì£¼ì‹œ", "ì˜ë•êµ°", "ì˜ì²œì‹œ", "ì²­ë„êµ°", "ê²½ì‚°ì‹œ", "ê¹€ì²œì‹œ", "ì„±ì£¼êµ°", "ì¹ ê³¡êµ°", "ê³ ë ¹êµ°"]
        if "ê²½ìƒë¶ë„" in s:
            return any(w.replace(" ", "") in s for w in gb_allow)
        # ê·¸ ì™¸ ê´‘ì—­ì€ ê¸°ë³¸ ì œì™¸
        return False

    if addr and not _is_address_in_scope(addr):
        print_exclude_once(base_notice, client_name, addr)
        return


    # 1ìˆœìœ„: í•™êµëª… ìš°ì„ (ë‚˜ë¼ì¥í„° ì „ìš©) - êµìœ¡ì²­ ë“± ìƒìœ„ê¸°ê´€ì´ ì„ì—¬ë„ 'í•™êµ'ë§Œ ë³´ê³  ë‹¨ì¼ ì§€ì‚¬ ë°°ì •
    school_office = _assign_office_by_school_name(client_name or "", base_notice.get("project_name","") or "")
    if school_office:
        n = dict(base_notice)
        n["assigned_office"] = school_office      # âœ… ë‹¨ì¼ ì§€ì‚¬ë§Œ
        n["address"] = mall_addr or ""            # ì£¼ì†ŒëŠ” mall_addr ìœ ì§€
        return _save(n)  

    # 2ìˆœìœ„:
    if addr:
        # íŠ¹ìˆ˜ê¶Œì—­: êµ¬ê¹Œì§€ë§Œ ë‚˜ì˜¨ ê²½ìš° A/B ì €ì¥
        if not has_dong_level(addr):
            offices = _special_gu_offices_if_match(addr)
            if offices and len(offices) == 2:
                n = dict(base_notice); n["assigned_office"] = f"{offices[0]}/{offices[1]}"; n["address"] = addr or ""
                #print(f"  [âœ… ì €ì¥ í›„ë³´ (A/B-ë‹¨ì¼í–‰)] {n.get('assigned_office')} / {n.get('client')}")
                return _save(n)
        # ë‹¨ì¼ ì˜¤í”¼ìŠ¤ ì§€ì •
        office = _assign_office_by_addr(addr)
        if office:
            n = dict(base_notice); n["assigned_office"] = office; n["address"] = addr
            #print(f"  [âœ… ì €ì¥ í›„ë³´ ({'ìƒì„¸ì£¼ì†Œ' if src=='usr' else 'ìˆ˜ìš”ê¸°ê´€ì£¼ì†Œ'})] {n.get('assigned_office')} / {n.get('client')}")
            return _save(n)
        

      

    # 3ìˆœìœ„: í´ë¼ì´ì–¸íŠ¸/ì‚¬ì—…ëª… íŒíŠ¸ (ì£¼ì†Œ ë¯¸ê²°ì • ì‹œ)
    if client_name:
        office = _assign_office_by_client_name(client_name)
        if office:
            n = dict(base_notice); n["assigned_office"] = office; n["address"] = mall_addr or ""
            #print(f"  [âœ… ì €ì¥ í›„ë³´] {n.get('assigned_office')} / {n.get('client')}")
            return _save(n)
        
    offices = assign_offices_by_keywords(client_name or "", base_notice.get("project_name",""))
    if offices:
        n = dict(base_notice)
        n["assigned_office"] = "/".join(offices) if isinstance(offices, (list,tuple)) else str(offices)
        n["address"] = mall_addr or ""
        #print(f"  [âš ï¸ ì €ì¥ í›„ë³´ (ì œëª© ê¸°ë°˜ ê´‘ì—­)] {n.get('assigned_office')} / {n.get('client')}")
        return _save(n)

    # ìµœì¢… ì œì™¸
    print_exclude_once(base_notice, client_name, mall_addr or addr or "")
    return


def finalize_notice_dict(base_notice, client_code, mall_addr, client_name):
    # ì €ì¥í•˜ì§€ ë§ê³  dictë¥¼ ëŒë ¤ì¤˜ì„œ ë²Œí¬ ì—…ì„œíŠ¸ ê²½ë¡œì—ì„œ ì‚¬ìš©
    return expand_and_store_with_priority(base_notice, client_code, mall_addr, client_name, save=False)



# =========================
# ìˆ˜ì§‘ê¸°
# =========================
# [ìˆ˜ì •] _build_base_notice í•¨ìˆ˜ì— source_system í•„ë“œ ì¶”ê°€
def _build_base_notice(stage: str, biz_type: str, project_name: str, client: str, phone: str,
                       model: str, qty: int, amount: str, is_cert: str, notice_date: str, detail_link: str,
                       source: str = 'G2B', kapt_code: Optional[str] = None) -> Dict: # kapt_code íŒŒë¼ë¯¸í„° ì¶”ê°€
    return {
        "stage": stage, "biz_type": biz_type,
        "project_name": project_name or "",
        "client": client or "",
        "address": "",
        "phone_number": phone or "",
        "model_name": model or "",
        "quantity": qty or 0,
        "amount": amount or "",
        "is_certified": is_cert or "í™•ì¸í•„ìš”",
        "notice_date": notice_date or "",
        "detail_link": detail_link or "",
        "assigned_office": "",
        "is_favorite": False, "status": "", "memo": "",
        "source_system": source,
        "kapt_code": kapt_code, # ë°˜í™˜ ë”•ì…”ì…”ë¦¬ì— kapt_code ì¶”ê°€
    }

def fetch_kapt_basic_info(
    kapt_code: str,
    *,
    allow_non_standard: bool = False,   # KBâ€¦ ê°™ì€ ë¹„í‘œì¤€ ì½”ë“œ í—ˆìš© ì—¬ë¶€
    max_retries: int = 2,
    backoff_sec: float = 0.25
) -> Optional[Dict[str, Any]]:
    """ë‹¨ì§€ ì½”ë“œë¡œ K-APT ê¸°ë³¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. (ì•ˆì „ê°€ë“œ/ì¬ì‹œë„ í¬í•¨)"""
    if not kapt_code:
        return None

    # í‘œì¤€ ì½”ë“œ(A########)ë§Œ ë³´ë ¤ë©´ ì•„ë˜ ê°€ë“œ ìœ ì§€, ë¹„í‘œì¤€ í—ˆìš©ì‹œ allow_non_standard=Trueë¡œ í˜¸ì¶œ
    if (not allow_non_standard) and (not re.match(r"^A\d{8}$", kapt_code)):
        return None

    params = {
        "serviceKey": _cfg("KAPT_SERVICE_KEY"),
        "kaptCode": kapt_code,
        "_type": "json",
    }

    for attempt in range(max_retries + 1):
        try:
            data = http_get_json(api_url(KAPT_BASIC_INFO_PATH), params)

            if not isinstance(data, dict):
                raise ValueError("empty/invalid response")

            resp = data.get("response") or {}
            body = resp.get("body") or {}

            item = body.get("item")
            # ì¼ë¶€ ì‘ë‹µì´ listë¡œ ì˜¬ ìˆ˜ ìˆì–´ ëŒ€ë¹„
            if item is None:
                items = body.get("items") or body.get("itemList")
                if isinstance(items, list) and items:
                    item = items[0]

            # ì •ìƒ dictë©´ ë°˜í™˜
            if isinstance(item, dict) and item:
                return item

            # ëª…ì‹œì ìœ¼ë¡œ 'ì—†ìŒ'ì¸ ê²½ìš° ì¡°ìš©íˆ None
            total = body.get("totalCount")
            if isinstance(total, (int, str)) and int(total or 0) == 0:
                return None

            raise ValueError("no item in response")

        except Exception as e:
            if attempt < max_retries:
                time.sleep(backoff_sec * (attempt + 1))
                continue
            print(f"  [Error] K-APT ê¸°ë³¸ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({kapt_code}): {e}")
            return None
    
def _compose_display_addr(item: dict) -> str:
    """
    ë„ë¡œëª…ì£¼ì†Œê°€ ìˆìœ¼ë©´ ìœ ì§€í•˜ë˜, as3(ì/ë©´/ë™) ë˜ëŠ” bjd_mapper ë™ëª…ì´ ìˆìœ¼ë©´ ê´„í˜¸ ë³´ê°•.
    ì—†ìœ¼ë©´ as1+as2+as3 ì¡°í•©.
    """
    road = (item.get("roadAddr") or item.get("addr") or "").strip()
    as1 = (item.get("as1") or "").strip()
    as2 = (item.get("as2") or "").strip()
    as3 = (item.get("as3") or "").strip()

    # bjd ê¸°ë°˜ ë™ëª… ë³´ê°• ì‹œë„
    dong_hint = ""
    try:
        from bjd_mapper import get_bjd_name
        bjd = (item.get("bjdCode") or item.get("bjd_code") or "").strip()
        if bjd:
            dong_hint = (get_bjd_name(bjd) or "").split()[-1]  # ë§ˆì§€ë§‰ í† í°(ë™/ì/ë©´)ë§Œ íŒíŠ¸ë¡œ
    except Exception:
        pass

    # as3ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if as3:
        dong_hint = as3

    if road:
        if dong_hint and dong_hint not in road:
            return f"{road} ({dong_hint})"
        return road

    combo = " ".join(x for x in (as1, as2, as3) if x)
    return combo

def _extract_kapt_phone(basic: dict | None) -> str:
    """
    K-APT ê¸°ë³¸ì •ë³´ì—ì„œ ê´€ë¦¬ì‚¬ë¬´ì†Œ ì „í™” ì¶”ì¶œ(í‚¤ ë³€ë™ ì•ˆì „).
    ìˆ«ì/í•˜ì´í”ˆë§Œ ë‚¨ê¸°ê³  0~9/-(ìµœëŒ€ í•˜ë‚˜ì”©) ì •ê·œí™”.
    """
    if not isinstance(basic, dict):
        return ""
    # í•„ë“œ í›„ë³´(ë³€ë™ ëŒ€ì‘)
    CAND_KEYS = [
        "mngTel", "mngTelNo", "mngTelno", "kaptTel", "telNo",
        "officeTel", "officeTelNo", "managerTel",
        "asTel", "as1Tel", "as2Tel", "as3Tel",
        "tel", "phone"
    ]
    raw = ""
    for k in CAND_KEYS:
        v = basic.get(k)
        if v and isinstance(v, str) and v.strip():
            raw = v.strip()
            break
    if not raw:
        return ""

    import re
    # ìˆ«ì/í•˜ì´í”ˆ ì™¸ ì œê±°
    digits = re.sub(r"[^0-9]", "", raw)
    if not digits:
        return ""
    # 8~11ìë¦¬ ì¼€ì´ìŠ¤ ì •ê·œí™”(ëŒ€êµ¬ ì§€ì—­ êµ­ë²ˆ/íœ´ëŒ€ í¬í•¨)
    if len(digits) == 8:
        return f"{digits[:4]}-{digits[4:]}"
    if len(digits) == 9:
        return f"{digits[:2]}-{digits[2:5]}-{digits[5:]}"
    if len(digits) == 10:
        return f"{digits[:2]}-{digits[2:6]}-{digits[6:]}" if digits.startswith(("02",)) else f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
    if len(digits) == 11:
        return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
    # ê·¸ ì™¸ëŠ” ìµœëŒ€í•œ í•˜ì´í”ˆ ì‚½ì… ëª»í•˜ë©´ ì›ë³¸ ë°˜í™˜
    return raw



def fetch_kapt_maintenance_history(kapt_code: str) -> list[dict]:
    """
    K-APT ìœ ì§€ê´€ë¦¬ ì´ë ¥ ì¡°íšŒ (ì•ˆì „ ì •ê·œí™”)
    - 'list has no attribute get' ì˜ˆì™¸ë¥¼ ì›ì²œ ì°¨ë‹¨
    """
    url = api_url(KAPT_MAINTENANCE_PATH)
    params = {
        "serviceKey": _cfg("KAPT_SERVICE_KEY"),
        "pageNo": "1",
        "numOfRows": "100",
        "kaptCode": (kapt_code or "").strip(),
        "type": "json"
    }
    data = http_get_json(url, params)
    rows = _kapt_items_safely(data)  # â† ì•ˆì „ ì •ê·œí™”

    out = []
    for r in rows:
        out.append({
            "parentParentName": _as_text(r.get("parentParentName")),
            "parentName": _as_text(r.get("parentName")),
            "mnthEtime": _as_text(r.get("mnthEtime")),
            "year": _as_text(r.get("year")),
            "useYear": _as_text(r.get("useYear")),
        })
    return out



def fetch_and_process_kapt_bids(search_ymd: str):
    """K-APT ì…ì°°ê³µê³  ìˆ˜ì§‘ â€” ë¡œê·¸ëŠ” 'ì œì™¸/ì €ì¥ ëŒ€ê¸°/ì¼ê´„ ì €ì¥'ë§Œ ì¶œë ¥ (ì£¼ì†ŒëŠ” ë¡œê·¸ ëì— í‘œì‹œ)."""
    print(f"\n--- [{to_ymd(search_ymd)}] ê³µë™ì£¼íƒ(K-APT) ì…ì°°ê³µê³  ìˆ˜ì§‘ ---")

    try:
        params_first = {
            "serviceKey": _cfg("KAPT_SERVICE_KEY"), "pageNo": "1", "numOfRows": "1",
            "startDate": search_ymd, "endDate": search_ymd, "_type": "json"
        }
        first = http_get_json(api_url(KAPT_BID_LIST_PATH), params_first)
        total = int(first.get("response", {}).get("body", {}).get("totalCount", 0))
        if total == 0:
            print("- ë°ì´í„° ì—†ìŒ"); return
    except Exception as e:
        print(f"[Error] K-apt ì´ ê±´ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}"); return

    page_size = 100
    total_pages = (total + page_size - 1) // page_size
    #print(f"- ì´ {total}ê±´ / {total_pages}p")
    print(f"- ì´ {total}ê±´")

    params_list = [{
        "serviceKey": _cfg("KAPT_SERVICE_KEY"), "pageNo": str(p), "numOfRows": str(page_size),
        "startDate": search_ymd, "endDate": search_ymd, "_type": "json"
    } for p in range(1, total_pages + 1)]

    pages = fetch_pages_parallel(api_url(KAPT_BID_LIST_PATH), params_list)

    buffer = []
    for data in pages:
        items = data.get("response", {}).get("body", {}).get("items", [])
        if not items:
            continue

        for it in items:
            title = (it.get("bidTitle") or "").strip()
            if not is_relevant_text(title,
                                    _as_text(it.get("codeClassifyType1")),
                                    _as_text(it.get("codeClassifyType2")),
                                    _as_text(it.get("codeClassifyType3")),
                                    _as_text(it.get("bidMethod")),
                                    _as_text(it.get("bidKaptname"))):
                continue


            kapt_code   = (it.get("aptCode") or "").strip()
            client_name = (it.get("bidKaptname") or "").strip() or "ë‹¨ì§€ëª… ì—†ìŒ"
            bid_no      = it.get("bidNum")
            detail_link = f"https://www.k-apt.go.kr/bid/bidDetail.do?bid_noti_no={bid_no}" if bid_no else ""
            biz_type    = it.get("codeClassifyType1", "ê¸°íƒ€")

            # ì£¼ì†Œ/ë²•ì •ë™ì½”ë“œ ë³´ê°•
            bjd_code, addr_txt = "", ""
            if kapt_code:
                basic = fetch_kapt_basic_info(kapt_code)
                if basic:
                    bjd_code = str(basic.get("bjdCode") or "")
                    addr_txt = (basic.get("doroJuso") or basic.get("kaptAddr") or "").strip()
            if not bjd_code:
                raw = str(it.get("bidArea") or "")
                if len(raw) >= 8:
                    bjd_code = raw[:10]
            if not addr_txt and bjd_code and HAS_BJD_MAPPER:
                addr_txt = get_bjd_name(bjd_code)

            # ê´€í• ì§€ì‚¬ íŒì •
            assigned_office = _assign_office_from_bjd_code(bjd_code, addr_txt)

            # ëŒ€êµ¬ê¶Œ ì™¸ë©´ ì œì™¸
            if assigned_office.startswith("ê´€í• ") and not (addr_txt.startswith("ëŒ€êµ¬") or bjd_code.startswith("27")):
                log_kapt_excluded(client_name, addr_txt or bjd_code)
                continue

            base = _build_base_notice(
                stage="ì…ì°°ê³µê³ ",
                biz_type=biz_type,
                project_name=title,
                client=client_name,
                phone="", model="", qty=0, amount="",
                is_cert="í™•ì¸í•„ìš”",
                notice_date=to_ymd(it.get("bidRegDate")),
                detail_link=detail_link,
                source='K-APT',
                kapt_code=kapt_code,
            )
            base["assigned_office"] = assigned_office

            n = finalize_notice_dict(base, None, addr_txt, client_name)
            if n:
                buffer.append(n)
                log_kapt_pending(assigned_office, client_name, addr_txt or bjd_code)

    if buffer:
        bulk_upsert_notices(buffer)
        log_kapt_bulk_saved(len(buffer))


# [ì¶”ê°€] K-APT ì…ì°° ê²°ê³¼ API ì—”ë“œí¬ì¸íŠ¸
KAPT_BID_RESULT_PATH = "/1613000/ApHusBidResultNoticeInfoOfferServiceV2/getPblAncDeSearchV2"





def _assign_office_from_bjd_code(bjd_code: str, addr_text: str = "") -> str:
    """
    ë‚˜ë¼ì¥í„° ê¸°ì¤€ ê´€í•  ì¶”ì • (ë¬¸ìì—´ ë°˜í™˜)
    - í¬í•­(ë‹¨ìˆœí™”): ë¶êµ¬+ë™/ì/ë©´ì´ë©´ í¥í•´/ì†¡ë¼/ì‹ ê´‘/ì²­í•˜/ê¸°ê³„/ê¸°ë¶/ì£½ì¥ â†’ ë¶í¬í•­, ê·¸ ì™¸ í¬í•­
      ë¶êµ¬(êµ¬ê¹Œì§€ë§Œ) â†’ 'í¬í•­ì§€ì‚¬/ë¶í¬í•­ì§€ì‚¬', ë¶êµ¬ ì™¸ í¬í•­ â†’ í¬í•­ì§€ì‚¬
    - ëŒ€êµ¬: ì¤‘êµ¬/ë¶êµ¬=ì§í• , ë™êµ¬/ìˆ˜ì„±êµ¬=ë™ëŒ€êµ¬, ì„œêµ¬/ë‚¨êµ¬=ì„œëŒ€êµ¬
      ë‹¬ì„œêµ¬: ë™ë ˆë²¨ ê¸°ë³¸ ë‚¨ëŒ€êµ¬(ê°ì‚¼/ë‘ë¥˜/ë³¸ë¦¬/ì„±ë‹¹/ì£½ì „ë§Œ ì„œëŒ€êµ¬), êµ¬ê¹Œì§€ë§Œ 'ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬'
      ë‹¬ì„±êµ°: ë‹¤ì‚¬/í•˜ë¹ˆ=ì„œëŒ€êµ¬, ê°€ì°½=ë™ëŒ€êµ¬, ê·¸ ì™¸ ë™/ì/ë©´=ë‚¨ëŒ€êµ¬, êµ°ê¹Œì§€ë§Œ 'ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬'
    - ê²½ë¶ ê¸°íƒ€: ê²½ì£¼/ê²½ì‚°/ê¹€ì²œ/ì˜ì²œ/ì¹ ê³¡/ì„±ì£¼/ì²­ë„/ê³ ë ¹/ì˜ë•
    - ì‹¤íŒ¨ â†’ 'ê´€í• ì§€ì‚¬í™•ì¸ìš”ë§'
    """
    def _has_dong_level(a: str) -> bool:
        return bool(re.search(r"(ë™|ì|ë©´|ë¦¬)\b", a or ""))

    # ì£¼ì†Œë¥¼ ë°˜ë“œì‹œ ë¬¸ìì—´ë¡œ í™•ë³´ (ìˆœí™˜ì˜ì¡´ ì—†ì´)
    addr = resolve_address_from_bjd(addr_text=addr_text, bjd_code=bjd_code)
    if not addr:
        return "ê´€í• ì§€ì‚¬í™•ì¸ìš”ë§"
    
    # â”€ ëŒ€êµ¬
    if ("ëŒ€êµ¬" in addr) or ("ëŒ€êµ¬ê´‘ì—­ì‹œ" in addr):
        has_dong = _has_dong_level(addr)
        if ("ì¤‘êµ¬" in addr) or ("ë¶êµ¬" in addr):
            return "ì§í• "
        if ("ë™êµ¬" in addr) or ("ìˆ˜ì„±êµ¬" in addr):
            return "ë™ëŒ€êµ¬ì§€ì‚¬"
        if ("ì„œêµ¬" in addr) or ("ë‚¨êµ¬" in addr):
            return "ì„œëŒ€êµ¬ì§€ì‚¬"
        if "ë‹¬ì„œêµ¬" in addr:
            if has_dong:
                if any(d in addr for d in ["ê°ì‚¼","ë‘ë¥˜","ë³¸ë¦¬","ì„±ë‹¹","ì£½ì „"]):
                    return "ì„œëŒ€êµ¬ì§€ì‚¬"
                return "ë‚¨ëŒ€êµ¬ì§€ì‚¬"
            return "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬"
        if "ë‹¬ì„±êµ°" in addr:
            if has_dong:
                if any(e in addr for e in ["ë‹¤ì‚¬ì","í•˜ë¹ˆë©´"]):
                    return "ì„œëŒ€êµ¬ì§€ì‚¬"
                if "ê°€ì°½ë©´" in addr:
                    return "ë™ëŒ€êµ¬ì§€ì‚¬"
                return "ë‚¨ëŒ€êµ¬ì§€ì‚¬"
            return "ë‚¨ëŒ€êµ¬ì§€ì‚¬/ì„œëŒ€êµ¬ì§€ì‚¬"
        return "ì§í• "



    # â”€ í¬í•­
    if (("í¬í•­" in addr) or ("í¬í•­ì‹œ" in addr)) and not ("ëŒ€êµ¬ê´‘ì—­ì‹œ" in addr or "ëŒ€êµ¬ì‹œ" in addr):
        if re.search(r"í¬í•­ì‹œ\s*ë¶êµ¬", addr):
            if _has_dong_level(addr):
                if any(s in addr for s in ["í¥í•´", "ì†¡ë¼", "ì‹ ê´‘", "ì²­í•˜", "ê¸°ê³„", "ê¸°ë¶", "ì£½ì¥"]):
                    return "ë¶í¬í•­ì§€ì‚¬"
                return "í¬í•­ì§€ì‚¬"
            return "í¬í•­ì§€ì‚¬/ë¶í¬í•­ì§€ì‚¬"
        return "í¬í•­ì§€ì‚¬"




    # â”€ ê²½ë¶ ê¸°íƒ€
    mapping = {
        "ê²½ì£¼": "ê²½ì£¼ì§€ì‚¬","ê²½ì‚°": "ê²½ì‚°ì§€ì‚¬","ê¹€ì²œ": "ê¹€ì²œì§€ì‚¬","ì˜ì²œ": "ì˜ì²œì§€ì‚¬",
        "ì¹ ê³¡": "ì¹ ê³¡ì§€ì‚¬","ì„±ì£¼": "ì„±ì£¼ì§€ì‚¬","ì²­ë„": "ì²­ë„ì§€ì‚¬","ê³ ë ¹": "ê³ ë ¹ì§€ì‚¬","ì˜ë•": "ì˜ë•ì§€ì‚¬",
    }
    for key, office in mapping.items():
        if key in addr:
            return office

    return "ê´€í• ì§€ì‚¬í™•ì¸ìš”ë§"

# ===== ê³µìš© ìƒìˆ˜/ë„ìš°ë¯¸ =====
PAGE_SIZE = 100  # ì „ì—­ í†µì¼

# ì—”ë“œí¬ì¸íŠ¸
PATH_PBL  = "/1613000/ApHusBidResultNoticeInfoOfferServiceV2/getPblAncDeSearchV2"   # ê³µê³ ì¼ ë²”ìœ„
PATH_CLOS = "/1613000/ApHusBidResultNoticeInfoOfferServiceV2/getBidClosDeSearchV2"  # ë§ˆê°ì¼ ë²”ìœ„
PATH_STTS = "/1613000/ApHusBidResultNoticeInfoOfferServiceV2/getBidSttusSearchV2"   # ìƒíƒœ+ì—°ë„

from collections import Counter
import re, csv, os
from datetime import datetime, timedelta

def _as_ymd8(dt: datetime) -> str:
    return dt.strftime("%Y%m%d")

def _parse_ymd8(s: str) -> datetime:
    s = "".join(ch for ch in str(s) if ch.isdigit())[:8]
    return datetime.strptime(s, "%Y%m%d")
from datetime import datetime, timedelta

def _is_business_day(d: datetime) -> bool:
    return d.weekday() < 5  # ì›”(0)~ê¸ˆ(4)

def prev_business_day(yyyymmdd: str) -> str:
    d = datetime.strptime(yyyymmdd, "%Y%m%d")
    while True:
        d -= timedelta(days=1)
        if _is_business_day(d):
            return d.strftime("%Y%m%d")

def next_business_day(yyyymmdd: str) -> str:
    d = datetime.strptime(yyyymmdd, "%Y%m%d")
    while True:
        d += timedelta(days=1)
        if _is_business_day(d):
            return d.strftime("%Y%m%d")

def _date8(s: str) -> str:
    if not s:
        return ""
    d = "".join(ch for ch in str(s) if ch.isdigit())
    return d[:8] if len(d) >= 8 else ""

# â˜… apt_list.csv ë§¤í•‘ ë¡œë” (ìœ ì—°í•œ í—¤ë” ëŒ€ì‘)
#   - config.KAPT_APT_LIST_PATH ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ì‹¤í–‰ ê²½ë¡œ ê¸°ì¤€ íƒìƒ‰
_APT_MAP = None  # ìºì‹œ

def _load_apt_map():
    global _APT_MAP
    if _APT_MAP is not None:
        return _APT_MAP

    path = _cfg("KAPT_APT_LIST_PATH")

    if not path:
        # í”í•œ ìœ„ì¹˜ í›„ë³´ë“¤
        for cand in [
            "./apt_list.csv",
            "./data/apt_list.csv",
            "C:/bh/_ing/kapt/apt_list.csv",
            "C:/bh/_final/kapt/apt_list.csv",
        ]:
            if os.path.exists(cand):
                path = cand
                break

    amap = {}
    if not path or not os.path.exists(path):
        print(f"[K-apt][ê´€í• ] apt_list.csv ë¯¸ë°œê²¬ â†’ bidArea í´ë°± ì‚¬ìš©")
        _APT_MAP = amap
        return amap

    try:
        with open(path, "r", encoding="utf-8-sig", newline="") as f:
            rdr = csv.DictReader(f)
            # ìœ ì—° í—¤ë” ì¶”ì¶œ
            # ì˜ˆì‹œ í—¤ë” í›„ë³´: aptCode / ë‹¨ì§€ì½”ë“œ, addr / address / ì£¼ì†Œ, bjd_code / ë²•ì •ë™ì½”ë“œ
            for row in rdr:
                code = row.get("aptCode") or row.get("ë‹¨ì§€ì½”ë“œ") or row.get("kaptCode") or ""
                code = str(code).strip()
                if not code:
                    continue
                addr = row.get("addr") or row.get("address") or row.get("ì£¼ì†Œ") or ""
                bjd  = row.get("bjd_code") or row.get("ë²•ì •ë™ì½”ë“œ") or row.get("bjdCode") or ""
                amap[code] = {
                    "addr": str(addr).strip(),
                    "bjd_code": str(bjd).strip(),
                }
        print(f"[K-apt][ê´€í• ] apt_list.csv ë¡œë“œ: {len(amap)}ê±´")
    except Exception as e:
        print(f"[K-apt][ê´€í• ] apt_list.csv ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        amap = {}

    _APT_MAP = amap
    return amap

def _resolve_office_by_apt_or_bidarea(kapt_code: str, bid_area: str):
    """
    1ìˆœìœ„: ë‹¨ì§€ì½”ë“œ ë§¤í•‘(ì •í™• ì£¼ì†Œ/ë²•ì •ë™)
    2ìˆœìœ„: bidArea(ì‹œÂ·ë„ ì½”ë“œ, ì˜ˆ: ëŒ€êµ¬=27)
    """
    amap = _load_apt_map()
    addr_txt, bjd_code = "", ""
    if kapt_code and kapt_code in amap:
        addr_txt = amap[kapt_code].get("addr", "") or ""
        bjd_code = amap[kapt_code].get("bjd_code", "") or ""
    if not bjd_code:
        bjd_code = str(bid_area or "")

    # addr_txt ì—†ê³  ëŒ€êµ¬ ì‹œÂ·ë„ì½”ë“œë¼ë©´ ìµœì†Œ í‘œì‹œ
    if not addr_txt and bjd_code.startswith("27"):
        addr_txt = "ëŒ€êµ¬"

    assigned = _assign_office_from_bjd_code(bjd_code, addr_txt)
    return assigned, addr_txt, bjd_code



def fetch_and_process_kapt_bid_results(search_ymd: str):
    """
    K-APT ì…ì°°ê²°ê³¼ ìˆ˜ì§‘
      - 1ìˆœìœ„: ë‹¹ì¼(ë§ˆê°/ê³µê³ )
      - ì‹¤íŒ¨ ì‹œ: ìƒíƒœ+ì—°ë„ ì „ìˆ˜ â†’ ë‹¹ì¼ Â±1ì˜ì—…ì¼ ë³´ì • í•„í„°
      - í‚¤ì›Œë“œ(ê°€ë³€ ì¸ì) + ê´€í•  + ì¤‘ë³µ ì œê±°
      - ì‚¬ìš©ì í‘œì‹œ ë¡œê·¸: ë‚˜ë¼ì¥í„° í†¤(ì´ Nê±´ / Pp, ì¼ê´„ ì €ì¥ Nê±´ / ë°ì´í„° ì—†ìŒ)
    """
    print(f"\n--- [{to_ymd(search_ymd)}] ê³µë™ì£¼íƒ(K-APT) ì…ì°°ê²°ê³¼ ìˆ˜ì§‘ ---")
    svc_key = getattr(_local_config, "KAPT_SERVICE_KEY_DECODING", None) or _cfg("KAPT_SERVICE_KEY")

    def _first_page(url_path, tag):
        try:
            r = http_get_json(api_url(url_path), {
                "serviceKey": svc_key, "_type": "json",
                "pageNo": "1", "numOfRows": "1",
                "startDate": search_ymd, "endDate": search_ymd,
            })
            resp = (r or {}).get("response") or {}
            header = resp.get("header") or {}
            body = resp.get("body") or {}
            code = header.get("resultCode")
            msg  = header.get("resultMsg")
            total = int(body.get("totalCount", 0) or 0)
            _debug(f"Â· [{tag}] resultCode={code}, totalCount={total}, msg={msg}")
            return total
        except Exception as e:
            _debug(f"Â· [{tag}] ìš”ì²­ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return 0

    # 1) ë‹¹ì¼ ìˆ˜ì§‘(ë§ˆê°ì¼/ê³µê³ ì¼)
    total_clos = _first_page(PATH_CLOS, "ë§ˆê°ì¼")
    total_pbl  = _first_page(PATH_PBL,  "ê³µê³ ì¼")

    pages_all = []

    def _fetch_pages(url_path, total, tag):
        if total <= 0:
            return []
        total_pages = (total + PAGE_SIZE - 1) // PAGE_SIZE
        #_debug(f"Â· [{tag}] ì´ {total}ê±´ / {total_pages}p â†’ ë³‘ë ¬ ìˆ˜ì§‘")
        _debug(f"Â· [{tag}] ì´ {total}ê±´")
        plist = [{
            "serviceKey": svc_key, "_type": "json",
            "pageNo": str(p), "numOfRows": str(PAGE_SIZE),
            "startDate": search_ymd, "endDate": search_ymd,
        } for p in range(1, total_pages + 1)]
        try:
            return fetch_pages_parallel(api_url(url_path), plist)
        except Exception as e:
            _debug(f"Â· [{tag}] í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return []

    if total_clos > 0:
        pages_all.extend(_fetch_pages(PATH_CLOS, total_clos, "ë§ˆê°ì¼"))
    if total_pbl  > 0:
        pages_all.extend(_fetch_pages(PATH_PBL,  total_pbl,  "ê³µê³ ì¼"))

    # ì‚¬ìš©ì ìš”ì•½(ë‹¹ì¼ í•©ì‚°)
    sum_total = (total_clos or 0) + (total_pbl or 0)
    if sum_total > 0:
        _print_total_summary(sum_total)  # "- ì´ Nê±´ / Pp"

    # 2) ìƒíƒœ+ì—°ë„ ì „ìˆ˜ â†’ ë³´ì •í•„í„°
    if not pages_all:
        _debug("Â· [ìƒíƒœ+ì—°ë„] ë³´ì • ìˆ˜ì§‘ ì‹œë„ (4=ìœ ì°°, 5=ë‚™ì°°)")

        def _collect_state(state):
            r = http_get_json(api_url(PATH_STTS), {
                "serviceKey": svc_key, "_type": "json",
                "pageNo": "1", "numOfRows": "1",
                "bidState": state, "searchYear": search_ymd[:4],
            })
            resp = (r or {}).get("response") or {}
            header = resp.get("header") or {}
            body = resp.get("body") or {}
            code = header.get("resultCode")
            total = int(body.get("totalCount", 0) or 0)
            _debug(f"Â· [ìƒíƒœ+ì—°ë„(state={state})] resultCode={code}, totalCount={total}, msg={header.get('resultMsg')}")
            if total <= 0:
                return []
            total_pages = (total + PAGE_SIZE - 1) // PAGE_SIZE
            #_debug(f"Â· [ìƒíƒœ+ì—°ë„(state={state})] ì´ {total}ê±´ / {total_pages}p â†’ ë³‘ë ¬ ìˆ˜ì§‘")
            _debug(f"Â· [ìƒíƒœ+ì—°ë„(state={state})] ì´ {total}ê±´")
            plist = [{
                "serviceKey": svc_key, "_type": "json",
                "pageNo": str(p), "numOfRows": str(PAGE_SIZE),
                "bidState": state, "searchYear": search_ymd[:4],
            } for p in range(1, total_pages + 1)]
            return fetch_pages_parallel(api_url(PATH_STTS), plist)

        st_pages = []
        st_pages.extend(_collect_state("5"))
        st_pages.extend(_collect_state("4"))

        raw_items = []
        for pg in st_pages:
            raw_items.extend((((pg or {}).get("response") or {}).get("body") or {}).get("items") or [])

        # ë¶„í¬/ì§„ë‹¨ì€ VERBOSEì—ì„œë§Œ
        clos_ctr = Counter(_date8(it.get("bidDeadline")) for it in raw_items if _date8(it.get("bidDeadline")))
        pbl_ctr  = Counter(_date8(it.get("bidRegdate"))  for it in raw_items if _date8(it.get("bidRegdate")))
        _debug(f"Â· [ìƒíƒœ+ì—°ë„ ì „ì²´] ì´ {len(raw_items)}ê±´ | ë¹ˆ ë§ˆê°ì¼:{len(raw_items)-sum(clos_ctr.values())}, ë¹ˆ ê³µê³ ì¼:{len(raw_items)-sum(pbl_ctr.values())}")

        alt_days = {search_ymd, prev_business_day(search_ymd), next_business_day(search_ymd)}
        filtered = [it for it in raw_items
                    if _date8(it.get("bidDeadline")) in alt_days
                    or _date8(it.get("bidRegdate"))  in alt_days]

        _print_total_summary(len(filtered), tag="")  # ì‚¬ìš©ì ìš”ì•½: "- ì´ Nê±´ / Pp (ë³´ì •)"

        if not filtered:
            _print_data_none()
            return

        pages_all = [{"response": {"body": {"items": filtered}}}]

    # 3) íŒŒì‹± +(í‚¤ì›Œë“œ/ê´€í• /ì¤‘ë³µ) ì •ì œ
    seen_bidnum = set()
    buffer = []
    stats = dict(total_items=0, after_kw=0, after_region=0, dedup_skip=0, excluded_region=0)
    EXCLUDE_LOG_MAX = 50
    excl_shown = 0

    for data in pages_all:
        items = (((data or {}).get("response") or {}).get("body") or {}).get("items") or []
        stats["total_items"] += len(items)
        for it in items:
            title = (it.get("bidTitle") or "").strip()
            state = (it.get("bidState") or "").strip()
            stage = "ê³„ì•½ì™„ë£Œ" if state == "5" else ("ì…ì°°ê²°ê³¼(ìœ ì°°)" if state == "4" else "ê¸°íƒ€")
            biz_type = it.get("codeClassifyType1", "ê¸°íƒ€")
            kapt_code = (it.get("aptCode") or "").strip()
            client_name = (it.get("bidKaptname") or "").strip() or "ë‹¨ì§€ëª… ì—†ìŒ"
            bid_no = (it.get("bidNum") or "").strip()

            # í‚¤ì›Œë“œ í•„í„°(ë‹¤í•„ë“œ)
            biz1 = _as_text(it.get("codeClassifyType1"))
            biz2 = _as_text(it.get("codeClassifyType2"))
            biz3 = _as_text(it.get("codeClassifyType3"))
            bid_method = _as_text(it.get("bidMethod"))
            kapt_name  = _as_text(it.get("bidKaptname"))
            if not _pass_keyword_filter(title, biz1, biz2, biz3, bid_method, kapt_name):
                continue
            stats["after_kw"] += 1

            # --- [FIX] ê´€í• /ì£¼ì†Œ/ë²•ì •ë™ì½”ë“œ ì´ˆê¸° ì„¸íŒ… ---
            # 1) apt_list.csv ìš°ì„  ì¡°íšŒ
            addr_txt, pre_office, bjd_code = lookup_apt_by_code(kapt_code)

            # 2) ëª¨ìë¼ë©´ K-APT ê¸°ë³¸ì •ë³´ë¡œ ë³´ê°•
            if not (addr_txt and bjd_code):
                basic = fetch_kapt_basic_info(kapt_code) or {}
                # ê¸°ë³¸ì •ë³´ì˜ ì£¼ì†Œ/ë²•ì •ë™ì½”ë“œ í•„ë“œëª…ì€ ì‹œìŠ¤í…œ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
                addr_txt = addr_txt or (basic.get("address") or basic.get("addr") or "")
                bjd_code = bjd_code or (basic.get("bjdCode") or basic.get("bjd_code") or "")

            # 3) ê·¸ë˜ë„ ë²•ì •ë™ì½”ë“œê°€ ì—†ìœ¼ë©´ bidArea(ì§€ì—­ì½”ë“œ)ë¼ë„ í´ë°±
            bjd_code = (bjd_code or str(it.get("bidArea") or "")).strip()

            # 4) ì´ˆê¸° ê´€í•  ì¶”ì • (apt_listì˜ pre_officeê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ ë²•ì •ë™/ì£¼ì†Œë¡œ ê³„ì‚°)
            assigned = (pre_office or _assign_office_from_bjd_code(bjd_code, addr_txt) or "").strip()

            # --- ê¸°ì¡´ ì½”ë“œ ---
            # ê´€í•  íŒì • (aptMap ìš°ì„  â†’ bidArea í´ë°±)
            assigned, addr_txt, bjd_code = _narrow_office_with_basic_info(assigned, kapt_code, addr_txt, bjd_code)


         
            if assigned.startswith("ê´€í• ") and not (bjd_code.startswith("27") or (addr_txt and addr_txt.startswith("ëŒ€êµ¬"))):
                stats["excluded_region"] += 1
                if excl_shown < EXCLUDE_LOG_MAX:
                    log_kapt_excluded(client_name, "-")
                    excl_shown += 1
                continue
            stats["after_region"] += 1

            # ì¤‘ë³µ ì œê±°
            if bid_no and bid_no in seen_bidnum:
                stats["dedup_skip"] += 1
                continue
            if bid_no:
                seen_bidnum.add(bid_no)

            detail = f"https://www.k-apt.go.kr/bid/bidResultDetail.do?bid_noti_no={bid_no}" if bid_no else ""

            base = _build_base_notice(
                stage=stage, biz_type=biz_type, project_name=title,
                client=client_name, phone="", model="", qty=0,
                amount=(it.get("amount") or ""), is_cert="í™•ì¸í•„ìš”",
                notice_date=to_ymd(it.get("bidDeadline") or it.get("bidRegdate")),
                detail_link=detail, source='K-APT', kapt_code=kapt_code
            )
            base["assigned_office"] = assigned
            base["address"] = addr_txt or base.get("address","") 
            n = finalize_notice_dict(base, None, addr_txt, client_name)
            if n:
                buffer.append(n)

    # 4) ì €ì¥/ìš”ì•½ ë¡œê·¸
    if not buffer:
        _debug(f"(ìˆ˜ì§‘:{stats['total_items']}, í‚¤ì›Œë“œí›„:{stats['after_kw']}, "
               f"ê´€í• í›„:{stats['after_region']}, ì¤‘ë³µì œì™¸:{stats['dedup_skip']}, "
               f"íƒ€ì§€ì—­ì œì™¸:{stats['excluded_region']})")
        _print_data_none()
        return

    try:
        bulk_upsert_notices(buffer)
        _print_bulk_saved(len(buffer))  # "  [âœ… ì¼ê´„ ì €ì¥] Nê±´"
    except Exception as e:
        print(f"  [Error] ì €ì¥ ì‹¤íŒ¨: {type(e).__name__}: {e} (í›„ë³´:{len(buffer)})")


def _collect_by_state_year(bid_state: str, year: str):
    svc_key = (
        getattr(_local_config, "KAPT_SERVICE_KEY_DECODING", None)
        if _local_config else None
    ) or _cfg("KAPT_SERVICE_KEY")

    first = http_get_json(api_url(PATH_STTS), {   # <-- ì—¬ê¸°
        "serviceKey": svc_key,
        "pageNo": "1", "numOfRows": "1",
        "bidState": bid_state, "searchYear": year,
        "_type": "json",
    })
    total = int(((first.get("response") or {}).get("body") or {}).get("totalCount", 0) or 0)
    if total == 0:
        return []

    total_pages = (total + PAGE_SIZE - 1) // PAGE_SIZE
    params_list = [{
        "serviceKey": svc_key,
        "pageNo": str(p), "numOfRows": str(PAGE_SIZE),
        "bidState": bid_state, "searchYear": year, "_type": "json"
    } for p in range(1, total_pages + 1)]

    pages = fetch_pages_parallel(api_url(PATH_STTS), params_list)  # <-- ì—¬ê¸°
    results = []
    for data in (pages or []):
        items = (((data or {}).get("response") or {}).get("body") or {}).get("items")
        if not items:
            continue
        if isinstance(items, list):
            results.extend(items)
        else:
            results.append(items)  # ë‹¨ì¼ dict ë°©ì–´
    return results

from datetime import datetime, timedelta
from calendar import monthrange

def _month_chunks(start_ymd: str, end_ymd: str):
    s = datetime.strptime(start_ymd, "%Y%m%d")
    e = datetime.strptime(end_ymd, "%Y%m%d")
    cur = datetime(s.year, s.month, 1)
    while cur <= e:
        last_day = monthrange(cur.year, cur.month)[1]
        chunk_start = max(s, cur)
        chunk_end   = min(e, datetime(cur.year, cur.month, last_day))
        yield chunk_start.strftime("%Y%m%d"), chunk_end.strftime("%Y%m%d")
        # ë‹¤ìŒ ë‹¬ 1ì¼
        if cur.month == 12:
            cur = datetime(cur.year + 1, 1, 1)
        else:
            cur = datetime(cur.year, cur.month + 1, 1)

def _count_private_contracts(svc_key, start_ymd, end_ymd):
    q = {"serviceKey": svc_key, "_type": "json", "pageNo": "1", "numOfRows": "1",
         "startDate": start_ymd, "endDate": end_ymd}
    r = http_get_json(api_url(KAPT_PRIVATE_CONTRACT_PATH), q)
    body = ((r or {}).get("response") or {}).get("body") or {}
    return int(body.get("totalCount", 0) or 0)

from datetime import datetime

def fetch_and_process_kapt_private_contracts(search_ymd: str):
    """K-APT ìˆ˜ì˜ê³„ì•½ ê³µì§€ ìˆ˜ì§‘ (ì‹œìŠ¤í…œ í‘œì¤€: regDate ê¸°ì¤€ ì •ë ¬/ì‹ ê·œ/ì €ì¥)."""
    # === ë‚ ì§œ ë²”ìœ„ ê³„ì‚° ===
    endDate = search_ymd  # ì¡°íšŒ ì¢…ë£Œì¼ = ì…ë ¥ì¼ (YYYYMMDD)
    end_dt = datetime.strptime(search_ymd, "%Y%m%d")
    startDate = f"{end_dt.year - 1}0101"  # ì§ì „ë…„ë„ 1/1 ~ ì¡°íšŒì¼ê¹Œì§€

    print(f"\n--- [K-APT ìˆ˜ì˜ê³„ì•½] ì¡°íšŒê¸°ê°„: {startDate} ~ {endDate} ---")

    svc_key = getattr(_local_config, "KAPT_SERVICE_KEY_DECODING", None) or _cfg("KAPT_SERVICE_KEY")
    PAGE = PAGE_SIZE

    # yyyymmdd -> yyyy-mm-dd
    def _dash(yyyymmdd: str) -> str:
        s = (yyyymmdd or "").strip()
        return f"{s[0:4]}-{s[4:6]}-{s[6:8]}" if len(s) == 8 and s.isdigit() else s

    def _try_count(params, tag: str) -> int:
        r = http_get_json(api_url(KAPT_PRIVATE_CONTRACT_PATH), params)
        body = ((r or {}).get("response") or {}).get("body") or {}
        total = int(body.get("totalCount", 0) or 0)
        _debug(
            f"Â· [{tag}] resultCode={((r or {}).get('response') or {}).get('header',{}).get('resultCode')} "
            f"totalCount={total}, msg={((r or {}).get('response') or {}).get('header',{}).get('resultMsg')}"
        )
        return total

    # === regDate ê¸°ì¤€ ì „ì²´ ê±´ìˆ˜ ì¡°íšŒ ===
    q1 = {
        "serviceKey": svc_key,
        "_type": "json",
        "pageNo": "1",
        "numOfRows": "1",
        "startDate": startDate,   # regDate ì‹œì‘(YYYYMMDD)
        "endDate": endDate        # regDate ì¢…ë£Œ(YYYYMMDD)
    }
    total = _try_count(q1, f"ë“±ë¡ì¼ {startDate}~{endDate}")

    if total == 0:
        print("  - ë°ì´í„° ì—†ìŒ")
        return

    # === í˜ì´ì§• ===
    total_pages = (total + PAGE - 1) // PAGE
    _debug(f"Â· [ìˆ˜ì˜ê³„ì•½] ì´ {total}ê±´ / {total_pages}p â†’ ë³‘ë ¬ ìˆ˜ì§‘")
    params_list = [{**q1, "pageNo": str(p), "numOfRows": str(PAGE)} for p in range(1, total_pages + 1)]

    # === ë³‘ë ¬ ìˆ˜ì§‘ ===
    pages = fetch_pages_parallel(api_url(KAPT_PRIVATE_CONTRACT_PATH), params_list)

    # === íŒŒì‹± ë° ì €ì¥ ===
    buffer, stats = [], dict(total_items=0, after_kw=0, after_region=0)

    # ìƒì„¸ë³´ê¸° ë§í¬: bidDetail.do (ì‹œìŠ¤í…œê³¼ ë™ì¼ UX)
    def _make_detail_link(pc_num: str, kapt_code: str | None) -> str:
        if not pc_num:
            return ""
        return (
            "https://www.k-apt.go.kr/bid/bidDetail.do"
            f"?searchBidGb=private_contract"
            f"&bidTitle=&aptName=&searchDateGb=reg"
            f"&dateStart={_dash(startDate)}&dateEnd={_dash(endDate)}"
            f"&dateArea=1&bidState=&codeAuth=&codeWay=&codeAuthSub=&codeSucWay="
            f"&codeClassifyType1=&codeClassifyType2=&codeClassifyType3="
            f"&pageNo=1&type=4&bidArea=&bidNum={pc_num}"
            f"&bidNo=&mainKaptCode=&aptCode={(kapt_code or '')}"
        )

    for data in pages:
        items = (((data or {}).get("response") or {}).get("body") or {}).get("items") or []
        if isinstance(items, dict):
            items = [items]
        stats["total_items"] += len(items)

        for it in items:
            title         = (it.get("pcTitle") or "").strip()
            pc_date_raw   = (it.get("pcDate") or "").strip()    # ê³„ì•½ì¼ì (ì°¸ê³ )
            pc_st_raw     = (it.get("pcStDate") or "").strip()  # ì‹œì‘
            pc_ed_raw     = (it.get("pcEdDate") or "").strip()  # ì¢…ë£Œ
            reg_date_raw  = (it.get("regDate") or "").strip()   # ë“±ë¡ì¼ (ì‹œìŠ¤í…œ ê¸°ì¤€)
            kapt_code     = it.get("kaptCode")
            client_name   = it.get("kaptName") or "ë‹¨ì§€ëª… ì—†ìŒ"
            mall_addr     = (it.get("area") or "").strip()
            contract_no   = (it.get("pcNum") or "").strip()

            # í…ìŠ¤íŠ¸ í•„í„°
            if not is_relevant_text(
                title,
                _as_text(it.get("pcReason", "")),
                _as_text(it.get("codeClassifyType1", "")),
            ):
                continue
            stats["after_kw"] += 1

            detail_link = _make_detail_link(contract_no, kapt_code)

            # â˜… ì‹œìŠ¤í…œ ì–¼ë¼ì¸: ì €ì¥ ê¸°ì¤€ì¼ = regDate
            notice_date = to_ymd(reg_date_raw)

            # í•„ìš” ì‹œ ê³„ì•½ì •ë³´ë¥¼ memoì— ë³´ì¡´ (DB ìŠ¤í‚¤ë§ˆê°€ contract_date ì—†ìŒ ê°€ì •)
            extra_memo = ""
            if pc_date_raw or pc_st_raw or pc_ed_raw:
                extra_memo = f"[ê³„ì•½ì¼]{pc_date_raw} [ê¸°ê°„]{pc_st_raw}~{pc_ed_raw}".strip()

            base = _build_base_notice(
                stage="ìˆ˜ì˜ê³„ì•½",
                biz_type=(it.get("codeClassifyType1", "") or "ê¸°íƒ€"),
                project_name=title,
                client=client_name,
                phone=(it.get("companyTel", "") or ""),
                model="",
                qty=0,
                amount=_as_text(it.get("pcAmount", "")),
                is_cert="í™•ì¸í•„ìš”",
                notice_date=notice_date,         # â† regDate ê³ ì •
                detail_link=detail_link,
                source="K-APT",
                kapt_code=kapt_code
            )

            # finalizeì—ì„œ memoë¥¼ í•©ì¹˜ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì„œ merge (ì˜µì…˜)
            if extra_memo:
                base["memo"] = (base.get("memo") or "")
                if base["memo"]:
                    base["memo"] += "\n"
                base["memo"] += extra_memo

            n = finalize_notice_dict(base, None, mall_addr, client_name)
            if n:
                stats["after_region"] += 1
                buffer.append(n)

    if not buffer:
        _debug(f"(ìˆ˜ì§‘:{stats['total_items']}, í‚¤ì›Œë“œí›„:{stats['after_kw']}, ê´€í• í›„:{stats['after_region']})")
        print("  - ë°ì´í„° ì—†ìŒ")
        return

    try:
        bulk_upsert_notices(buffer)
        print(f"  [âœ… ì¼ê´„ ì €ì¥] {len(buffer)}ê±´")
        _debug(f"(ìˆ˜ì§‘:{stats['total_items']}, í‚¤ì›Œë“œí›„:{stats['after_kw']}, ê´€í• í›„:{stats['after_region']})")
    except Exception as e:
        print(f"  [Error] ì €ì¥ ì‹¤íŒ¨: {type(e).__name__}: {e} (í›„ë³´:{len(buffer)})")

# --- ë°œì£¼ê³„íš ---
def fetch_and_process_order_plans(search_ymd: str):
    print(f"\n--- [{to_ymd(search_ymd)}] ë°œì£¼ê³„íš(ë‚˜ë¼ì¥í„°) ìˆ˜ì§‘ ---")

    # 1) ì´ê±´ìˆ˜ 1íšŒ ì¡°íšŒ
    first = http_get_json(api_url(ORDER_PLAN_LIST_PATH), {
        "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
        "pageNo": "1", "numOfRows": "1",
        "inqryDiv": "1", "inqryBgnDt": f"{search_ymd}0000", "inqryEndDt": f"{search_ymd}2359"
    })
    body = _as_dict(first.get("response", {}).get("body"))
    total = int(body.get("totalCount", 0))
    if total == 0:
        print("  - ë°ì´í„° ì—†ìŒ"); return

    page_size   = 100
    total_pages = (total + page_size - 1) // page_size
    #print(f"  - ì´ {total}ê±´ / {total_pages}p")
    print(f"  - ì´ {total}ê±´")

    # 2) í˜ì´ì§€ ë³‘ë ¬ ìš”ì²­
    params_list = [{
        "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
        "pageNo": str(p), "numOfRows": str(page_size),
        "inqryDiv": "1", "inqryBgnDt": f"{search_ymd}0000", "inqryEndDt": f"{search_ymd}2359"
    } for p in range(1, total_pages + 1)]

    pages = fetch_pages_parallel(api_url(ORDER_PLAN_LIST_PATH), params_list)

    # 3) í˜ì´ì§€ ê²°ê³¼ ì²˜ë¦¬ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œë§Œ DB ì €ì¥)
    for data in pages:
        items = _as_items_list(_as_dict(data.get("response", {}).get("body")))
        for it in items:
            if it.get("bsnsDivNm") != "ë¬¼í’ˆ":
                continue
            title = it.get("bizNm", "")
            if not is_relevant_text(title,
                                    _as_text(it.get("bsnsDivNm")),
                                    _as_text(it.get("itemNm") or it.get("prdctNm")),
                                    _as_text(it.get("dminsttNm") or it.get("dmndInsttNm"))):
                continue


            client_code = it.get("orderInsttCd") or it.get("dminsttCd")
            client_name = it.get("orderInsttNm") or it.get("dminsttNm") or "ê¸°ê´€ëª… ì—†ìŒ"
            mall_addr   = guess_mall_addr(it)

            plan_no = it.get('orderPlanUntyNo') or ''
            detail_link = f"https://www.g2b.go.kr/pt/menu/selectSubFrame.do?framesrc=/pt/orderplan/orderPlanDetail.do?orderPlanNo={plan_no}" if plan_no else ""

            base = _build_base_notice(
                "ë°œì£¼ê³„íš", "ë¬¼í’ˆ", title, client_name, it.get("telNo", ""),
                "ê³„íš ë‹¨ê³„ í™•ì¸", 0, it.get("sumOrderAmt") or "", "í™•ì¸í•„ìš”",
                to_ymd(it.get("nticeDt")), detail_link
            )
            expand_and_store_with_priority(base, client_code, mall_addr, client_name)


# --- ì…ì°°ê³µê³  ---
def fetch_and_process_bid_notices(search_ymd: str):
    print(f"\n--- [{to_ymd(search_ymd)}] ì…ì°°ê³µê³ (ë‚˜ë¼ì¥í„°)) ìˆ˜ì§‘ ---")
    page, page_size, total_pages = 1, 100, 1
    while page <= total_pages:
        params = {
            "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
            "pageNo": str(page), "numOfRows": str(page_size),
            "bidNtceBgnDt": f"{search_ymd}0000", "bidNtceEndDt": f"{search_ymd}2359"
        }
        try:
            data = http_get_json(api_url(BID_LIST_PATH), params)
            if not isinstance(data, dict):
                print("  - ì‘ë‹µ ì—†ìŒ(ë„¤íŠ¸ì›Œí¬/ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜). ì¬ì‹œë„ ë˜ëŠ” ë‹¤ìŒ í˜ì´ì§€ë¡œ ì§„í–‰")
                page += 1
                time.sleep(0.35)
                continue            
            
            
            body = _as_dict(data.get("response", {}).get("body"))
            if page == 1:
                total = int(body.get("totalCount", 0))
                if total == 0:
                    print("  - ë°ì´í„° ì—†ìŒ")
                    break
                total_pages = (total + page_size - 1) // page_size
                #print(f"  - ì´ {total}ê±´ / {total_pages}p")
                print(f"  - ì´ {total}ê±´")

            items = _as_items_list(body)
            if not items:
                page += 1
                time.sleep(0.35)
                continue

            for it in items:
                if it.get("bsnsDivNm") and it.get("bsnsDivNm") != "ë¬¼í’ˆ":
                    continue
                title = it.get("bidNtceNm", "") or it.get("bidNm", "")
                if not is_relevant_text(title,
                                        _as_text(it.get("bsnsDivNm")),
                                        _as_text(it.get("itemNm") or it.get("prdctNm")),
                                        _as_text(it.get("dminsttNm") or it.get("dmndInsttNm"))):
                    continue


                client_code = it.get("dmndInsttCd") or it.get("dminsttCd")
                client_name = it.get("dmndInsttNm") or it.get("dminsttNm") or "ê¸°ê´€ëª… ì—†ìŒ"
                mall_addr = guess_mall_addr(it)

                # ìƒì„¸ URL
                detail_link = it.get("bidNtceUrl")
                if not detail_link:
                    bid_no = it.get('bidNtceNo') or ''
                    if bid_no:
                        detail_link = f"http://www.g2b.go.kr/pt/menu/selectSubFrame.do?framesrc=/pt/bid/bidInfoList.do?taskClCd=1&bidno={bid_no}"

                base = _build_base_notice(
                    "ì…ì°°ê³µê³ ", "ë¬¼í’ˆ", title, client_name, it.get("dmndInsttOfclTel", "") or it.get("telNo",""),
                    "ê³µê³  í™•ì¸ í•„ìš”", 0, it.get("asignBdgtAmt") or "", "í™•ì¸í•„ìš”",
                    to_ymd(it.get("bidNtceDate") or it.get("ntceDt")), detail_link or ""
                )
                expand_and_store_with_priority(base, client_code, mall_addr, client_name)

            page += 1
            time.sleep(0.35)
        except Exception as e:
            session.rollback()
            print(f"  [Error] ì…ì°°ê³µê³  ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            break


# --- ê³„ì•½ì™„ë£Œ ---
def fetch_and_process_contracts(search_ymd: str):
    from datetime import datetime, timedelta
    print(f"\n--- [{to_ymd(search_ymd)}] ê³„ì•½ì™„ë£Œ(ë‚˜ë¼ì¥í„°) ìˆ˜ì§‘ ---")

    start_dt = f"{search_ymd}0000"
    end_dt = f"{search_ymd}2359"

    # 1) ì´ê±´ìˆ˜ 1íšŒ ì¡°íšŒ
    first = http_get_json(api_url(CNTRCT_LIST_PATH), {
        "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
        "pageNo": "1", "numOfRows": "1",
        "inqryDiv": "1", "inqryBgnDt": start_dt, "inqryEndDt": end_dt
    })
    body = _as_dict(first.get("response", {}).get("body"))
    total = int(body.get("totalCount", 0))
    if total == 0:
        print("  - ë°ì´í„° ì—†ìŒ"); return

    page_size   = 100
    total_pages = (total + page_size - 1) // page_size
    #print(f"  - ì´ {total}ê±´ / {total_pages}p")
    print(f"  - ì´ {total}ê±´")

    # 2) íŒŒë¼ë¯¸í„° ë¬¶ìŒ ìƒì„± í›„ ë³‘ë ¬ ì¡°íšŒ
    params_list = [{
        "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
        "pageNo": str(p), "numOfRows": str(page_size),
        "inqryDiv": "1", "inqryBgnDt": start_dt, "inqryEndDt": end_dt
    } for p in range(1, total_pages + 1)]

    pages = fetch_pages_parallel(api_url(CNTRCT_LIST_PATH), params_list)

    # 3) í˜ì´ì§€ ê²°ê³¼ ì²˜ë¦¬ (ë²Œí¬ì—…ì„œíŠ¸ìš© ë²„í¼)
    buffer = []
    for data in pages:
        items = _as_items_list(_as_dict(data.get("response", {}).get("body")))
        for it in items:
            title = it.get("cntrctNm", "") or it.get("contNm","")
            if not is_relevant_text(title,
                                    _as_text(it.get("bsnsDivNm")),
                                    _as_text(it.get("itemNm") or it.get("prdctNm")),
                                    _as_text(it.get("dminsttNm") or it.get("dmndInsttNm"))):
                continue

            dm_cd = it.get("dminsttCd") or it.get("dmndInsttCd")
            cn_cd = it.get("cntrctInsttCd") or it.get("insttCd")
            client_code = dm_cd or cn_cd
            client_name = it.get("dminsttNm") or it.get("dmndInsttNm") or it.get("cntrctInsttNm") or it.get("insttNm") or "ê¸°ê´€ëª… ì—†ìŒ"
            mall_addr = guess_mall_addr(it)

            detail_link = it.get("cntrctDtlInfoUrl") or ""
            if not detail_link:
                unty_cntrct_no = it.get('untyCntrctNo')
                if unty_cntrct_no:
                    detail_link = f"https://www.g2b.go.kr:8067/contract/contDetail.jsp?Union_number={unty_cntrct_no}"

            base = _build_base_notice(
                "ê³„ì•½ì™„ë£Œ", "ë¬¼í’ˆ", title, client_name,
                it.get("cntrctInsttOfclTelNo", "") or it.get("telNo", ""),
                "ê³„ì•½ í™•ì¸ í•„ìš”", 0,
                it.get("cntrctAmt") or it.get("totAmt") or "",
                "í™•ì¸í•„ìš”",
                to_ymd(it.get("cntrctCnclsDate") or it.get("cntrctDate") or it.get("contDate")),
                detail_link
            )
            # ì£¼ì†Œ/ê´€í•  ê²°ì •ì€ expand_and_store_with_priorityì—ì„œ ì§„í–‰
            # â†’ ë²Œí¬ì—…ì„œíŠ¸ë¥¼ ìœ„í•´ ì¦‰ì‹œ DBì“°ì§€ ë§ê³  notice dict ìì²´ë¥¼ ëª¨ìë‹ˆë‹¤.
            # expand_and_store_with_priority ë‚´ë¶€ê°€ ì¦‰ì‹œ upsert/commit êµ¬ì¡°ë¼ë©´,
            # 'ì €ì¥' ëŒ€ì‹  'í™•ì •ëœ n dict'ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì–‡ê²Œ ë˜í•‘í•´ ë²„í¼ì— ì¶”ê°€í•˜ëŠ” ë°©ì‹ ê¶Œì¥
            n = finalize_notice_dict(base, client_code, mall_addr, client_name)  # ì•„ë˜ Bì—ì„œ ì œê³µ
            if n: buffer.append(n)

    # 4) ë²Œí¬ ì—…ì„œíŠ¸
    if buffer:
        bulk_upsert_notices(buffer)
        print(f"  [âœ… ì¼ê´„ ì €ì¥] {len(buffer)}ê±´")





# --- ë‚©í’ˆìš”êµ¬ ---


def _fetch_dlvr_detail(req_no: str):
    DLVR_DETAIL_PATH = "/1230000/at/ShoppingMallPrdctInfoService/getDlvrReqDtlInfoList"
    detail_params = {
        "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
        "inqryDiv": "2", "dlvrReqNo": req_no, "numOfRows": "100", "pageNo": "1"
    }
    try:
        data = http_get_json(api_url(DLVR_DETAIL_PATH), detail_params)
        body = _as_dict(data.get("response", {}).get("body"))
        return _as_items_list(body)
    except Exception as e:
        print(f"  [Error] ë‚©í’ˆìš”êµ¬ ìƒì„¸ ì‹¤íŒ¨({req_no}): {e}")
        return []

def _fetch_dlvr_detail_with_key(req_no: str):
    return req_no, _fetch_dlvr_detail(req_no)

def fetch_and_process_delivery_requests(search_ymd: str):
    print(f"\n--- [{to_ymd(search_ymd)}] ë‚©í’ˆìš”êµ¬(ë‚˜ë¼ì¥í„°) ìˆ˜ì§‘ ---")
    buffer = []
    CHUNK = 200  # ë²Œí¬ ë‹¨ìœ„

    # 1) ì´ê±´ìˆ˜ 1íšŒ
    first = http_get_json(api_url(DLVR_LIST_PATH), {
        "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
        "pageNo": "1", "numOfRows": "1",
        "inqryDiv": "1", "inqryBgnDate": search_ymd, "inqryEndDate": search_ymd
    })
    body = _as_dict(first.get("response", {}).get("body"))
    total = int(body.get("totalCount", 0))
    if total == 0:
        print("  - ë°ì´í„° ì—†ìŒ"); return

    page_size   = 100
    total_pages = (total + page_size - 1) // page_size
    #print(f"  - ì´ {total}ê±´ / {total_pages}")
    print(f"  - ì´ {total}ê±´")

    # 2) í˜ì´ì§€ ë³‘ë ¬ (ìš”ì•½ ëª©ë¡)
    params_list = [{
        "ServiceKey": _cfg("NARA_SERVICE_KEY"), "type": "json",
        "pageNo": str(p), "numOfRows": str(page_size),
        "inqryDiv": "1", "inqryBgnDate": search_ymd, "inqryEndDate": search_ymd
    } for p in range(1, total_pages + 1)]
    pages = fetch_pages_parallel(api_url(DLVR_LIST_PATH), params_list)

    # 3) ê° í˜ì´ì§€ì—ì„œ req_no ìˆ˜ì§‘ + ìƒì„¸ ë³‘ë ¬
    meta_by_req: Dict[str, Dict] = {}
    tasks = []
    with ThreadPoolExecutor(max_workers=12) as ex:
        for data in pages:
            items = _as_items_list(_as_dict(data.get("response", {}).get("body")))
            for it in items:
                req_nm = it.get("reqstNm") or it.get("dlvrReqNm") or ""
                if not is_relevant_text(req_nm):
                    continue
                req_no = it.get("dlvrReqNo") or it.get("reqstNo") or ""
                if not req_no:
                    continue

                dminstt_raw = it.get("dminsttInfo") or it.get("dmndInsttInfo") or ""
                dm_cd, dm_nm = parse_dminstt_code_from_complex(dminstt_raw)
                meta_by_req[req_no] = {
                    "req_nm": req_nm,
                    "client_code": dm_cd,
                    "client_name": dm_nm or it.get("dmndInsttNm") or it.get("dminsttNm") or "ê¸°ê´€ëª… ì—†ìŒ",
                    "mall_addr": guess_mall_addr(it),
                    "tel": it.get("cntrctDeptTelNo") or it.get("telNo") or "",
                    "rcpt": to_ymd(it.get("rcptDate") or it.get("dlvrReqRcptDate")),
                    "hdr_qty": _to_int(it.get("dlvrReqQty") or it.get("reqQty") or it.get("totQty")),
                    "hdr_amt": _to_int(it.get("dlvrReqAmt")),
                }
                tasks.append(ex.submit(_fetch_dlvr_detail_with_key, req_no))

    # 4) ìƒì„¸ ê²°ê³¼ ë°›ì•„ì„œ ì €ì¥(ì•„ë‹ˆê³ : í›„ë³´ dict ìˆ˜ì§‘)
    for fut in as_completed(tasks):
        req_no, products = fut.result()
        meta = meta_by_req.get(req_no)
        if not meta:
            continue

        if products:
            num_items = len(products)
            for product in products:
                prdct_nm = product.get("prdctNm") or ""
                if not is_relevant_text(meta["req_nm"], prdct_nm):
                    continue

                # ëª¨ë¸ëª… ì¶”ì¶œ
                model_name = product.get("modelNm")
                if not model_name:
                    name_all = product.get("prdctIdntNoNm", "")
                    if name_all:
                        parts = [p.strip() for p in name_all.split(",")]
                        model_name = parts[2] if len(parts) >= 3 else name_all
                model_name = model_name or "ëª¨ë¸ëª… ì—†ìŒ"

                # 1) KEA API + ìœ ì‚¬ë„ ê¸°ë°˜ ì¸ì¦ íŒì •
                certification_status = kea_cert_with_similarity(model_name)

                # 2) ê¸°ë³¸ì ìœ¼ë¡œ API ì‘ë‹µì´ ë¶ˆí™•ì‹¤í•˜ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ ë‹¤ì‹œ ì²´í¬
                if certification_status == "í™•ì¸í•„ìš”":
                    tmp = kea_check_certification(model_name)
                    if tmp != "í™•ì¸í•„ìš”":
                        certification_status = tmp

                # ìˆ˜ëŸ‰/ê¸ˆì•¡
                qty = (
                    _to_int(product.get("prdctQty"))
                    or _to_int(product.get("qty"))
                    or _to_int(product.get("orderQty"))
                    or _to_int(product.get("ordQty"))
                    or 0
                )
                if qty == 0 and num_items == 1:
                    qty = meta["hdr_qty"]

                amt_int = (
                    _to_int(product.get("prdctAmt"))
                    or _to_int(product.get("amt"))
                    or (meta["hdr_amt"] if num_items == 1 else 0)
                )
                amt = str(amt_int)  # ë¬¸ìì—´ë¡œ ì €ì¥ ê¶Œì¥

                base = _build_base_notice(
                    "ë‚©í’ˆìš”êµ¬", "ë¬¼í’ˆ", meta["req_nm"], meta["client_name"],
                    meta["tel"], model_name, qty, amt,
                    certification_status, meta["rcpt"], f"dlvrreq:{req_no}"
                )
                n = expand_and_store_with_priority(
                    base, meta["client_code"], meta["mall_addr"], meta["client_name"], save=False
                )
                if n: buffer.append(n)
        else:
            # ìƒì„¸ê°€ ë¹„ì–´ë„ í—¤ë” í•œ ì¤„ ì €ì¥
            base = _build_base_notice(
                "ë‚©í’ˆìš”êµ¬", "ë¬¼í’ˆ", meta["req_nm"], meta["client_name"],
                meta["tel"], "ì„¸ë¶€ë‚´ì—­ ë¯¸í™•ì¸",
                meta["hdr_qty"], str(meta["hdr_amt"]),  # ë¬¸ìì—´ë¡œ
                "í™•ì¸í•„ìš”", meta["rcpt"], f"dlvrreq:{req_no}"
            )
            n = expand_and_store_with_priority(
                base, meta["client_code"], meta["mall_addr"], meta["client_name"], save=False
            )
            if n: buffer.append(n)

        # ì£¼ê¸°ì  ë²Œí¬ ì €ì¥
        if len(buffer) >= CHUNK:
            bulk_upsert_notices(buffer); buffer.clear()

    # ë‚¨ì€ ê²ƒ ë§ˆë¬´ë¦¬
    if buffer:
        print(f"  [âœ… ì¼ê´„ ì €ì¥] {len(buffer)}ê±´")
        bulk_upsert_notices(buffer)


def resolve_address_from_bjd(addr_text, bjd_code) -> str:
    """
    [ì£¼ì†Œ ì „ìš©] ë²•ì •ë™ì½”ë“œâ†’ì£¼ì†Œ ë³´ê°•ê¸°.
    - ì ˆëŒ€ ì§€ì‚¬/ê´€í•  ê²°ì • í•¨ìˆ˜ í˜¸ì¶œ ì•ˆ í•¨.
    - addr_textê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©.
    - ì—†ìœ¼ë©´ bjd_codeë¡œ bjd_mapper.get_bjd_name_str ë˜ëŠ” get_bjd_name í˜¸ì¶œ.
    - í•­ìƒ 'ë¬¸ìì—´'ì„ ë°˜í™˜(ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´).
    """
    # -- ì•ˆì „ ë¬¸ìì—´í™”
    def _as_text(x) -> str:
        if x is None:
            return ""
        if isinstance(x, str):
            return x
        if isinstance(x, (int, float)):
            return str(x)
        if isinstance(x, list):
            return " ".join(_as_text(v) for v in x)
        try:
            return json.dumps(x, ensure_ascii=False)
        except Exception:
            return str(x)

    # 1) addr_text ìš°ì„ 
    addr = _as_text(addr_text).strip()

    # 2) ë¹„ì—ˆìœ¼ë©´ bjd_codeë¡œ ë³´ê°•
    if not addr:
        # bjd_codeë¥¼ ë¬¸ìì—´í™”
        code = _as_text(bjd_code).strip()
        name = ""
        if code:
            # bjd_mapper.get_bjd_name_str ìš°ì„  ì‚¬ìš©
            try:
                from bjd_mapper import get_bjd_name_str
                name = _as_text(get_bjd_name_str(code)).strip()
            except Exception:
                # ì—†ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ globalsì˜ get_bjd_name ì‚¬ìš©
                try:
                    if "get_bjd_name" in globals():
                        name = _as_text(globals()["get_bjd_name"](code)).strip()
                except Exception:
                    name = ""
        addr = name

    # 3) í”í•œ ì¡ê°’ ì¹˜í™˜
    if addr in ("-", "0", "None", "null", "NULL"):
        addr = ""

    # 4) ê³µë°± ì •ê·œí™”
    addr = " ".join(addr.split())
    return addr




# =========================
# [GUI ì—°ë™] êµ¬ì„±
# =========================
# === ë§¨ ìœ„/ì„¤ì • ì˜ì—­ ì–´ë”˜ê°€ì— ì¶”ê°€ ===
SKIP_STAGES = {
    "order_plan": True,     # ë‚˜ë¼ì¥í„° ë°œì£¼ê³„íš ìŠ¤í‚µ
    "kapt_private": True,   # K-APT ìˆ˜ì˜ê³„ì•½ ìŠ¤í‚µ
}

# === STAGES_CONFIG ì •ì˜ ë°”ë¡œ ì•„ë˜ë¥¼ ì´ì²˜ëŸ¼ ë°”ê¿”ì£¼ì„¸ìš” ===
STAGES_CONFIG = {
    "order_plan": {"name": "ë°œì£¼ê³„íš(ë‚˜ë¼ì¥í„°)", "func": fetch_and_process_order_plans},
    "bid_notice": {"name": "ì…ì°°ê³µê³ (ë‚˜ë¼ì¥í„°)", "func": fetch_and_process_bid_notices},
    "contract":   {"name": "ê³„ì•½ì™„ë£Œ(ë‚˜ë¼ì¥í„°)", "func": fetch_and_process_contracts},
    "delivery":   {"name": "ë‚©í’ˆìš”êµ¬(ë‚˜ë¼ì¥í„°)", "func": fetch_and_process_delivery_requests},
    "kapt_bid":   {"name": "ì…ì°°ê³µê³ (K-APT)", "func": fetch_and_process_kapt_bids},
    "kapt_result":{"name": "ì…ì°°ê²°ê³¼(K-APT)", "func": fetch_and_process_kapt_bid_results},
    "kapt_private":{"name":"ìˆ˜ì˜ê³„ì•½(K-APT)", "func": fetch_and_process_kapt_private_contracts},
}

# â†“â†“â†“ ì¶”ê°€: ì‹¤í–‰ ëŒ€ìƒì—ì„œ ìŠ¤í‚µê°’(True)ì¸ í‚¤ ì œê±°
STAGES_CONFIG = {
    k: v for k, v in STAGES_CONFIG.items()
    if not SKIP_STAGES.get(k, False)
}


def fetch_data_for_stage(search_date: str, stage_config: dict):
    """
    gui_app.pyì˜ SyncWorkerì—ì„œ í˜¸ì¶œí•  ì§„ì…ì .
    """
    if "func" in stage_config and isinstance(stage_config["func"], Callable):
        stage_func = stage_config["func"]
        stage_func(search_date)
    else:
        raise ValueError(f"Invalid stage_config: 'func' not found or not callable for {stage_config.get('name')}")

def get_db_session():
    SessionLocal = sessionmaker(bind=engine)
    return SessionLocal()


def recheck_all_certifications():
    session = get_db_session()
    notices = session.query(Notice).all()

    for n in notices:
        model = n.model or n.model_name or ""
        if not model:
            n.is_certified = "í™•ì¸í•„ìš”"
            continue
        
        # ìœ ì‚¬ë„ + KEA API ì¸ì¦ í™•ì¸
        cert = kea_cert_with_similarity(model)

        # ë³´ìˆ˜ì  2ì°¨ ì²´í¬
        if cert == "í™•ì¸í•„ìš”":
            tmp = kea_check_certification(model)
            if tmp != "í™•ì¸í•„ìš”":
                cert = tmp

        n.is_certified = cert

    session.commit()
    print("ëª¨ë“  ê¸°ì¡´ ë°ì´í„°ì˜ ì¸ì¦ ì—¬ë¶€ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    print("="*50)
    print("COLLECT_DATA.PY ë‹¨ë… í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("="*50)
    
    # í…ŒìŠ¤íŠ¸í•  ë‚ ì§œë¥¼ ì§€ì •í•©ë‹ˆë‹¤. (ì˜ˆ: '20250904')
    test_date = "20250904" 
    
    print(f"\n>>> í…ŒìŠ¤íŠ¸ ë‚ ì§œ: {test_date}\n")

    # DB í…Œì´ë¸”ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)
    print("[DB] í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê¸°ì¡´ í…Œì´ë¸”ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    Base.metadata.drop_all(engine)
    Base.metadata.create_all(engine)

    # í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì€ ìˆ˜ì§‘ í•¨ìˆ˜ë¥¼ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.
    # ë¬¸ì œê°€ ë˜ëŠ” K-APT í•¨ìˆ˜ë§Œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ì•„ë˜ G2B í•¨ìˆ˜ë“¤ì€ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.
    
    print("\n--- ë‚˜ë¼ì¥í„°(G2B) ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ---")
    #fetch_and_process_order_plans(test_date)
    fetch_and_process_bid_notices(test_date)
    fetch_and_process_contracts(test_date)
    fetch_and_process_delivery_requests(test_date)

    print("\n--- ê³µë™ì£¼íƒ(K-APT) ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ---")
    fetch_and_process_kapt_bids(test_date)
    #fetch_and_process_kapt_private_contracts(test_date)
    fetch_and_process_kapt_bid_results(test_date)

    print("\n>>> í…ŒìŠ¤íŠ¸ ì™„ë£Œ. DBì— ì €ì¥ëœ K-APT ë°ì´í„° ê±´ìˆ˜ í™•ì¸:")
    
    session = Session()
    kapt_count = session.query(Notice).filter(Notice.source_system == 'K-APT').count()
    g2b_count = session.query(Notice).filter(Notice.source_system == 'G2B').count()
    session.close()

    print(f"  - K-APT ì €ì¥ ê±´ìˆ˜: {kapt_count} ê±´")
    print(f"  - G2B ì €ì¥ ê±´ìˆ˜: {g2b_count} ê±´")
    print("\n[ì™„ë£Œ] `data/eers_data.db` íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ê³ , GUIë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ê°€ ì •ìƒ ì¡°íšŒë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")